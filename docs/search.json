[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Конечные автоматы в морфологическом анализе 2025/2026",
    "section": "",
    "text": "Введение\nДанные материалы являются конспектом курса Т. Б. Казаковой и Г. А. Мороза ‘Конечные автоматы в морфологическом анализе’ 2025–2026. Данный курс предоставляет углубленное изучение применения конечных автоматов в морфологическом анализе. Конечные автоматы используются для моделирования морфологии языков, особенно малоресурсных. Курс охватывает основные этапы создания морфологического анализатора в системе lexd и twol, проверку его на корпусах, взвешивание трансдьюсера для дизамбигуации. В качестве иллюстративного материала будут использоваться лингвистические задачи и реальные примеры из практики преподавателей. В нашем курсе мы стараемся предложить стратегии компьютерного правилого моделирвания морфологических и морфонологически проблем, стараясь охватить явления интересные с точки зрения теоретической лингвистики.\nВ курсе будет 6 домашних заданий и экзамен. Все домашние задания имеют одинаковый вес. Экзамен весит 0.4. За курс предусмотрен автомат, если студента устраивает накопленная оценка.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#список-использованных-программ",
    "href": "index.html#список-использованных-программ",
    "title": "Конечные автоматы в морфологическом анализе 2025/2026",
    "section": "Список использованных программ",
    "text": "Список использованных программ\n```{shell}\nlexd -V\n```\n\n\nlexd 1.3.5\n\n\n```{shell}\nhfst-twolc -V\n```\n\n\n\nhfst-twolc 0 (hfst 3.16.2)\nCopyright (C) 2010 University of Helsinki,\nLicense GPLv3: GNU GPL version 3 \n&lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and \nredistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n```{shell}\ncg3 -V\n```\n\n\nVISL CG-3 Disambiguator version 1.6.4.13898\nCopyright (C) 2007-2025 GrammarSoft ApS. Licensed under GPLv3+\n\n\n```{shell}\ngit -v\n```\n\n\ngit version 2.43.0",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#домашние-задания",
    "href": "index.html#домашние-задания",
    "title": "Конечные автоматы в морфологическом анализе 2025/2026",
    "section": "Домашние задания",
    "text": "Домашние задания\n\nСсылка на первое домашнее задание. Дедлайн: 19 февраля 23:59.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html",
    "href": "01_introduction_to_transducers.html",
    "title": "1  Автоматический морфологический анализ",
    "section": "",
    "text": "1.1 Приведение к основе (stemming)\nМорфологический анализ, как его обычно видят лингвисты, обычно включает в себя несколько вещей:\nВ зависимости от целей люди делают акцент на разные аспекты морфологического анализа. Для многих NLP задач полезным является приведение к начальной форме, лингвистический корпусной анализ практически невозможен без заранее определенных морфологических форм, и все три необходимы для представления языкового материала в научной работе. Также стоит помнить, что для разных задач могут быть важны разные категории, например, лингвисты, когда приводят примеры, редко перечисляют несловоизменительную информацию (например, род для существительных), которая может быть важна в каких-то задачах.\nДостаточно широкое применение в ранюю эпоху NLP получили методы, которые позволяют привести словоформы к основе или квазиоснове. Эта процедура помогала уменьшить разнообразие форм в тексте, что облегчало поиск и извлечение информации. В работе (Singh и Gupta 2017) приводится целая классификация стемеров:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html#много-данных",
    "href": "01_introduction_to_transducers.html#много-данных",
    "title": "1  Автоматический морфологический анализ",
    "section": "1.2 Много данных",
    "text": "1.2 Много данных\nОбычно, если много данных, люди используют нейросети. Для морфологического анализа русского языка их использовали в следующих работах (Arefyev, Gratsianova, и Popov 2018; Sorokin и Kravtsova 2018; Bolshakova и Sapin 2019a, 2019b, 2020; Garipov, Morozov, и Glazkova 2023). Используются разные архитектуры:\n\ncвёрточные нейронные сеть (convolutional neural network, CNN);\nдеревья решения с градиентным бустингом (decision trees with gradient boosting);\nдвунаправленная длинная цепь элементов краткосрочной памяти (Biderectional long short-term memory network, Bi-LSTM);\nи другие.\n\nДругой важный проект, который можно упомянуть в этом разделе: UDPipe — проект, основанный на размеченных в формате Universal Dependencies трибанках большого количества языков (Straka 2018). Отметим, что их задача амбициознее: они строят модель, совмещающую морфологический парсер и синтаксический парсер дерева зависимостей. Внутри: длинная цепь элементов краткосрочной памяти (LSTM), которая работает на основе векторного представления слов.\nТакже стоит упомянуть проект Morfessor, в котором используют скрытые марковские цепи (HMM) (Grönroos и др. 2014).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html#другие-правиловые-подходы",
    "href": "01_introduction_to_transducers.html#другие-правиловые-подходы",
    "title": "1  Автоматический морфологический анализ",
    "section": "1.3 Другие правиловые подходы",
    "text": "1.3 Другие правиловые подходы\nСтоит отметить, что существуют не основанные на трансдьюсерах правиловые подходы, например,\n\nпроект uniparser-morph Тимофея Архангельского (Архангельский 2012);\nнечто, что работает в SIL Fieldworks;\nмножество узконаправленных парсеров, написанных для конкертных языков.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html#от-автоматов-к-морфологическим-трансдьюсерам",
    "href": "01_introduction_to_transducers.html#от-автоматов-к-морфологическим-трансдьюсерам",
    "title": "1  Автоматический морфологический анализ",
    "section": "1.4 От автоматов к морфологическим трансдьюсерам",
    "text": "1.4 От автоматов к морфологическим трансдьюсерам\n\n1.4.1 Конечные автоматы\nТеория автоматов — это дисциплина на стыке математики и компьютерных наук, которая появилась в XX веке. Первые конечных автоматов были предложены в работах (Mealy 1955; Moore 1956). Данный раздел основан на первой главе из (Beesley и Karttunen 2003: 1–42).\nПод автоматами мы понимаем абстрактные машины, которые принимают разные состояния, а изменение состояний вызывается некоторым действием:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nМы не будем давать формального определения конечных автоматов, а перечислим его составляющие:\n\nалфавит, который автомат понимает;\nконечное количество состояний;\nпереходы между состояниями;\nодно начальное состояние (часто обозначют нулем);\nнабор конечных состояний (часто обозначают двойным кружочком).\n\nКонечные автоматы можно использовать для побуквенной верефикации поданных на вход слов:\n\n\n\n\n\n\n\n\n\nЕсли программа смогла пройти путь до конечного состояния (обозначен двойным кружочком), значит операция завершилась успехом, в остальных случаях — неудачей. Обычно путь, ведущий к неудаче, не отображают.\n\n\n\n\n\n\n\n\n\nМожно сделать так, чтобы автомат верефицировал несколько слов:\n\n\n\n\n\n\n\n\n\nПолученный автомат можно оптимизировать, так, чтобы там было меньше узлов, а задачи он решал те же самые:\n\n\n\n\n\n\n\n\n\nРазличают детерминированные и недетерминированные конечные автоматы. Последние отличаются от первых тем, что не выполняют одно из следующих требований:\n\nлюбой переход единственным образом определяется по текущему состоянию и входному символу;\nчтение входного символа требуется для каждого изменения состояния.\n\nСуществуют математические работы, доказывающие, что для любого регулярного языка существует детерминированный конечный автомат с наименьшим возможным числом состояний. Такой автомат единственен с точностью до изоморфизма. Нам это важно как знание, что наши лингвистические автоматы можно оптимизировать.\nВажно отметить, что в данном разделе наши автоматы были представлены диаграммами состояний, однако автомат можно представить и в виде таблицы переходов, в которой каждая строка соотвествует одному состоянию, а столбцу допустимый входной символ (и выходной, если речь о трансдьюсерах, см. ниже). Данный формат еще называют ATT. Дополнительный столбец может соответсвовать столбцу весов.\n\n\n\nисходное состояние\nследующее состояние\nвходной символ\n\n\n\n\n0\n1\nс\n\n\n1\n2\nл\n\n\n2\n3\nо\n\n\n3\n4\nн\n\n\n0\n5\nу\n\n\n5\n1\nк\n\n\n0\n6\nк\n\n\n6\n1\nу\n\n\n\nТо же самое, записанное в формате БНФ (Бэкусовская нормальная форма или Бэкуса-Науэра форма):\n0::с1|у5|к6\n1::л2\n2::о3\n3::н4  \n5::к1\n6::у1\n\n\n1.4.2 Трансдьюсеры\nВсё, что мы рассмотрели до этого момента, позволяло лишь принимать/отвергать слова, поданные на вход. Если мы немного усложним автомат, добавив в него еще выходной алфавит, то мы получим трансдьюсер (в русской википедии они названы конечными автоматами с выходом). Мы будем использовать обновленную нотацию: то, что представлено на вход, мы пишем слева от двоеточия, а то, что получается на выходе — справа.\n\n\n\n\n\n\n\n\n\nВ добавку к проверке наших слов, которое обеспечивалось конечными автоматами, мы получаем нечто на выходе. Пока в нашем примере на выходе получается то же самое, что на входе. Пустую строку принято обозначать греческой буквой эпсилон: ε. Таким образом мы можем представить трансдьюсер, который, наконец-то, делает некоторый морфологический анализ.\n\n\n\n\n\n\n\n\n\n\n\n\nвход\nвыход\n\n\n\n\nслон\nслон&lt;n&gt;&lt;nom&gt;&lt;sg&gt;\n\n\nслоном\nслон&lt;n&gt;&lt;ins&gt;&lt;sg&gt;\n\n\nуклон\nуклон&lt;n&gt;&lt;nom&gt;&lt;sg&gt;\n\n\nуклоном\nуклон&lt;n&gt;&lt;ins&gt;&lt;sg&gt;\n\n\nкулон\nкулон&lt;n&gt;&lt;nom&gt;&lt;sg&gt;\n\n\nкулоном\nкулон&lt;n&gt;&lt;ins&gt;&lt;sg&gt;\n\n\n\nВажно также отметить, что вообще-то не обязательно что-то делать на каждом этапе. Можно сделать трансдьюсер, который по духу будет автоматом, но дойдя до некоторого состояния, будет выдавать лэйбл.\n\n\n\n\n\n\n\n\n\nЛингвистические трансдьюсеры можно использовать для\n\nморфологического анализа\nтранслитерации/транскрипции\nпредиктивного ввода\nв системах проверки правописания\nв системах автоматического перевода близкородственных языков\nв системах распознавания речи\nи др.\n\nС трансдьюсерами можно делать много разных операций:\n\nобъединение (конечные и начальные состояния совпадают, все промежуточные сохраняются);\nконкатенация (конечное состояние одного трансдьюсера становится начальным состоянием другого);\nпересечение (в особых случаях;\nвычитание (в особых случаях);\nкомпозиция (обсудим ниже).\n\nКомпозиция трансдьюсеров аналогична функциям join для таблиц. После композиции трансдьюсера &lt;x, y&gt; и &lt;y, z&gt; получается новый трансдьюсер &lt;x, z&gt;.\n\n\n\n\n\n\n\n\n\nИменно при помощи композиции трансдьюсеров можно объединять трансдьюсеры с разными “целями”: например, трансдьюсер, который приводит к начальной форме и пишет морфологические теги, с трансдьюсером, который переводит основы. Кроме того, бывают взвешенные трансдьюсеры, в которых каждому переходу в новое состояние приписывается некоторый вес. Такой трансдьюсер позволяет не просто получать возможные варианты, но и ранжировать их, что важно при создании спеллчекеров.\nЗакончим перечислением преимуществ трансдьюсеров. Оптимизированный трансдьюсер оптимален с точки зрения объема требуемой памяти для хранения и скорости поиска. Композиция трансдьюсеров открывает большой и богатый мир, позволяет создавать и комбинировать между собой очень разные инструменты.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html#проблемы-моделирования-морфологии-языка",
    "href": "01_introduction_to_transducers.html#проблемы-моделирования-морфологии-языка",
    "title": "1  Автоматический морфологический анализ",
    "section": "1.5 Проблемы моделирования морфологии языка",
    "text": "1.5 Проблемы моделирования морфологии языка\n\nПроблема описаний:\n\nнеполнота: то, что исследователи посчитали достаточным для грамматического описания, может быть недостаточно для моделирования; для некоторого языка может быть доступно грамматическое описание, но отсутствовать словарь и наоборот; словарь может не содержать информации про словоизменительный класс каких-то единиц, которые различаются в грамматическом описании.\nпротиворечивые источники: грамматические описания могут противоречить друг другу; грамматические описания могут противоречить словарям\n\nПроблема вариативности:\n\nидиолектная\nдиалектная\nсвязанная с какими-нибудь социолингвистическими параметрами (в первую очередь такие как пол и возраст, но можно придумать и другие)\n\nПроблема неоднозначности (особенно перекошенной частотно):\n\nлексической\nморфологическая\nсинтаксическая\n…\n\nПроблема идиом и суплетивизма: в какой форме стоит глагол в ругательстве ** твою мать?\nПроблема циклов, возникающих при словообразовании:\n\nслуга (N) &gt; служить (V) &gt; услужить (V) &gt; услужение (N), услуга (N)1\n\nПроблема морфологической сложности: для языков с бедной морфологией проще строить прсотой правиловый морфологический парсер, а не трансдьюсер\nТехнические сложности: весь софт для создания морфологических трансдьюсеров пишут под Linux, за исключением может быть пакета hfst-dev для Python\nНе так просто работать совместно над одним проектом, чаще всего лучше иметь двух людей — специалиста по языку, и специалиста по трансдьюсерам.\n\nСмотрите еще обсуждения на FST в работе (Wintner 2008).\n\n\n\n\nArefyev, N. V., T. Y. Gratsianova, и K. P. Popov. 2018. «Morphological segmentation with sequence to sequence neural network». В KКомпьютерная лингвистика и интеллектуальные технологии, 85–95.\n\n\nBeesley, K. R., и L. Karttunen. 2003. Finite State Morphology: Xerox tools and techniques. Stanford: Center for Study of Language Information.\n\n\nBolshakova, E. I., и A. S. Sapin. 2019a. «Bi-LSTM model for morpheme segmentation of Russian words». В Artificial Intelligence and Natural Language: 8th Conference, AINL 2019, Tartu, Estonia, November 20–22, 2019, Proceedings 8, 151–60. Springer.\n\n\n———. 2019b. «Comparing models of morpheme analysis for Russian words based on machine learning». В Компьютерная лингвистика и интеллектуальные технологии, 104–13.\n\n\n———. 2020. «An Experimental Study of Neural Morpheme Segmentation Models for Russian Word Forms.» В CMCL, 79–89.\n\n\nGaripov, T., D. Morozov, и A. Glazkova. 2023. «Generalization ability of CNN-based Morpheme Segmentation». В 2023 Ivannikov Ispras Open Conference (ISPRAS), 58–62. IEEE.\n\n\nGrönroos, S.-A., S. Virpioja, P. Smit, и M. Kurimo. 2014. «Morfessor FlatCat: An HMM-based method for unsupervised and semi-supervised learning of morphology». В Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, 1177–85.\n\n\nMealy, G. H. 1955. «A method for synthesizing sequential circuits». The Bell System Technical Journal 34 (5): 1045–79.\n\n\nMoore, E. F. 1956. «Gedanken-experiments on sequential machines». Automata studies 34: 129–53.\n\n\nSingh, J., и V. Gupta. 2017. «A systematic review of text stemming techniques». Artificial Intelligence Review 48: 157–217.\n\n\nSorokin, Alexey, и Anastasia Kravtsova. 2018. «Deep convolutional networks for supervised morpheme segmentation of Russian language». В Artificial Intelligence and Natural Language: 7th International Conference, AINL 2018, St. Petersburg, Russia, October 17–19, 2018, Proceedings 7, 3–10. Springer.\n\n\nStraka, Milan. 2018. «UDPipe 2.0 prototype at CoNLL 2018 UD shared task». В Proceedings of the CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies, 197–207.\n\n\nWintner, S. 2008. «Strengths and weaknesses of finite-state technology: a case study in morphological grammar development». Natural Language Engineering 14 (4): 457–69.\n\n\nАрхангельский, Т. А. 2012. «Принципы построения морфологического парсера для разноструктурных языков». Phdthesis, Московский государственный университет им. М. В. Ломоносова.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "01_introduction_to_transducers.html#footnotes",
    "href": "01_introduction_to_transducers.html#footnotes",
    "title": "1  Автоматический морфологический анализ",
    "section": "",
    "text": "Я лишь предполагаю такую историческую деревацию, возможно, этимологии этих слов таят в себе значительно более сложную историю.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Автоматический морфологический анализ</span>"
    ]
  },
  {
    "objectID": "02_introduction_to_lexd.html",
    "href": "02_introduction_to_lexd.html",
    "title": "2  Введение в lexd: морфология",
    "section": "",
    "text": "2.1 Техническое введение\nВ данном разделе мы обсуждаем синтаксис программы lexd (Swanson и Howell 2021). Данная программа работает в связке\nЭто консольная программа, работающая на юниксоподобных системах. Чтобы избежать сложностей на начальных этапах курса, мы решили вначале познакомиться с синтаксисом lexd и попробовать описывать разные языковые явления, не затрудняя всех установкой и запуском у себя на компьютере. В связи с этим мы предлагаем выучить следующие четыре команды, которые будут работать на операционных системах Linux, основанных на Debian/Ubuntu, и в Google Colab:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Введение в `lexd`: морфология</span>"
    ]
  },
  {
    "objectID": "02_introduction_to_lexd.html#техническое-введение",
    "href": "02_introduction_to_lexd.html#техническое-введение",
    "title": "2  Введение в lexd: морфология",
    "section": "",
    "text": "с бесплатным програмным обеспечением с открытым исходным кодом Helsinki Finite-State Tookit hfst (Lindén и др. 2011);\nаналогичным инструментом от Apertium lttoolbox (Ortiz Rojas, Forcada, и Ramı́rez Sánchez 2005).\n\n\n\nскачиваем инструкции для установки lexd и hfst и дальнейшей работы с ними, записанные в простом текстовом файле, которые можно прочитать, если открыть ссылку из команды. Команда make запускает установку. Для того, чтобы это работало в Google Colab нужно перед командой нужно поставить восклицательный знак: !curl .... Знак доллара означает, что дальше следует команда командной строки, не надо его никуда копировать.\n\n```{shell}\n$ curl -s https://raw.githubusercontent.com/agricolamz/2026_morphological_transducers/refs/heads/main/task_tests/Makefile -o Makefile; make\n```\n\nдальше мы ожидаем, что вы создадите в коллабе или у себя на компьютере (если у вас Linux), файл с названием task.lexd. В Google Colab для этого достаточно вставить первой строкой кодового блока %%writefile task.lexd. Вот пример такого файла:\n\n```{lexd}\nPATTERNS\nVerbRoot VerbInfl\n\nLEXICON VerbRoot\nsing&lt;v&gt;:sing\nwalk&lt;v&gt;:walk\ndance&lt;v&gt;:dance\n\nLEXICON VerbInfl\n&lt;pres&gt;:\n&lt;pres&gt;&lt;3&gt;&lt;sg&gt;:s\n```\n\nПосле того, как вы установили нужные программы и создали файл, можно посмотреть формы и разборы, которые генерируются трансдьюсером. Это можно сделать следующей командой (не забудьте поставить восклицательный знак перед make в Google Colab):\n\n```{shell}\n$ make forms\n\nsing&lt;v&gt;&lt;pres&gt;:sing\nsing&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;:sings\nwalk&lt;v&gt;&lt;pres&gt;:walk\nwalk&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;:walks\ndance&lt;v&gt;&lt;pres&gt;:dance\ndance&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;:dances\n```\n\nКроме того можно посмотреть анализ/генерацию конкретных форм (не забудьте поставить восклицательный знак перед make в Google Colab):\n\n```{shell}\n$ make analysis FORM=\"sings\"\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; sings sing&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;    0,000000\n```\n```{shell}\n$ make generation FORM=\"walk&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;\"\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; walk&lt;v&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;  walks   0,000000\n```\n\nВ ходе курса мы будем разбирать разные лингвистические задачи. У каждой задачи есть номер и автоматический тест, который его проверяет. Чтобы запустить автоматическую проверку, следует ввести команду, где первое число – номер раздела, а второе число – номер задачи. Например, для того, чтобы проверить, работает ли проверка задания, попробуйте запустить следующую команду:\n\n```{shell}\n$ make test_02_01\n```\n\nЧтобы окончательно посмотреть все варианты, попробуйте изменить последнюю строчку файла task.lexd на &lt;pres&gt;&lt;3&gt;&lt;sg&gt;:S и снова перезапустить команду:\n\n```{shell}\n$ make test_02_01\n```",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Введение в `lexd`: морфология</span>"
    ]
  },
  {
    "objectID": "02_introduction_to_lexd.html#программа-lexd",
    "href": "02_introduction_to_lexd.html#программа-lexd",
    "title": "2  Введение в lexd: морфология",
    "section": "2.2 Программа lexd",
    "text": "2.2 Программа lexd\nУ программы lexd есть подробный туториал, так что данный раздел во многом опирается на него. Давайте подробнее рассмотрим lexd файл, который мы видели в прошлом разделе:\n```{lexd}\n1PATTERNS\nVerbRoot VerbInfl   \n\n2LEXICON VerbRoot\n3sing&lt;v&gt;:sing\nwalk&lt;v&gt;:walk\ndance&lt;v&gt;:dance\n\n4LEXICON VerbInfl\n5&lt;pres&gt;:\n6&lt;pres&gt;&lt;3&gt;&lt;sg&gt;:s\n```\n\n1\n\nОбязательный раздел PATTERNS, в котором каждая строка сообщает, как могут соединяться элементы из разных групп лексикона.\n\n2\n\nГруппа лексикона, которая состоит из слова LEXICON и имени, под которым данная группа появляется в разделе PATTERNS\n\n3\n\nНаполнение группы. Первым идет разбор, а потом после двоеточия языковой материал. Морфологические теги принято записывать в треугольных скобках.\n\n4\n\nВторая группа LEXICON и ее имя.\n\n5\n\nПример нулевой морфемы.\n\n6\n\nПример морфемы с несколькими морфологическими тегами.\n\n\nОтметим, что можно создавать свои именнованные подразделы PATTERN, которые потом можно использовать в разделе PATTERNS, например:\n```{lexd}\nPATTERNS\nVerbStem Tense PersonNumber\n\n1PATTERN VerbStem\nVerbRoot\nVerbRoot Causative\n\nLEXICON VerbRoot\n...\n\nLEXICON Causative\n...\n\nLEXICON Tense \n...\n\nLEXICON PersonNumber\n...\n```\n\n1\n\nИменованный раздел PATTERN, который используется потом в разделе PATTERNS.\n\n\nТаким образом, в каждом файле lexd должен быть раздел PATTERNS, содержащий в себе переменные, которые могут быть заданы либо в разделе PATTERN, либо в разделе LEXICON, либо их анонимные варианты (см. раздел Раздел 2.2.2). Также разные разделы можно переименовывать при помощи группы ALIAS (см. мануал). Комментарии можно оформлять при помощи хеша #.\n\n\n\n\n\n\nЗадание 02_02\n\n\n\nНиже представлен фрагмент ицаринской (даргинские, нахско-дагестанские) парадигмы из (Sumbatova и Mutalov 2003). Попробуйте смоделировать ее при помощи lexd. Для корректного моделирования формы sup.lat нужна морфонология, так что при моделировании используйте форму в скобках. При моделировании придется покривить душой: -б в формах ess и dir — инфикс классного показателя. Помните, что каждый морфологический тег следует обрамлять в отдельные треугольные скобки, например, ссика&lt;n&gt;&lt;ant&gt;&lt;dir&gt;:ссикасабал. Для ориентации в нашем lexd файле 26 строк.\n\n\n\nформа\nкозел\nмедведь\n\n\n\n\nabs\nкьаца\nссика\n\n\nerg\nкьацал\nссикал\n\n\ngen\nкьацала\nссикала\n\n\ncom\nкьацаццилли\nссикаццилли\n\n\nsup.lat\nкьацай (&lt; кьацайи)\nссикай (&lt; ссикайи)\n\n\nsup.ess\nкьацайиб\nссикайиб\n\n\nsup.dir\nкьацайибал\nссикайибал\n\n\nsup.el\nкьацайир\nссикайир\n\n\nsub.lat\nкьацагъу\nссикагъу\n\n\nsub.ess\nкьацагъуб\nссикагъуб\n\n\nsub.dir\nкьацагъубал\nссикагъубал\n\n\nsub.el\nкьацагъур\nссикагъур\n\n\nant.lat\nкьацаса\nссикаса\n\n\nant.ess\nкьацасаб\nссикасаб\n\n\nant.dir\nкьацасабал\nссикасабал\n\n\nant.el\nкьацасар\nссикасар\n\n\npost.lat\nкьацагьа\nссикагьа\n\n\npost.ess\nкьацагьаб\nссикагьаб\n\n\npost.dir\nкьацагьабал\nссикагьабал\n\n\npost.el\nкьацагьар\nссикагьар\n\n\nin.lat\nкьацацци\nссикацци\n\n\nin.ess\nкьацацциб\nссикацциб\n\n\nin.dir\nкьацаццибал\nссикаццибал\n\n\nin.el\nкьацаццир\nссикаццир\n\n\n\n\n\n\n2.2.1 Операторы\nКвантификация напоминает регулярные выражения:\n\n? — ноль или один раз\n* — ноль и более раз 1\n+ — один и более раз\n\n\nс операторомбез оператора\n\n\n```{lexd}\nPATTERNS\nRoot Negation?\n\nLEXICON Root\n...\n\nLEXICON Negation\n...\n```\n\n\n```{lexd}\nPATTERNS\nRoot\nRoot Negation\n\nLEXICON Root\n...\n\nLEXICON Negation\n...\n```\n\n\n\n\n| — оператор или (можно с пробелами вокруг)\n\n\nс операторомбез оператора\n\n\n```{lexd}\nPATTERNS\nRoot PastInflection|PresentInflection\n\nLEXICON Root\n...\n\nLEXICON PastInflection\n...\n\nLEXICON PresentInflection\n...\n```\n\n\n```{lexd}\nPATTERNS\nRoot PastInflection\nRoot PresentInflection\n\nLEXICON Root\n...\n\nLEXICON PastInflection\n...\n\nLEXICON PresentInflection\n...\n```\n\n\n\n\nКроме того есть операторы, названные в матералах lexd ситом, &gt; и &lt;:\n\n\nс операторомбез оператора\n\n\n```{lexd}\nPATTERNS\nVerbRoot &gt; TAM &gt; CLITICS\n\nLEXICON Root\n...\n\nLEXICON TAM\n...\n\nLEXICON CLITICS\n...\n```\n\n\n```{lexd}\nPATTERNS\nVerbRoot\nVerbRoot TAM\nVerbRoot TAM CLITICS\n\nLEXICON Root\n...\n\nLEXICON TAM\n...\n\nLEXICON CLITICS\n...\n```\n\n\n\n\n\n2.2.2 Анонимные разделы\nНекоторые фрагменты аннотации можно вставлять прямо в раздел PATTERNS. Для этого используются квадртные скобки.\n\nсокращенный вариантразвернутый вариант 1развернутый вариант 2\n\n\n```{lexd}\nPATTERNS\nNounStem [&lt;n&gt;:] NounNumber\n\nLEXICON NounStem\nsock\nninja\n\nLEXICON NounNumber\n&lt;sg&gt;:\n&lt;pl&gt;:s\n```\n\n\n```{lexd}\nPATTERNS\nNounStem NounNumber\n\nLEXICON NounStem\nsock&lt;n&gt;:sock\nninja&lt;n&gt;:ninja\n\nLEXICON NounNumber\n&lt;sg&gt;:\n&lt;pl&gt;:s\n```\n\n\n```{lexd}\nPATTERNS\nNounStem NounTag NounNumber\n\n1LEXICON NounTag\n&lt;n&gt;:\n\nLEXICON NounStem\nsock\nninja\n\nLEXICON NounNumber\n&lt;sg&gt;:\n&lt;pl&gt;:s\n```\n\n1\n\nНовый раздел LEXICON.\n\n\n\n\n\nВ мануале это трюк назван Anonymous LEXICON, видимо, потому что предполагается, что мы таким образом избегаем создания дополнительного раздела LEXICON (см. развернутый пример 2).\nПо аналогии с анонимным разделом LEXICON есть анонимный раздел PATTERN. Для этого используются круглые скобки.\n\nсокращенный вариантразвернутый вариант\n\n\n```{lexd}\nPATTERNS\n(VerbRoot Causative?) | AuxRoot Tense PersonNumber\n\nLEXICON VerbRoot\n...\n\nLEXICON Causative\n...\n\nLEXICON AuxRoot\n...\n\nLEXICON Tense \n...\n\nLEXICON PersonNumber\n...\n```\n\n\n```{lexd}\nPATTERNS\nVerbStem | AuxRoot Tense PersonNumber\n\n1PATTERN VerbStem\nVerbRoot Causative?\n\nLEXICON VerbRoot\n...\n\nLEXICON Causative\n...\n\nLEXICON AuxRoot\n...\n\nLEXICON Tense \n...\n\nLEXICON PersonNumber\n...\n```\n\n1\n\nНовый раздел PATTERN.\n\n\n\n\n\n\n\n2.2.3 Теги\nНа содержимое разделов LEXICON можно вешать теги. Это может быть полезно, например, для моделирования словоизменительных классов. Рассмотрим пример из русского языка (славянские, индоевропейские):\n```{lexd}\nPATTERNS\nNounStem[hard] Inflection[hard]\nNounStem[soft] Inflection[soft]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян[soft]\nТаня:Тан[soft]\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я[soft]\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и[soft]\n```\nТо же самое можно записать при помощи одного тега, используя операцию отмены тега:\n```{lexd}\nPATTERNS\nNounStem[hard] Inflection[hard]\nNounStem[-hard] Inflection[-hard]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян\nбуря:бур\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и\n```\nОднако в русском языке можно найти аффиксы, которые присоединяются к обоим типам основ, в таком случае, придется усложнить наше описание:\n```{lexd}\nPATTERNS\nNounStem[hard] Inflection[hard]\nNounStem[soft] Inflection[soft]\nNounStem Inflection[-hard,-soft]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян[soft]\nТаня:Тан[soft]\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я[soft]\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и[soft]\n&lt;pos&gt;:ин\n```\nАвторы lexd добавили возможность взаимодействия тегов, чтобы не надо было писать одно и то же.\n\n(A B)[x]  = (A[x] B) | (A B[x])\n\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\n(A B)[x]\n\nLEXICON A\naaa[x]\nbbb\n\nLEXICON B\nAAA[x]\nBBB\n```\n\n\naaaAAA\naaaBBB\nbbbAAA\n\n\n\n\n(A B)[-x] = A[-x] B[-x]\n\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\n(A B)[-x]\n\nLEXICON A\naaa[x]\nbbb\n\nLEXICON B\nAAA[x]\nBBB\n```\n\n\nbbbBBB\n\n\n\n\nA[|[x,y]] = A[x] | A[y]\n\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nA[|[x,y]]\n\nLEXICON A\naaa[x]\nbbb[y]\nccc\n```\n\n\naaa\nbbb\n\n\n\n\nA[^[x,y]] = A[x,-y] | A[-x,y]\n\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nA[^[x,y]]\n\nLEXICON A\naaa[x]\nbbb[y]\nccc[z]\nddd[x,y]\neee[x,z]\nfff[y,z]\nggg\n```\n\n\naaa\neee\nbbb\nfff \n\n\n\n^ — очень полезный оператор, который позволяет смоделировать согласование по признакам\n(A B)[^[x,y]] = (A[x,-y] B[x,-y]) | (A[-x,y] B[-x, y]):\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\n(A B)[^[x,y]]\n\nLEXICON A\naaa[x]\nbbb[y]\nccc\n\nLEXICON B\nAAA[x]\nBBB[y]\nCCC\n```\n\n\naaaAAA\naaaCCC\ncccAAA\ncccBBB\nbbbCCC\nbbbBBB\n\n\n\nЭто позволяет смоделировать наш русский пример одной строчкой:\n\nс операторомстарый вариант\n\n\n```{lexd}\nPATTERNS\n(NounStem Inflection)[^[hard,soft]]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян[soft]\nТаня:Тан[soft]\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я[soft]\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и[soft]\n&lt;pos&gt;:ин\n```\n\n\n```{lexd}\nPATTERNS\nNounStem[hard] Inflection[hard]\nNounStem[soft] Inflection[soft]\nNounStem Inflection[-hard,-soft]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян[soft]\nТаня:Тан[soft]\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я[soft]\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и[soft]\n&lt;pos&gt;:ин\n```\n\n\n\n\n\n\n\n\n\nЗадание 02_03\n\n\n\nНиже представлен фрагмент финской (фино-угорские, уральские) парадигмы (Karlsson 2013), как видно, здесь слова двух словоизменительных типов. Попробуйте смоделировать представленную парадигму, используя теги.\n\n\n\nglosses\nзал\nобщежитие\nдверь\nзима\n\n\n\n\nnom.sg\nsali\nhostelli\novi\ntalvi\n\n\nacc.sg\nsali\nhostelli\novi\ntalvi\n\n\ngen.sg\nsalin\nhostellin\noven\ntalven\n\n\nprt.sg\nsalia\nhostellia\novea\ntalvea\n\n\nin.ess.sg\nsalissa\nhostellissa\novessa\ntalvessa\n\n\nin.abl.sg\nsalista\nhostellista\novesta\ntalvesta\n\n\nat.ess.sg\nsalilla\nhostellilla\novella\ntalvella\n\n\nat.abl.sg\nsalilta\nhostellilta\novelta\ntalvelta\n\n\nat.all.sg\nsalille\nhostellille\novelle\ntalvelle\n\n\nfrml.sg\nsalina\nhostellina\novena\ntalvena\n\n\ntrans.sg\nsaliksi\nhostelliksi\noveksi\ntalveksi\n\n\npriv.sg\nsalitta\nhostellitta\novetta\ntalvetta\n\n\n\n\n\n\n\n2.2.4 Моделирование разрывных морфем: инфиксы, редупликация, семитские корни\nВ разеделе PATTERNS можно перечислять разные стороны единиц (или входное и выходное значение), записанных в LEXICON2. Это позволяет:\n\nопускать либо глоссы, либо морфемы (полезно для моделирования редупликации);\nиметь разный порядок глосс и морфем (но зачем?).\n\nВот пример моделирования дистрибутивных числительных (т. е. числительных со значением ‘по Х’) в зиловском андийском (андийские, нахско-дагестанские):\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nNumerals NumeralMarker\nNumeralReduplication :Numerals Numerals NumeralMarker NumeralDistributiveMarker\n\nLEXICON Numerals\nчIе               # числительное 2\nлъоб              # числительное 3\nойлIи             # числительное 6\n\nLEXICON NumeralMarker\n&lt;num&gt;:гу\n\nLEXICON NumeralDistributiveMarker\n&lt;distr&gt;:\n\nLEXICON NumeralReduplication\n&lt;rdp&gt;:\n```\n\n\nчIе&lt;num&gt;:чIегу\nлъоб&lt;num&gt;:лъобгу\nойлIи&lt;num&gt;:ойлIигу\n&lt;rdp&gt;чIе&lt;num&gt;&lt;distr&gt;:чIечIегу\n&lt;rdp&gt;лъоб&lt;num&gt;&lt;distr&gt;:лъоблъобгу\n&lt;rdp&gt;ойлIи&lt;num&gt;&lt;distr&gt;:ойлIиойлIигу\n\n\n\nКроме того для моделирования разрывных морфем, вводится номер в круглых скобках, а элементы морфемы перечисляются через пробел. Вот пример, из иврита (симитские, афразийские):\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nC(1) V(1) C(2) :V(2) C(3) V(2):\n\nLEXICON C(3)\nsh m r          # соблюдать, защищать\ny sh v          # садиться\n\nLEXICON V(2)\n:a &lt;v&gt;&lt;p3&gt;&lt;sg&gt;:a\n:o &lt;v&gt;&lt;pprs&gt;:e\n```\n\n\nshmr&lt;v&gt;&lt;p3&gt;&lt;sg&gt;:shamar\nshmr&lt;v&gt;&lt;pprs&gt;:shomer\nyshb&lt;v&gt;&lt;p3&gt;&lt;sg&gt;:yashab\nyshb&lt;v&gt;&lt;pprs&gt;:yosheb\n\n\n\n\n\n\n\n\n\nЗадание 02_04\n\n\n\nНиже даны количественные, кратные (со значение ‘Х раз’) и дистрибутивные (со значение ‘по Х’) числительные адыгейского языка (абхазо-адыгские) (Рогава и Керашева 1966: 79, 81). Смоделируйте перечисленные формы:\n\n\n\n\n\n\n\n\n\n\nзначение\nлемма\nколичественные &lt;card&gt;\nкратные  &lt;adv&gt;\nдистрибутивные  &lt;distr&gt;\n\n\n\n\n1\nзы\nзы\nзэ\nзырыз\n\n\n3\nщы\nщы\nщэ\nщырыщ\n\n\n4\nплIы\nплIы\nплIэ\nплIырыплI\n\n\n5\nтфы\nтфы\nтфэ\nтфырытф\n\n\n6\nхы\nхы\nхэ\nхырых\n\n\n10\nпшIы\nпшIы\nпшIэ\nпшIырыпшI\n\n\n\n\n\n\n\n2.2.5 Регулярные выражения\nВ разделе LEXICON допускаются регулярные выражения, для этого их нужно обромлять косыми чертами /:\n\nгруппировка при помощи скобок ()\nквантификация при помощи ?, *, и +. Объект квантификации должен быть обрамлен круглыми скобками.\nлогическое ‘или’ |\nгруппы символов при помощи квадратных скобок []\nпромежутки символов [a-z]\n\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nSomeLexicon\n\nLEXICON SomeLexicon\n/x(y|zz)?[n-p]/\n```\n\n\nxyn\nxyo\nxyp\nxzzn\nxzzo\nxzzp\nxn\nxo\nxp\n\n\n\nЭто позволяет добавлять разбор неизвестных единиц в lexd. Важно отметить, что добавление некоторых типов регулярных выражение делает циклический трансдьюсер, поэтому результат такого трансдьюссера можно посмотреть только при помощи команд make analysis FORM=\"...\" или make generation FORM=\"...\".\n\nlexdрезультат\n\n\n```{lexd}\nPATTERNS\nStem Affix\n\nLEXICON Stem\nстол\nдом\n/([а-я])*/\n\nLEXICON Affix\n&lt;nom&gt;&lt;sg&gt;:\n&lt;gen&gt;&lt;sg&gt;:а\n&lt;acc&gt;&lt;sg&gt;:\n&lt;dat&gt;&lt;sg&gt;:у\n&lt;ins&gt;&lt;sg&gt;:ом\n&lt;loc&gt;&lt;sg&gt;:е\n```\n\n\n```{shell}\n$ make analysis FORM=\"комом\"\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; комом ком&lt;ins&gt;&lt;sg&gt;    0,000000\nкомом   комом&lt;acc&gt;&lt;sg&gt;  0,000000\nкомом   комом&lt;nom&gt;&lt;sg&gt;  0,000000\n```\n\n\n\n\n\n\n\nKarlsson, F. 2013. Finnish: An essential grammar. Routledge.\n\n\nLindén, K., E. Axelson, S. Hardwick, T. A. Pirinen, и M. Silfverberg. 2011. «Hfst—framework for compiling and applying morphologies». В Systems and Frameworks for Computational Morphology: Second International Workshop, SFCM 2011, Zurich, Switzerland, August 26, 2011. Proceedings 2, 67–85. Springer.\n\n\nOrtiz Rojas, S., M. L. Forcada, и G. Ramı́rez Sánchez. 2005. «Construcción y minimización eficiente de transductores de letras a partir de diccionarios con paradigmas». Procesamiento del lenguaje natural 35: 51–57.\n\n\nSumbatova, N. R., и R. O. Mutalov. 2003. A grammar of Itsari Dargwa. Muenchen: Lincom Europa.\n\n\nSwanson, D., и N. Howell. 2021. «Lexd: A finitestate lexicon compiler for non-suffixational morphologies». В Multilingual Facilitation, 133–46.\n\n\nРогава, Г. В., и З. И. Керашева. 1966. Грамматика адыгейского языка. Майкоп, Краснодар: Краснодарское книж. издательство.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Введение в `lexd`: морфология</span>"
    ]
  },
  {
    "objectID": "02_introduction_to_lexd.html#footnotes",
    "href": "02_introduction_to_lexd.html#footnotes",
    "title": "2  Введение в lexd: морфология",
    "section": "",
    "text": "Мы какое-то время думали, зачем это может быть нужно и придумали только странные сценарии типа пра-пра-пра-пра-бабушка. Но вообще это порождает циклы, от которых одни проблемы.↩︎\nК сожалению, нельзя делать аналогичное для единиц из разделов PATTERN.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Введение в `lexd`: морфология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html",
    "href": "03_phonological_rules.html",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "",
    "text": "3.1 Фонологическое введение\nПреобладающая фонологическая теория в XX веке — генеративная фонология (Chomsky и Halle 1968). Согласно этой теории существует два представления: глубинное (underlying/phonological representation) и поверхностное (surface form, phonetic representation). Фонология в этой теории сводится к набору линейно упорядоченных правил, которые применяются циклически, преобразуя результат работы синтаксической деревации в фонетические цепочки.\nИз-за того, что правила в этой теории строго упорядочены возникают случаи, когда правила взаимодействуют друг с другом. Классификация таких случаев приводится в работе (Kiparsky 1982 (1968)):\nВот комиксы, которые по нашей задумке должны дополнительно иллюстрировать разницу между разными порядками.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#фонологическое-введение",
    "href": "03_phonological_rules.html#фонологическое-введение",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "",
    "text": "глубинное представление &gt; фонологическое правило 1 &gt; фонологическое правило 2 &gt; … &gt; поверхностное представление.\n\n\n\nпитающий порядок (feeding). Так называют порядок, при котором применение одного правила увеличивает количество контекстов применение другого правила, так что другое правило срабатывает. 1\nблокирующий порядок (bleeding). Так называют порядок, при котором применение одного правила уменьшает количество контекстов применения другого правила, так что другое правило не срабатывает. 2\nпротивопитающий порядок (counterfeeding). Так называют порядок, при котором применение одного правила увеличивает количество контекстов применение другого правила, однако другое правило не срабатывает.\nпротивоблокирующий порядок (counterbleeding). Так называют порядок, при котором применение одного правила уменьшает количество контекстов применения другого правила, однако другое правило все равно срабатывает.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#двухуровневая-фонологияморфология",
    "href": "03_phonological_rules.html#двухуровневая-фонологияморфология",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "3.2 Двухуровневая фонология/морфология",
    "text": "3.2 Двухуровневая фонология/морфология\nДвухуровневая фонология/морфология (two level morphology) была разработана в диссертации (Koskenniemi 1983). Стоит отметить, что мы используем данный формализм для моделирования (мор)фонологических правил, однако данный формализм называют двухуровневой морфологией (в том числе и сам автор). Вообще, еще в 1972 вышла диссертация (Johnson 1972), в которой автор указывал на некоторые недостатки последовательности фонологических правил, которые были приняты в генеративной фонологии, а также доказывал, что любую последовательность правил можно моделировать при помощи трансдьюсера, однако эта работа осталась незамеченной.\nВ рамках двухуровневой фонологии/морфологии:\n\nправила — посимвольные ограничения на поверхностное представление, которые применяются параллельно.\nправила могут оперировать единицами глубинного (другое название — лексическое) представления, поверхностного представления или одновременно обоих.\n\nНапример, получить из глубинной формы spy&gt;s поверхностную форму spies можно двумя правилами (нотацию мы подробнее обсудим позже):\n\ny:i &lt;=&gt; _ 0:e\n0:e &lt;=&gt; y: _ %&gt;:0\n\nПервое правило обращается одновременно к глубинному (0:) и поверхностному (:e) представлениям. Второе правило обращается только к глубинному представлению (y:) и поверхностному (%&gt;:0) представлениям.\nИспользование ограничений, вместо правил, чуть позже возникла в фонологии в виде Теории оптимальности (OT, (Prince и Smolensky 1994)), однако в рамках OT предпалагаются, что ограничения носят универсальный характер и есть во всех языках, в то время, как ограничения двухуровневой фонологии/морфологии — имеют частный внутриязыковой характер.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#программа-twol",
    "href": "03_phonological_rules.html#программа-twol",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "3.3 Программа twol",
    "text": "3.3 Программа twol\nВ данном разделе мы будем обсуждать синтаксис twol. Данный раздел основан на (Beesley и Karttunen 2003). Кроме того Элен Картина обратила мое внимание на пакет twol для Python, разработанный Киммо Коскенниэми, автором twol.\n\n3.3.1 Техническое введение\nМы будем использовать программу hfst-twolc. Чтобы избежать сложностей на начальных этапах курса, мы решили вначале познакомиться с синтаксисом twol и попробовать описывать разные языковые явления, не затрудняя всех установкой и запуском нужных программ у себя на компьютере.\n\nДля начала работы следует, как и раньше, скачать Makefile:\n\n```{shell}\n$ curl -s https://raw.githubusercontent.com/agricolamz/2026_morphological_transducers/refs/heads/main/task_tests/Makefile -o Makefile; make\n```\n\nдальше, как и раньше, следует создать в колабе или у себя на компьютере (если у вас Linux), файл с названием task.lexd. В Google Colab для этого достаточно вставить первой строкой кодового блока %%writefile task.lexd. Вот пример такого файла:\n\n```{lexd}\nPATTERNS\nNoun (Suffix[-adj] | (Suffix[adj] Inflection))?\n\nLEXICON Noun\nночь\nпечь\n\nLEXICON Suffix\n&lt;dim&gt;:ка\n&lt;adj&gt;:н[adj]\n\nLEXICON Inflection\n&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ой\n```\n\nнововведением является возможность создания файла с названием task.twol, в котором будет содержаться код для обработки (мор)фонологии. Не забудьте вставить %%writefile task.twol в Google Colab. Вот пример такого файла:\n\n```{twol}\nAlphabet\n  а е й к н о п ч ь ь:0;\n\nRules\n\n\"чк чн пишется без ь\"\n! например, ночьной -&gt; ночной или печька -&gt; печка\n\nь:0 &lt;=&gt; _ к;\n        _ н;\n```\n\nПосле того, как вы установили нужные программы и создали файлы, как и раньше, можно посмотреть формы и разборы, которые генерируются трансдьюсером (не забудьте поставить восклицательный знак перед make в Google Colab):\n\n```{shell}\n$ make forms\n\nночь&lt;dim&gt;:ночка\nночь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночной\nночь\nпечь&lt;dim&gt;:печка\nпечь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печной\nпечь\n```\n\nКроме того можно посмотреть анализ/генерацию конкретных форм (не забудьте поставить восклицательный знак перед make в Google Colab):\n\n```{shell}\n$ make analysis FORM=\"печка\"\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; печка печь&lt;dim&gt;   0,000000\n```\n```{shell}\n$ make generation FORM=\"ночь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\"\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; ночь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt; ночной  0,000000\n```\n\nКак и раньше, получившийся трансдьюсер можно протестировать:\n\n```{shell}\n$ make test_03_01\n```\n\nК сожалению, сейчас, если была получена ошибка из программы twol следует удалить лишние созданные файлы перед тем как приступить к исправлению:\n\n```{shell}\n$ make clean\n```\n\n\n3.3.2 Структура .twol файла\nКаждый .twol файл состоит из нескольких блоков, которые удобно обсуждать на приведенном выше примере:\n```{twol}\n1Alphabet\n2  а е й к н о п ч ь ь:0;\n\n3Rules\n\n4\"чк чн пишется без ь\"\n5! например, ночьной -&gt; ночной или печька -&gt; печка\n\n6ь:0 &lt;=&gt; _ к;\n7        _ н;\n```\n\n1\n\nРаздел, которые перечисляет все используемые символы.\n\n2\n\nДекларацией конца раздела является точка с запятой, поэтому иногда удобно писать элементы в несколько строчек, особенно если есть некотрые логические блоки. Из примера видно сокращение: одним символом можно записывать идентичные пары входного и выходного символов (т. е. й:й можно записывать как й).\n\n3\n\nРаздел с правилами.\n\n4\n\nНазвание правила (паразительно, но это обязательный элемент 3).\n\n5\n\nВ любом месте файла можно написать комментарий. Начало комментария обозначается восклицательным знаком.\n\n6\n\nСамо правило. Оно обычно имеет следующие элементы: какая-то пара входного/выходного символа (ь:0), дальше оператор (&lt;=&gt;), дальше контекст (_ к), все это заканчивается точкой с запятой.\n\n7\n\nНесколько контекстов для одного правила можно записывать вместе. На всякий случай — отбивка не важна, просто сделал для удобочитаемости.\n\n\n\n\n3.3.3 Раздел алфавита\nХорошо бы, чтобы раздел алфавита содержал все пары символов (по-английски, symbol pairs или feasible pairs) глубинного и поверхностного представлений, которые представлены в вашем материале. Важно отметить, что twol достраивает пары символов из правил, так что программа будет работать и с неполным алфавитом .twol. Для нашего примера достаточно следующего файла:\n```{twol}\nAlphabet\n  ь;\n\nRules\n\n\"чк чн пишется без ь\"\n\nь:0 &lt;=&gt; _ к;\n        _ н;\n```\nТак как под капотом происходит композиция трансдьюсеров (см. Раздел 1.4.2), то символы, которые не участвуют в правилах twol, не будут затронуты. И, единственное, что twol не может восстановить, что у нас есть две сущности: ь:0 и ь:ь.\nОднако все это из разряда вредных советов. Если вы моделируете язык или даже его фрагмент, лучше перечислить все символы (и их прописные варианты). Это позволит избежать проблем при анализе некачественных входных данных.\n\n\n3.3.4 Обращение к разным представлениям\nЕдинцы, которыми манипулирует twol — пары глубинного и поверхносного представления. Поэтому в правилах могут появлятся\n\nполностью определенные единицы, например, а:ы или ь:0\nединицы, определенные только на одном из представлений\n\nглубинном, например, ы:\nповерхностном, например, :я\n\nplaceholder со значением любой знак: : (еще можно ?)\n\n\n\n3.3.5 Архифонемы\nИногда (мор)фонологические проблемы можно решать, не постулируя некоторую конкретную сегментную глубинную форму, а используя, так называемые архифонемы4. Эти единицы записываются в lexd в фигурных скобках, например {А}. Существует конвенция, что если архифонема позже только удаляется, то ее пишут строчными буквами, например, {s}, а если архифонема представляет собой набор поверхностных форм, то ее пишут заглавными буквами, например, {А}. Важно отметить, что в файле .twol фигурные скобки нужно экранировать при помощи знака процента %. Для примера давайте перемоделируем пример из прошлого раздела:\n\nlexdtwolдо применения twolпосле применения twolстарый файл lexd\n\n\n```{lexd}\nPATTERNS\nNounStem Inflection\n\nLEXICON NounStem\nмама:мам\nпапа:пап\nняня:нян{j}\nТаня:Тан{j}\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:{А}\n&lt;gen&gt;&lt;sg&gt;:{Ы}\n&lt;pos&gt;:ин\n```\n\n\n```{twol}\nAlphabet\n  а и м н п я Т\n  %{А%}:а %{А%}:я %{Ы%}:ы %{Ы%}:и %{j%}:0;\n\nRules\n\n\"удаляем маркер мягкой основы\" \n\n%{j%}:0 &lt;=&gt; _ ;\n\n\"гласные после мягкой основы в именительном\" \n\n%{А%}:я &lt;=&gt; %{j%}:0 _ ;\n\n\"гласные после мягкой основы в родительном\" \n\n%{Ы%}:и &lt;=&gt; %{j%}:0 _ ;\n```\n\n\nмама&lt;nom&gt;&lt;sg&gt;:мам{А}\nмама&lt;gen&gt;&lt;sg&gt;:мам{Ы}\nмама&lt;pos&gt;:мамин\nпапа&lt;nom&gt;&lt;sg&gt;:пап{А}\nпапа&lt;gen&gt;&lt;sg&gt;:пап{Ы}\nпапа&lt;pos&gt;:папин\nняня&lt;nom&gt;&lt;sg&gt;:нян{j}{А}\nняня&lt;gen&gt;&lt;sg&gt;:нян{j}{Ы}\nняня&lt;pos&gt;:нян{j}ин\nТаня&lt;nom&gt;&lt;sg&gt;:Тан{j}{А}\nТаня&lt;gen&gt;&lt;sg&gt;:Тан{j}{Ы}\nТаня&lt;pos&gt;:Тан{j}ин  \n\n\nмама&lt;nom&gt;&lt;sg&gt;:мама\nмама&lt;gen&gt;&lt;sg&gt;:мамы\nмама&lt;pos&gt;:мамин\nпапа&lt;nom&gt;&lt;sg&gt;:папа\nпапа&lt;gen&gt;&lt;sg&gt;:папы\nпапа&lt;pos&gt;:папин \nняня&lt;nom&gt;&lt;sg&gt;:няня\nняня&lt;gen&gt;&lt;sg&gt;:няни\nняня&lt;pos&gt;:нянин\nТаня&lt;nom&gt;&lt;sg&gt;:Таня\nТаня&lt;gen&gt;&lt;sg&gt;:Тани\nТаня&lt;pos&gt;:Танин\n\n\n```{lexd}\nPATTERNS\nNounStem[hard] Inflection[hard]\nNounStem[soft] Inflection[soft]\nNounStem Inflection[-hard,-soft]\n\nLEXICON NounStem\nмама:мам[hard]\nпапа:пап[hard]\nняня:нян[soft]\nТаня:Тан[soft]\n\nLEXICON Inflection\n&lt;nom&gt;&lt;sg&gt;:а[hard]\n&lt;nom&gt;&lt;sg&gt;:я[soft]\n&lt;gen&gt;&lt;sg&gt;:ы[hard]\n&lt;gen&gt;&lt;sg&gt;:и[soft]\n&lt;pos&gt;:ин\n```\n\n\n\nСледует обратить внимание, на то, что мы не писали правил для пар {А}:а и {Ы}:ы. twol сам использовал указанные нами переходы во всех контекстах, не специализированных правилом перехода после мягких основ. Это поведение напрямую зависит от оператора, используемого в правиле (см. раздел Раздел 3.3.8).\nАрхифонемы позволяют моделировать морфонологические правила, так как мы всегда можем вставить единицу {some_name}:0, которая показывает на важную морфонологическую границу, рядом с которой что-то происходит (см. еще Раздел 3.4).\n\n\n\n\n\n\nЗадание 03_02\n\n\n\nВ работе (Иткин 2007: 118) описано чередование н~н’ в русском языке. Попробуйте заполнить раздел Rules в файле .twol, чтобы получился трансдьюсер, моделирующий следующие формы:\n\n\n\n&lt;nom&gt;&lt;sg&gt;\n&lt;dat&gt;&lt;sg&gt;\n&lt;ins&gt;&lt;pl&gt;\n&lt;gen&gt;&lt;pl&gt;\n\n\n\n\nбашня\nбашней\nбашнями\nбашен\n\n\nпесня\nпесней\nпеснями\nпесен\n\n\nбойня\nбойней\nбойнями\nбоен\n\n\nдеревня\nдеревней\nдеревнями\nдеревень\n\n\nкухня\nкухней\nкухнями\nкухонь\n\n\nТаня\nТаней\nТанями\nТань\n\n\n\n\nlexdtwol\n\n\n```{lexd}\nPATTERNS\n(NounRoot NounInfl)[^[hard,soft]]\n\nLEXICON NounRoot\nбашня&lt;n&gt;:баш{Е}н[hard]\nпесня&lt;n&gt;:пес{Е}н[hard]\nбойня&lt;n&gt;:бой{Е}н[hard]\nдеревня&lt;n&gt;:дерев{Е}н[soft]\nкухня&lt;n&gt;:кух{О}н[soft]\nТаня&lt;n&gt;:Тан[soft]\n\nLEXICON NounInfl\n&lt;nom&gt;&lt;sg&gt;:я\n&lt;dat&gt;&lt;sg&gt;:ей\n&lt;dat&gt;&lt;pl&gt;:ями\n&lt;gen&gt;&lt;pl&gt;:{GEN.PL}[hard]\n&lt;gen&gt;&lt;pl&gt;:{GEN.PL}ь[soft]\n```\n\n\n```{twol}\nAlphabet\n    б ш н п с й д р в к х Т м ь\n    а я е у и о\n    %{Е%}:0 %{О%}:0 %{GEN.PL%}:0 \n    %{Е%}:е %{О%}:о\n    й:0\n;\n\nRules\n\n```\n\n\n\n\n\n\n\n3.3.6 Разделы Sets и Definitions\nРаздел Sets — опциональный раздел .twol файла, в котором можно создать группы символов, к которым потом можно обращаться в правилах по имени. Рассмотрим пример моделирования устранение зияния гласных в адыгейском языке (Аркадьев и др. 2009: 27–28):\n\n\n\n\n&lt;abs&gt;\n‘это не X’  &lt;neg&gt;\n‘это X?’  \n\n\n\n\nженщина\nшъуз\nшъузэп\nшъуза\n\n\nмужчина\nлIы\nлIэп\nлIа\n\n\nдом\nунэ\nунэп\nуна\n\n\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot Infl\n\nLEXICON Root\nшъуз\nлIы\nунэ\n\nLEXICON Infl\n&lt;abs&gt;:\n&lt;neg&gt;:эп\n&lt;q&gt;:а\n```\n\n\n```{twol}\nAlphabet\n    I а з л н п у ш ъ ы э\n    а:0 ы:0 э:0;\n    \nSets\n    Vowels = а ы э;\n\nRules\n\n\"устранение зияния гласных\"\nVowels:0 &lt;=&gt; _ Vowels;\n```\n\n\nлIы&lt;q&gt;:лIа\nлIы&lt;neg&gt;:лIэп\nлIы&lt;abs&gt;:лIы\nунэ&lt;q&gt;:уна\nунэ&lt;neg&gt;:унэп\nунэ&lt;abs&gt;:унэ\nшъуз&lt;abs&gt;:шъуз\nшъуз&lt;q&gt;:шъуза\nшъуз&lt;neg&gt;:шъузэп\n\n\n\nОбратим внимание, что Vowels:0 в рамках нашей задачи можно было бы записать экономичнее :0 (см. ниже раздел Раздел 3.3.4). Однако, такое правило при моделировании большего фрагмента грамматики скорее всего может привести к непредвиденным эффектам. Тем более, что такая запись плохо читаема вне контекста.\nОпределение множеств может содержать имена множеств, определенных ранее. Все перечисленные в множествах единицы должны быть определены в разделе Alphabet.\nОпределяемые элементы в разделе Sets не могут иметь :. Раскрытие переменных происходит по обычным правилам: если после или до имени переменной не стоит двоеточия, значит он будет раскрываться подобными симвалами (т. е. а перейдет в а:а и т. д.). В нашем примере переменная Vowels в файле .twol раскрылась всеми возможными способами: а – а, а – ы, а – э, ы – ы, ы – э, э – э. Т. е. каждое употребление переменной\nРаздел Definitions — опциональный раздел .twol файла, в котором можно давать некоторым левым или правым фрагментам контекста имена, к которым потом можно обращаться в правилах. Это осмысленно, если один и тот же контекст встречается в разных правилах, пример из (Beesley и Karttunen 2003: 29)\n```{twol}\n...\nDefinitions\n\n  XContext = [ p | t | k | g:k ] ;\n  YContext = [ m | n | n g ] ;\n  \n...\n```\nКроме того, внутри правил можно создавать свои переменные. Так адыгейский трансдьюсер для устранения зияния гласных можно переписать, опеределяя переменную прямо в правиле, а не в разделе Sets:\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot Infl\n\nLEXICON Root\nшъуз\nлIы\nунэ\n\nLEXICON Infl\n&lt;abs&gt;:\n&lt;neg&gt;:эп\n&lt;q&gt;:а\n```\n\n\n```{twol}\nAlphabet\n    I а з л н п у ш ъ ы э\n    а:0 ы:0 э:0;\n\nRules\n\n\"устранение зияния гласных\"\nVowels:0 &lt;=&gt; _ [а | ы | э];\n    where Vowels in (а ы э);\n```\n\n\nлIы&lt;q&gt;:лIа\nлIы&lt;neg&gt;:лIэп\nлIы&lt;abs&gt;:лIы\nунэ&lt;q&gt;:уна\nунэ&lt;neg&gt;:унэп\nунэ&lt;abs&gt;:унэ\nшъуз&lt;abs&gt;:шъуз\nшъуз&lt;q&gt;:шъуза\nшъуз&lt;neg&gt;:шъузэп\n\n\n\nКажется, что правило можно было бы еще упростить и использовать переменную дважды:\nVowels:0 &lt;=&gt; _ Vowels;\n    where Vowels in (а ы э);\nК сожалению, это работает неправильно:\nлIы&lt;abs&gt;:лIы\nлIы&lt;q&gt;:лIыа\nлIы&lt;neg&gt;:лIыэп\nунэ&lt;neg&gt;:унэп\nунэ&lt;abs&gt;:унэ\nунэ&lt;q&gt;:унэа\nшъуз&lt;abs&gt;:шъуз\nшъуз&lt;q&gt;:шъуза\nшъуз&lt;neg&gt;:шъузэп \nДело в том, что программа расписывает такое правило следующим образом:\nа:0 &lt;=&gt; _ а;\nы:0 &lt;=&gt; _ ы;\nэ:0 &lt;=&gt; _ э;\nА для корректной работы, нужно следующее расписывание:\nа:0 &lt;=&gt; _ [а ы э];\nы:0 &lt;=&gt; _ [а ы э];\nэ:0 &lt;=&gt; _ [а ы э];\nИнтересно, почему это работало в случае, если определить переменную в разделе Sets.\nПеременных может быть несколько, и после их определения значения у созданных переменных можно соединить, используя аргумент matched. Рассмотрим это на примере палатализации в польском (индоевропейские, славянские):\n\n\n\n\nпризнак\nнога\nрука\n\n\n\n\n&lt;nom&gt;&lt;sg&gt;\ncecha\nnoga\nręka\n\n\n&lt;acc&gt;&lt;sg&gt;\ncechę\nnogę\nrękę\n\n\n&lt;dat&gt;&lt;sg&gt;\ncesze\nnodze\nręce\n\n\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot Infl\n\nLEXICON Root\ncecha&lt;n&gt;:cech\nnoga&lt;n&gt;:nog\nręka&lt;n&gt;:ręk\n\nLEXICON Infl\n&lt;nom&gt;&lt;sg&gt;:a\n&lt;acc&gt;&lt;sg&gt;:ę\n&lt;dat&gt;&lt;sg&gt;:{dat.sg}e\n```\n\n\n```{twol}\nAlphabet\n    a c h e n o g r ę k\n    k:c g:dz h:sz c:0\n    %{dat.sg%}:0;\n\nRules\n\n\"палатализация\"\nC:Cpal &lt;=&gt; _ %{dat.sg%}:0;\n    where C in (k g h)\n          Cpal in (c dz sz)\n          matched ;\n\n\"починка диграфа ch -&gt; sz\"\nc:0 &lt;=&gt; _ h:sz;\n\n\"гиперфонема для дательного\"\n%{dat.sg%}:0 &lt;=&gt; _;\n```\n\n\ncecha&lt;n&gt;&lt;dat&gt;&lt;sg&gt;:cesze\ncecha&lt;n&gt;&lt;nom&gt;&lt;sg&gt;:cecha\ncecha&lt;n&gt;&lt;acc&gt;&lt;sg&gt;:cechę\nnoga&lt;n&gt;&lt;nom&gt;&lt;sg&gt;:noga\nnoga&lt;n&gt;&lt;acc&gt;&lt;sg&gt;:nogę\nnoga&lt;n&gt;&lt;dat&gt;&lt;sg&gt;:nodze\nręka&lt;n&gt;&lt;nom&gt;&lt;sg&gt;:ręka\nręka&lt;n&gt;&lt;acc&gt;&lt;sg&gt;:rękę\nręka&lt;n&gt;&lt;dat&gt;&lt;sg&gt;:ręce\n\n\n\nВ файле .twol в правиле палатализация были созданы две переменные C и Cpal, которые были определены ниже. Также в этом .twol файле есть пример соответствия одному входному символу многосимвольный выход (g:dz). К сожалению, нельзя определять многосимвольные входные символы кроме как через гиперфонемы. matched в правиле означает, не свободное определение переменных, а определенные отношения, если определено несколько переменных: первый элемент одной переменной соотносится с первым элементом второй переменной и т. д. Если matched убрать, то программа будет генерировать все возможные комбинации.\n\n\n3.3.7 Операторы и другое\n\n.#. — означает границу слова (как левую так, и правую)\nОператоры квантификации\n\nОператор ? — ноль или один раз\nОператор * — ноль и более раз\nОператор + — один и более раз\nОператор ^n — n раз\nОператор ^n,m — от n до m раз\n\n\nОператоры квантификации важны, так как позволяют задавать сложные контексты для моделирования дистантных отношений.\n\nОператор \\ работает как логическое не, например \\Vowels задает контекст для единц, не входящих в группу Vowels.\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot\n\nLEXICON Root\naa \nba \nla \nma \nxa \nya \nza\n```\n\n\n```{twol}\nAlphabet\n    a b l m x y z a:b;\n\nSets\n    X = x y z;\n\nRules\n\n\"change\"\na:b &lt;=&gt; \\X _;\n```\n\n\naa:bb\nba:bb\nxa      # есть в X\nya      # есть в X\nza      # есть в X\nla:lb\nma:mb\n\n\n\n\nОператор | имеет семантику ‘или’, что в данном случае будет работать как объединение множеств\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot\n\nLEXICON Root\naa \nba \nla \nma \nxa \nya \nza\n```\n\n\n```{twol}\nAlphabet\n    a b l m x y z a:b;\n\nSets\n    X = x y z;\n    Y =   y z l m;\n\nRules\n\n\"change\"\na:b &lt;=&gt; X | Y _;\n```\n\n\naa\nba\nxa:xb   # есть в X\nya:yb   # есть и в X, и Y\nza:zb   # есть и в X, и Y\nla:lb   # есть в Y\nma:mb   # есть в Y\n\n\n\nТак в одном из примеров выше для переходов после мягких основ существует одинаковый контекст, так что можно использовать объединение при помощи оператора | (квадратные скобки добавлены для удобочитаемости, работает и без них):\n\nс операторомбез оператора как выше\n\n\n```{twol}\nAlphabet\n  а и м н п я Т\n  %{А%}:а %{А%}:я %{Ы%}:ы %{Ы%}:и %{j%}:0;\n\nRules\n\n\"удаляем маркер мягкой основы\" \n\n%{j%}:0 &lt;=&gt; _ ;\n\n\"гласные после мягкой основы\" \n\n[ %{А%}:я | %{Ы%}:и ] &lt;=&gt; %{j%}:0 _ ;\n```\n\n\n```{twol}\nAlphabet\n  а и м н п я Т\n  %{А%}:а %{А%}:я %{Ы%}:ы %{Ы%}:и %{j%}:0;\n\nRules\n\n\"удаляем маркер мягкой основы\" \n\n%{j%}:0 &lt;=&gt; _ ;\n\n\"гласные после мягкой основы в номинативе\" \n\n%{А%}:я &lt;=&gt; %{j%}:0 _ ;\n\n\"гласные после мягкой основы в генетиве\" \n\n%{Ы%}:и &lt;=&gt; %{j%}:0 _ ;\n```\n\n\n\n\nОператор & имеет семантику ‘и’, что в данном случае будет вызывать пересечение множеств\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot\n\nLEXICON Root\naa \nba \nla \nma \nxa \nya \nza\n```\n\n\n```{twol}\nAlphabet\n    a b l m x y z a:b;\n\nSets\n    X = x y z;\n    Y =   y z l m;\n\nRules\n\n\"change\"\na:b &lt;=&gt; X & Y _;\n```\n\n\naa\nba\nxa      # есть в X\nya:yb   # есть и в X, и Y\nza:zb   # есть и в X, и Y\nla      # есть в Y\nma      # есть в Y\n\n\n\n\nОператор - имеет семантику ‘без’, что в данном случае будет вызывать разность множеств\n\n\nlexdtwolрезультат\n\n\n```{lexd}\nPATTERNS\nRoot\n\nLEXICON Root\naa \nba \nla \nma \nxa \nya \nza\n```\n\n\n```{twol}\nAlphabet\n    a b l m x y z a:b;\n\nSets\n    X = x y z;\n    Y =   y z l m;\n\nRules\n\n\"change\"\na:b &lt;=&gt; X - Y _;\n```\n\n\naa\nba\nxa:xb   # есть в X\nya      # есть и в X, и Y\nza      # есть и в X, и Y\nla      # есть в Y\nma      # есть в Y\n\n\n\n\n\n\n\n\n\nЗадание 03_03\n\n\n\nЗаполните раздел правил в файле .twol ниже, чтобы получившийся трансдьюсер моделировал образование сравнительной степени в английском (германские, индоевропейские), например, big - bigger:\n\nlexdtwol\n\n\n```{lexd}\nPATTERNS\nRoot Infl\n\nLEXICON Root\nbig\nhot\nsad\nthin\nwet\n# group 2\nbright\ncheap\ncold\ndeep\nhard\nhigh\nold\nrich\nsmall\nslick\nsoft\ntall\n\nLEXICON Infl\n&lt;comp&gt;:{comp}er\n```\n\n\n```{twol}\nAlphabet\n    a b c d e f g h i j k l m n o p q r s t u v w x y z\n    0:g 0:t 0:d 0:n\n    %{comp%}:0;\n\nSets\n    Vowels = a e i o u y;\n\nRules\n\n\"геминация\"\n\n\n\"гиперфонема для морфологии\"\n\n\n```\n\n\n\n\n\n\n\n3.3.8 Операторы правил\nСуществует 4 оператора правил, однако на практике вам чаще всего понадобиться только первый. В (Beesley и Karttunen 2003) даются следующие определения этим операторам:\n\na:b &lt;=&gt; l _ r ;\n\nPositive Reading 1: Если появляется пара a:b, то она должна быть в контексте l _ r.\nNegative Reading 1: Если пара a:b появляется вне контекста l _ r, программа выдает ошибку.\nPositive Reading 2: Если глубинное a появляется в контексте l _ r, тогда оно должно быть реализовано на поверхностном уровне как b.\nNegative Reading 2: Если глубинное a появляется в контексте l _ r и реализуется на поверхностном уровне чем-то кроме как b, программа выдает ошибку.\n\na:b =&gt; l _ r ;\n\nPositive Reading: Если появляется пара a:b, то она должна быть в контексте l _ r.\nNegative Reading: Если пара a:b появляется вне контекста l _ r, программа выдает ошибку.\n\na:b &lt;= l _ r ;\n\nPositive Reading: Если глубинное a появляется в контексте l _ r, тогда оно должно быть реализовано на поверхностном уровне как b.\nNegative Reading: Если глубинное a появляется в контексте l _ r и реализуется на поверхностном уровне чем-то кроме как b, программа выдает ошибку.\n\na:b /&lt;= l _ r ;\n\nPositive Reading: Глубинное а никогда не может быть реализовано на поверхностном уровне как b в контексте l _ r\nNegative Reading: Если глубинное а реализовано на поверхностном уровне как b в контексте l _ r, программа выдает ошибку.\n\n\nНиже приводится таблица из (Beesley и Karttunen 2003), которая иллюстрирует поведение операторов:\n\n\n\n\n\n\n\n\n\n\nоператор\nпример 1\nпример 2\nпример 3\nпример 4\n\n\n\n\na:b &lt;=&gt; l _ r;\nlar:lbr\nlar:lar\nlbr:lbr\nxay:xby\n\n\na:b =&gt; l _ r;\nlar:lbr\nlar:lar\nlbr:lbr\nxay:xby\n\n\na:b &lt;= l _ r;\nlar:lbr\nlar:lar\nlbr:lbr\nxay:xby\n\n\na:b /&lt;= l _ r;\nlar:lbr\nlar:lar\nlbr:lbr\nxay:xby\n\n\n\nВ большинстве случаев нужно использовать оператор &lt;=&gt;. Потребность в других операторах возникает только если есть конфликтующие правила.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#sec-morphonology",
    "href": "03_phonological_rules.html#sec-morphonology",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "3.4 Морфонология",
    "text": "3.4 Морфонология\nМорфонология может иметь разный вид в языке, в связи с чем, я предлагаю разные трюки для описания их в twol.\n\nморфонология связана с аффиксом: root-{morphonology}aff\nморфонология связана с определенными корнями:\n\nroot{morphonology}-aff\nили ввести косвенные основы в .lexd\n\nморфонология вызывается взаимодействием определенных корней и аффиксов (око - очи, но яблоко - яблоки):\n\nroot{morphonology}-{morphonology}aff\nили ввести новые единицы в .lexd\n\nуникальные лексемы/формы всегда можно словарно ввести в .lexd",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#глоссирование",
    "href": "03_phonological_rules.html#глоссирование",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "3.5 Глоссирование",
    "text": "3.5 Глоссирование\nВсе это время анализ, который мы получали, содержал исключительно грамматические теги без морфемных границ. Основной разработчик парадигмы правилового морфологического анализа — Apertium — скорее заинтересован в создании переводчиков, поэтому морфемные границы отходят на второй план. Однако ожидание большинства лингвистов все же заключается в том, что морфемные границы в анализе должны быть, так как для них нет другой мотивации вкладываться в работу над морфологическим анализатором, кроме как если у них есть множество неотглоссированных текстов, которые они не хотят глоссировать вручную.\nПолучается так:\n\nНа вход мы подаем текст на языке.\nНа выходе мы получаем множество пар, где каждый элемент это\n\nанализ с поморфемной разбивкой;\nматериал на языке с поморфемной разбивкой.\n\n\nОтметим, что морфологическая граница традиционно в таких случаях обозначается &gt;. Это не самый удобный для читаемости символ, но его ожидают разные другие полезные инструменты на основе трансдьюсеров. Если мы хотим разработать инструмент для глоссироваания, нам имеет смысл создать два похожих трансдьюсера:\n\nwalk&gt;s — walk&lt;v&gt;&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;\nwalks — walk&lt;v&gt;&gt;&lt;pres&gt;&lt;3&gt;&lt;sg&gt;\n\nОдин от другого отличается одним правилом в twol.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#мысли-в-сторону",
    "href": "03_phonological_rules.html#мысли-в-сторону",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "3.6 Мысли в сторону",
    "text": "3.6 Мысли в сторону\nВажно отметить, что twol отлично помогает моделировать переходы от глубинного представления в поверхностное, что помогает моделировать (мор)фонологию. Однако не стоит считать двухуровневую фонологию фонологической теорией. Она разрабатывалась прежде всего для практических нужд, а в таком случае все средства хороши. Правила двухуровневой фонологии часто слишком языкоцентричны, и поэтому сложно перейти от них к некоторым обобщениям. Кроме того, отказ от последовательности правил приводит к тому, что получившиеся правила двухуровневой фонологии часто неестественны, так как вынуждены включать в себя работу сразу всех (мор)фонологических процессов языка.\n\n\n\n\nBeesley, K. R., и L. Karttunen. 2003. «Two-level rule compiler».\n\n\nChomsky, N., и M. Halle. 1968. The sound pattern of English. New York, Evanstone, London: Haper & Row.\n\n\nJohnson, C. D. 1972. Formal aspects of phonological description. The Hague, Paris: Mouton.\n\n\nKiparsky, P. 1982 (1968). «Linguistic universals and linguistic change». В Explanation in Phonology, под редакцией P. Kiparsky. Dordrecht, Cinnaminson: Foris Publications.\n\n\nKoskenniemi, K. 1983. «Two-level morphology: A general computational model for word-form recognition and production». Phdthesis, University of Helsenki, Department of General Linguistics.\n\n\nPrince, A., и P. Smolensky. 1994. «Optimality Theory: Constraint interaction in generative grammar». Rutgers University, Piscataway, NJ., Rutgers Center for Cognitive Science.\n\n\nАркадьев, П. М., Ю. А. Ландер, А. Б. Летучий, Н. Р. Сумбатова, и Я. Г. Тестелец. 2009. «Введение. Основные сведения об адыгейском языке». В Аспекты полисинтетизма: очерки по грамматике адыгейского языка, 17–120.\n\n\nИткин, И. Б. 2007. Русская морфонология. Москва: Гнозис.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "03_phonological_rules.html#footnotes",
    "href": "03_phonological_rules.html#footnotes",
    "title": "3  Введение в twol: (мор)фонология",
    "section": "",
    "text": "Вот пример питающего порядка из бразильского португальского (индоевропейские).\n\n\n\nправило\nформа\nтранскрипция\nглосса\n\n\n\n\nпалатализация\nbato\n[bátu]\nбить-1sg\n\n\nпалатализация\nbate\n[bátʃi]\nбить-3sg\n\n\nпалатализация\nardo\n[áɾdu]\nжечь-1sg\n\n\nпалатализация\narde\n[áɾdʒi]\nжечь-3sg\n\n\nэпентеза i\npacto\n[pákitu]\nсоглашение\n\n\nэпентеза i\ncaptar\n[kapitáɾ]\nвзять в плен\n\n\nэпентеза i\npsicologia\n[pisikoloʒíɐ]\nпсихология\n\n\n\nА вот взаимодействие правил:\n\n\n\n\n\n\n\n\n\n\n\n/kaptáɾ/  взять в плен\n/áɾdi/  жечь-3sg\n/advɛ́χsu/  враждебный\n/futbɔ́w/  футбол\n\n\n\n\nэпентеза I\nkapitáɾ\n—\nadivɛ́χsu\nfutibɔ́w\n\n\nпалатализация\n—\náɾdʒi\nadʒivɛ́χsu\nfutʃibɔ́w\n\n\n\n↩︎\nВот пример питающего порядка из литовского (индоевропейские).\n\n\n\nправило\nтранскрипция\nперевод\n\n\n\n\nэпентеза i\n[at-koːpʲtʲi]\nприйти\n\n\nэпентеза i\n[atʲi-tʲeisʲtʲi]\nприсудить\n\n\nэпентеза i\n[ap-kalʲbʲetʲi]\nоговорить\n\n\nэпентеза i\n[apʲi-putʲi]\nподгнить\n\n\nозвончение\n[at-praʃʲiːtʲi]\nспросить\n\n\nозвончение\n[ad-gautʲi]\nвернуть\n\n\nозвончение\n[ap-ʃaukjtji]\nобъявить\n\n\nозвончение\n[ab-gautji]\nобмануть\n\n\n\nА вот взаимодействие правил:\n\n\n\n\n\n\n\n\n\n\n\n/ap-putʲi/  подгнить\n/at-gautʲi/  вернуть\n/at-duotʲi/  отдать\n/ap-bʲekʲtʲi/  обежать\n\n\n\n\nэпентеза i\napʲi-putʲi\n—\natʲi-duotʲi\napʲi-bʲekʲtʲi\n\n\nозвончение\n—\nad-gautʲi\n—\n—\n\n\n\n↩︎\nЕсли удалить из работающего файла одно из названий правила получится ошибка, в которой самым явным будет часть сообщения Error: twol.hfst is not a valid transducer file.↩︎\nЭтот термин предложил Н. С. Трубецкой для единиц, находящихся в слабой фонологической позиции и характеризующихся неполным набором признаковых спецификаций (например, оглушение на конце слова в русском). Однако в практике пользователей lexd и twol архифонемы — это просто единицы удобные для моделирования. Иногда их используют и для случаев подразумевавшихся Трубецким, например, для моделирования гармонии, но иногда их используют для моделирования других морфонологических сложностей.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Введение в `twol`: (мор)фонология</span>"
    ]
  },
  {
    "objectID": "04_language_tasks.html",
    "href": "04_language_tasks.html",
    "title": "4  Моделирование языковых явлений",
    "section": "",
    "text": "4.1 Амбификс в литовском",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Моделирование языковых явлений</span>"
    ]
  },
  {
    "objectID": "04_language_tasks.html#амбификс-в-литовском",
    "href": "04_language_tasks.html#амбификс-в-литовском",
    "title": "4  Моделирование языковых явлений",
    "section": "",
    "text": "Задание 04_01\n\n\n\nСмоделируйте при помощи lexd и twol поведение литовского (балтийские, индо-европейские) возвратного суффикса -si, который занимает позицию перед корнем, если к корню присоединяется любой префикс.\n\n\n\nперевод\nroot-inf-rfl\nneg-rfl-root-inf\n\n\n\n\nучиться\nmoky-ti-s\nne-si-moky-ti\n\n\nрадоваться\ndžiaug-ti-s\nne-si-džiaug-ti\n\n\nосматриваться\nžvalgy-ti-s\nne-si-žvalgy-ti\n\n\nгордиться\ndidžiuo-ti-s\nne-si-didžiuo-ti\n\n\nсмеяться\njuok-ti-s\nne-si-juok-ti",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Моделирование языковых явлений</span>"
    ]
  },
  {
    "objectID": "04_language_tasks.html#кабардинское-глагольное-согласование",
    "href": "04_language_tasks.html#кабардинское-глагольное-согласование",
    "title": "4  Моделирование языковых явлений",
    "section": "4.2 Кабардинское глагольное согласование",
    "text": "4.2 Кабардинское глагольное согласование\n\n\n\n\n\n\nЗадание 04_03\n\n\n\nМы составили на основе (Кумахов и др. 2006: 169–179) (и с помощью Ю. А. Ландера) фрагмент парадигмы глагола ‘идти’1 в кабардино-черкесском языке (адыгские, абхазо-адыгские). В таблице приводятся поверхностная и глубинные формы. Смоделируйте приведенные формы, используя lexd и twol. Графема у обозначает в данном примере лабиализацию [ʷ] или комбинацию губного аппроксиманта и ы [wə]. При моделировании не следует соединять в одной форме префикс дериктива &lt;dir&gt; и аппликативов (&lt;com&gt;, &lt;ben&gt;, &lt;mal&gt;). В кабардино-черкесском языке это возможно, но требует дополнительной морфонологии. Алфавит: а б в д з к н о п с т у ф х щ ъ ы э I.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Моделирование языковых явлений</span>"
    ]
  },
  {
    "objectID": "04_language_tasks.html#метатеза-в-венгерском",
    "href": "04_language_tasks.html#метатеза-в-венгерском",
    "title": "4  Моделирование языковых явлений",
    "section": "4.3 Метатеза в венгерском",
    "text": "4.3 Метатеза в венгерском\n\n\n\n\n\n\nЗадание 04_03\n\n\n\nВ венгерском языке (угорские, фино-угорские) в нескольких корнях встречается метатеза, затрагивающая h и сонорный (Kenesei, Vago, и Fenyvesi 1998). Она происходит перед несколькими показателями -ek &lt;pl&gt;, -et &lt;acc&gt; и -en &lt;on&gt;&lt;ess&gt;. Диграф ly обозначает палатальный апроксимант [j]. Неизменная часть алфавита: b e é g h i k l n ő p r t ü y z.\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumber\ncase\nшкала\nартиллерист\nсомнение\nхлопья\nбремя, груз\nчаша\n\n\n\n\nSG\nNOM\npikkely\ntüzér\nkétely\npehely\nteher\nkehely\n\n\nSG\nACC\npikkely-t\ntüzér-t\nkétely-t\npelyh-et\nterh-et\nkelyh-et\n\n\nSG\nDAT\npikkely-nek\ntüzér-nek\nkétely-nek\npehely-nek\nteher-nek\nkehely-nek\n\n\nSG\nPRP\npikkely-ért\ntüzér-ért\nkétely-ért\npehely-ért\nteher-ért\nkehely-ért\n\n\nSG\nTERM\npikkely-ig\ntüzér-ig\nkétely-ig\npehely-ig\nteher-ig\nkehely-ig\n\n\nSG\nFRML\npikkely-ként\ntüzér-ként\nkétely-ként\npehely-ként\nteher-ként\nkehely-ként\n\n\nSG\nIN+ESS\npikkely-ben\ntüzér-ben\nkétely-ben\npehely-ben\nteher-ben\nkehely-ben\n\n\nSG\nON+ESS\npikkely-en\ntüzér-en\nkétely-en\npelyh-en\nterh-en\nkelyh-en\n\n\nSG\nAT+ESS\npikkely-nél\ntüzér-nél\nkétely-nél\npehely-nél\nteher-nél\nkehely-nél\n\n\nSG\nIN+ALL\npikkely-be\ntüzér-be\nkétely-be\npehely-be\nteher-be\nkehely-be\n\n\nSG\nON+ALL\npikkely-re\ntüzér-re\nkétely-re\npehely-re\nteher-re\nkehely-re\n\n\nSG\nAT+ALL\npikkely-hez\ntüzér-hez\nkétely-hez\npehely-hez\nteher-hez\nkehely-hez\n\n\nSG\nIN+ABL\npikkely-ből\ntüzér-ből\nkétely-ből\npehely-ből\nteher-ből\nkehely-ből\n\n\nSG\nON+ABL\npikkely-ről\ntüzér-ről\nkétely-ről\npehely-ről\nteher-ről\nkehely-ről\n\n\nSG\nAT+ABL\npikkely-től\ntüzér-től\nkétely-től\npehely-től\nteher-től\nkehely-től\n\n\nSG\nLGSPEC1\npikkely-é\ntüzér-é\nkétely-é\npehely-é\nteher-é\nkehely-é\n\n\nSG\nLGSPEC2\npikkely-éi\ntüzér-éi\nkétely-éi\npehely-éi\nteher-éi\nkehely-éi\n\n\nPL\nNOM\npikkely-ek\ntüzér-ek\nkétely-ek\npelyh-ek\nterh-ek\nkelyh-ek\n\n\nPL\nACC\npikkely-ek-et\ntüzér-ek-et\nkétely-ek-et\npelyh-ek-et\nterh-ek-et\nkelyh-ek-et\n\n\nPL\nDAT\npikkely-ek-nek\ntüzér-ek-nek\nkétely-ek-nek\npelyh-ek-nek\nterh-ek-nek\nkelyh-ek-nek\n\n\nPL\nPRP\npikkely-ek-ért\ntüzér-ek-ért\nkétely-ek-ért\npelyh-ek-ért\nterh-ek-ért\nkelyh-ek-ért\n\n\nPL\nTERM\npikkely-ek-ig\ntüzér-ek-ig\nkétely-ek-ig\npelyh-ek-ig\nterh-ek-ig\nkelyh-ek-ig\n\n\nPL\nFRML\npikkely-ek-ként\ntüzér-ek-ként\nkétely-ek-ként\npelyh-ek-ként\nterh-ek-ként\nkelyh-ek-ként\n\n\nPL\nIN+ESS\npikkely-ek-ben\ntüzér-ek-ben\nkétely-ek-ben\npelyh-ek-ben\nterh-ek-ben\nkelyh-ek-ben\n\n\nPL\nON+ESS\npikkely-ek-en\ntüzér-ek-en\nkétely-ek-en\npelyh-ek-en\nterh-ek-en\nkelyh-ek-en\n\n\nPL\nAT+ESS\npikkely-ek-nél\ntüzér-ek-nél\nkétely-ek-nél\npelyh-ek-nél\nterh-ek-nél\nkelyh-ek-nél\n\n\nPL\nIN+ALL\npikkely-ek-be\ntüzér-ek-be\nkétely-ek-be\npelyh-ek-be\nterh-ek-be\nkelyh-ek-be\n\n\nPL\nON+ALL\npikkely-ek-re\ntüzér-ek-re\nkétely-ek-re\npelyh-ek-re\nterh-ek-re\nkelyh-ek-re\n\n\nPL\nAT+ALL\npikkely-ek-hez\ntüzér-ek-hez\nkétely-ek-hez\npelyh-ek-hez\nterh-ek-hez\nkelyh-ek-hez\n\n\nPL\nIN+ABL\npikkely-ek-ből\ntüzér-ek-ből\nkétely-ek-ből\npelyh-ek-ből\nterh-ek-ből\nkelyh-ek-ből\n\n\nPL\nON+ABL\npikkely-ek-ről\ntüzér-ek-ről\nkétely-ek-ről\npelyh-ek-ről\nterh-ek-ről\nkelyh-ek-ről\n\n\nPL\nAT+ABL\npikkely-ek-től\ntüzér-ek-től\nkétely-ek-től\npelyh-ek-től\nterh-ek-től\nkelyh-ek-től\n\n\nPL\nLGSPEC1\npikkely-ek-é\ntüzér-ek-é\nkétely-ek-é\npelyh-ek-é\nterh-ek-é\nkelyh-ek-é\n\n\nPL\nLGSPEC2\npikkely-ek-éi\ntüzér-ek-éi\nkétely-ek-éi\npelyh-ek-éi\nterh-ek-éi\nkelyh-ek-éi\n\n\n\n\n\n\n\n\n\nKenesei, István, Robert M Vago, и Anna Fenyvesi. 1998. Hungarian (descriptive grammars). London; New York: Routledge.\n\n\nКумахов, М. А., М. Л. Апажев, З. Х. Бижева, Б. Ч. Бижоев, Дж. Н. Коков, Х. Т. Таов, и Р. Х. Темирова. 2006. «Кабардино-черкесский язык в двух томах».",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Моделирование языковых явлений</span>"
    ]
  },
  {
    "objectID": "04_language_tasks.html#footnotes",
    "href": "04_language_tasks.html#footnotes",
    "title": "4  Моделирование языковых явлений",
    "section": "",
    "text": "Стоит иметь ввиду, что комитативные формы от данного глагола чаще употребляются не в буквальном значении, а в значении “выйти замуж”.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Моделирование языковых явлений</span>"
    ]
  },
  {
    "objectID": "05_technical_class.html",
    "href": "05_technical_class.html",
    "title": "5  Введение в командную строку",
    "section": "",
    "text": "5.1 Командная оболочка\nКогда вы открываете линуксовскую командную строку, чаще всего вы сталкиваетесь с интерпретатором bash –– это командная оболочка, которая позволяет отдавать команды вашей операционной системе и компьютеру. В данных материалах мы используем обобщенное shell, включающее в себя разные командные оболочки. Так как обычно команды в командной строке исполняются сразу, знак доллара традиционно используют для обозначения строчки с командой:\nРазумеется, когда дело доходит до больших и сложных программ, то их записывают в скрипты, которым обычно дают расширение .sh и начинают с шебанга. Вот пример простой программы:\nЕсли записать эту программу в файл my_script.sh, то ее потом можно запустить следующей командой:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Введение в командную строку</span>"
    ]
  },
  {
    "objectID": "05_technical_class.html#командная-оболочка",
    "href": "05_technical_class.html#командная-оболочка",
    "title": "5  Введение в командную строку",
    "section": "",
    "text": "```{shell}\n$ echo \"hi all\"\n\nhi all\n```\n\n#!/bin/bash\n\necho \"Please enter your name: \"\nread name\necho \"Nice to meet you $name\"\n\n```{shell}\n$ sh my_script.sh\n```",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Введение в командную строку</span>"
    ]
  },
  {
    "objectID": "05_technical_class.html#программы-для-команадной-строки",
    "href": "05_technical_class.html#программы-для-команадной-строки",
    "title": "5  Введение в командную строку",
    "section": "5.2 Программы для команадной строки",
    "text": "5.2 Программы для команадной строки\nВ данном разделе мы опишем несколько базовых программ для командной строки, которые могут быть важны для работы. Для командной строки написано очень много разных программ. Важно понимать, что их все (естественно, после установки, если это необходимо) можно запустить, набрав в консоли название программы. Начнем наше знакомство с программы ls, которая перечисляет (list) содержание папок:\n```{shell}\n$ ls\n\n01_00_stemmers.png                  01_09_first_transducer.png\n01_01_light_switch.jpg              01_10_morphology.png\n01_02_light_switch_automaton.png    01_11_transducer_composition.png\n01_03_turnstile.jpg                 01_12_morphology2.png\n01_04_turnstile_automaton.png       03_01_Russita-feeding.png\n01_05_elephant.png                  03_02_Russita-bleeding.png\n01_06_elephant_short.png            03_03_Russita-counterfeeding.png\n01_07_multiple_words.png            03_04_Russita-counterbleeding.png\n01_08_multiple_words_optimized.png\n```\nУ большинства программ есть некоторые однобуквенные аргументы (почему-то их принято называть флагами), которые перечисляют после минуса. Например флаг -l позволяет увидеть файлы в виде таблицы с некоторой дополнительной информацией про каждый файл.\n```{shell}\n$ ls -l\n\ntotal 2636\n-rw-rw-r-- 1 agricolamz agricolamz 122183 Jan  6 18:52 01_00_stemmers.png\n-rw-rw-r-- 1 agricolamz agricolamz 121071 Feb 18  2022 01_01_light_switch.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 156956 Jan  6 07:45 01_02_light_switch_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz  18075 Feb 18  2022 01_03_turnstile.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 167801 Jan  6 07:47 01_04_turnstile_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz 200090 Jan  6 07:50 01_05_elephant.png\n-rw-rw-r-- 1 agricolamz agricolamz 106598 Jan  6 07:51 01_06_elephant_short.png\n-rw-rw-r-- 1 agricolamz agricolamz 226555 Jan  6 07:52 01_07_multiple_words.png\n-rw-rw-r-- 1 agricolamz agricolamz 139834 Jan  6 07:58 01_08_multiple_words_optimized.png\n-rw-rw-r-- 1 agricolamz agricolamz 132286 Jan  6 16:02 01_09_first_transducer.png\n-rw-rw-r-- 1 agricolamz agricolamz 117095 Jan  6 17:15 01_10_morphology.png\n-rw-rw-r-- 1 agricolamz agricolamz 253314 Jan  6 18:32 01_11_transducer_composition.png\n-rw-rw-r-- 1 agricolamz agricolamz 261112 Jan 12 18:20 01_12_morphology2.png\n-rwxr-xr-x 1 agricolamz agricolamz 159091 Jan 13  2017 03_01_Russita-feeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 150034 Jan 13  2017 03_02_Russita-bleeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 140454 Jan 13  2017 03_03_Russita-counterfeeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 193216 Jan 13  2017 03_04_Russita-counterbleedin\n```\nДавайте использовать несколько флагов. Добавим флаг -r для обратной сортировки:\n```{shell}\n$ ls -l -r\n\ntotal 2636\n-rwxr-xr-x 1 agricolamz agricolamz 193216 Jan 13  2017 03_04_Russita-counterbleeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 140454 Jan 13  2017 03_03_Russita-counterfeeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 150034 Jan 13  2017 03_02_Russita-bleeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 159091 Jan 13  2017 03_01_Russita-feeding.png\n-rw-rw-r-- 1 agricolamz agricolamz 261112 Jan 12 18:20 01_12_morphology2.png\n-rw-rw-r-- 1 agricolamz agricolamz 253314 Jan  6 18:32 01_11_transducer_composition.png\n-rw-rw-r-- 1 agricolamz agricolamz 117095 Jan  6 17:15 01_10_morphology.png\n-rw-rw-r-- 1 agricolamz agricolamz 132286 Jan  6 16:02 01_09_first_transducer.png\n-rw-rw-r-- 1 agricolamz agricolamz 139834 Jan  6 07:58 01_08_multiple_words_optimized.png\n-rw-rw-r-- 1 agricolamz agricolamz 226555 Jan  6 07:52 01_07_multiple_words.png\n-rw-rw-r-- 1 agricolamz agricolamz 106598 Jan  6 07:51 01_06_elephant_short.png\n-rw-rw-r-- 1 agricolamz agricolamz 200090 Jan  6 07:50 01_05_elephant.png\n-rw-rw-r-- 1 agricolamz agricolamz 167801 Jan  6 07:47 01_04_turnstile_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz  18075 Feb 18  2022 01_03_turnstile.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 156956 Jan  6 07:45 01_02_light_switch_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz 121071 Feb 18  2022 01_01_light_switch.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 122183 Jan  6 18:52 01_00_stemmers.png\n```\nВсе однобуквенные флаги можно соединять вместе:\n```{shell}\n$ ls -lr\n\ntotal 2636\n-rwxr-xr-x 1 agricolamz agricolamz 193216 Jan 13  2017 03_04_Russita-counterbleeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 140454 Jan 13  2017 03_03_Russita-counterfeeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 150034 Jan 13  2017 03_02_Russita-bleeding.png\n-rwxr-xr-x 1 agricolamz agricolamz 159091 Jan 13  2017 03_01_Russita-feeding.png\n-rw-rw-r-- 1 agricolamz agricolamz 261112 Jan 12 18:20 01_12_morphology2.png\n-rw-rw-r-- 1 agricolamz agricolamz 253314 Jan  6 18:32 01_11_transducer_composition.png\n-rw-rw-r-- 1 agricolamz agricolamz 117095 Jan  6 17:15 01_10_morphology.png\n-rw-rw-r-- 1 agricolamz agricolamz 132286 Jan  6 16:02 01_09_first_transducer.png\n-rw-rw-r-- 1 agricolamz agricolamz 139834 Jan  6 07:58 01_08_multiple_words_optimized.png\n-rw-rw-r-- 1 agricolamz agricolamz 226555 Jan  6 07:52 01_07_multiple_words.png\n-rw-rw-r-- 1 agricolamz agricolamz 106598 Jan  6 07:51 01_06_elephant_short.png\n-rw-rw-r-- 1 agricolamz agricolamz 200090 Jan  6 07:50 01_05_elephant.png\n-rw-rw-r-- 1 agricolamz agricolamz 167801 Jan  6 07:47 01_04_turnstile_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz  18075 Feb 18  2022 01_03_turnstile.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 156956 Jan  6 07:45 01_02_light_switch_automaton.png\n-rw-rw-r-- 1 agricolamz agricolamz 121071 Feb 18  2022 01_01_light_switch.jpg\n-rw-rw-r-- 1 agricolamz agricolamz 122183 Jan  6 18:52 01_00_stemmers.png\n```\nКроме того, некоторые однобуквенные флаги имеют неоднобуквенный аналог. Все неоднобуквенные флаги начинаются с двух минусов. Например, флаг -r имеет неоднобуквенный аналог --reverse:\n```{shell}\nls --reverse\n\n03_04_Russita-counterbleeding.png   01_07_multiple_words.png\n03_03_Russita-counterfeeding.png    01_06_elephant_short.png\n03_02_Russita-bleeding.png          01_05_elephant.png\n03_01_Russita-feeding.png           01_04_turnstile_automaton.png\n01_12_morphology2.png               01_03_turnstile.jpg\n01_11_transducer_composition.png    01_02_light_switch_automaton.png\n01_10_morphology.png                01_01_light_switch.jpg\n01_09_first_transducer.png          01_00_stemmers.png\n01_08_multiple_words_optimized.png\n```\nДостаточно ожидаемо неоднобуквенные флаги нельзя комбинировать так, как комбинируются однобуквенные:\n```{shell}\n$ ls --reversel\n\nls: unrecognized option '--reversel'\nTry 'ls --help' for more information.\n```\nОтчёт об ошибке сообщает нам самое важное: где почитать документацию. В большинстве программ это можно сделать, вызвав аргумент --help. Иногда более вменяемый мануал дает команда man.\n```{shell}\n$ ls --help\n\nUsage: ls [OPTION]... [FILE]...\nList information about the FILEs (the current directory by default).\nSort entries alphabetically if none of -cftuvSUX nor --sort is specified.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --all                  do not ignore entries starting with .\n  -A, --almost-all           do not list implied . and ..\n      --author               with -l, print the author of each file\n  -b, --escape               print C-style escapes for nongraphic characters\n      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;\n                             e.g., '--block-size=M'; see SIZE format below\n\n  -B, --ignore-backups       do not list implied entries ending with ~\n  -c                         with -lt: sort by, and show, ctime (time of last\n                             change of file status information);\n                             with -l: show ctime and sort by name;\n                             otherwise: sort by ctime, newest first\n\n  -C                         list entries by columns\n      --color[=WHEN]         color the output WHEN; more info below\n  -d, --directory            list directories themselves, not their contents\n  -D, --dired                generate output designed for Emacs' dired mode\n  -f                         list all entries in directory order\n  -F, --classify[=WHEN]      append indicator (one of */=&gt;@|) to entries WHEN\n      --file-type            likewise, except do not append '*'\n      --format=WORD          across -x, commas -m, horizontal -x, long -l,\n                             single-column -1, verbose -l, vertical -C\n\n      --full-time            like -l --time-style=full-iso\n  -g                         like -l, but do not list owner\n      --group-directories-first\n                             group directories before files;\n                             can be augmented with a --sort option, but any\n                             use of --sort=none (-U) disables grouping\n\n  -G, --no-group             in a long listing, don't print group names\n  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.\n      --si                   likewise, but use powers of 1000 not 1024\n  -H, --dereference-command-line\n                             follow symbolic links listed on the command line\n      --dereference-command-line-symlink-to-dir\n                             follow each command line symbolic link\n                             that points to a directory\n\n      --hide=PATTERN         do not list implied entries matching shell PATTERN\n                             (overridden by -a or -A)\n\n      --hyperlink[=WHEN]     hyperlink file names WHEN\n      --indicator-style=WORD\n                             append indicator with style WORD to entry names:\n                             none (default), slash (-p),\n                             file-type (--file-type), classify (-F)\n\n  -i, --inode                print the index number of each file\n  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN\n  -k, --kibibytes            default to 1024-byte blocks for file system usage;\n                             used only with -s and per directory totals\n\n  -l                         use a long listing format\n  -L, --dereference          when showing file information for a symbolic\n                             link, show information for the file the link\n                             references rather than for the link itself\n\n  -m                         fill width with a comma separated list of entries\n  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs\n  -N, --literal              print entry names without quoting\n  -o                         like -l, but do not list group information\n  -p, --indicator-style=slash\n                             append / indicator to directories\n  -q, --hide-control-chars   print ? instead of nongraphic characters\n      --show-control-chars   show nongraphic characters as-is (the default,\n                             unless program is 'ls' and output is a terminal)\n\n  -Q, --quote-name           enclose entry names in double quotes\n      --quoting-style=WORD   use quoting style WORD for entry names:\n                             literal, locale, shell, shell-always,\n                             shell-escape, shell-escape-always, c, escape\n                             (overrides QUOTING_STYLE environment variable)\n\n  -r, --reverse              reverse order while sorting\n  -R, --recursive            list subdirectories recursively\n  -s, --size                 print the allocated size of each file, in blocks\n  -S                         sort by file size, largest first\n      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),\n                             time (-t), version (-v), extension (-X), width\n\n      --time=WORD            select which timestamp used to display or sort;\n                               access time (-u): atime, access, use;\n                               metadata change time (-c): ctime, status;\n                               modified time (default): mtime, modification;\n                               birth time: birth, creation;\n                             with -l, WORD determines which time to show;\n                             with --sort=time, sort by WORD (newest first)\n\n      --time-style=TIME_STYLE\n                             time/date format with -l; see TIME_STYLE below\n  -t                         sort by time, newest first; see --time\n  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8\n  -u                         with -lt: sort by, and show, access time;\n                             with -l: show access time and sort by name;\n                             otherwise: sort by access time, newest first\n\n  -U                         do not sort; list entries in directory order\n  -v                         natural sort of (version) numbers within text\n  -w, --width=COLS           set output width to COLS.  0 means no limit\n  -x                         list entries by lines instead of by columns\n  -X                         sort alphabetically by entry extension\n  -Z, --context              print any security context of each file\n      --zero                 end each output line with NUL, not newline\n  -1                         list one file per line\n      --help        display this help and exit\n      --version     output version information and exit\n\nThe SIZE argument is an integer and optional unit (example: 10K is 10*1024).\nUnits are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) or KB,MB,... (powers of 1000).\nBinary prefixes can be used, too: KiB=K, MiB=M, and so on.\n\nThe TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.\nFORMAT is interpreted like in date(1).  If FORMAT is FORMAT1&lt;newline&gt;FORMAT2,\nthen FORMAT1 applies to non-recent files and FORMAT2 to recent files.\nTIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.\nAlso the TIME_STYLE environment variable sets the default style to use.\n\nThe WHEN argument defaults to 'always' and can also be 'auto' or 'never'.\n\nUsing color to distinguish file types is disabled both by default and\nwith --color=never.  With --color=auto, ls emits color codes only when\nstandard output is connected to a terminal.  The LS_COLORS environment\nvariable can change the settings.  Use the dircolors(1) command to set it.\n\nExit status:\n 0  if OK,\n 1  if minor problems (e.g., cannot access subdirectory),\n 2  if serious trouble (e.g., cannot access command-line argument).\n\nGNU coreutils online help: &lt;https://www.gnu.org/software/coreutils/&gt;\nFull documentation &lt;https://www.gnu.org/software/coreutils/ls&gt;\nor available locally via: info '(coreutils) ls invocation'  \n```\nНекоторые аргументы функций принимают значения от пользователей, обычно их разделяют пробелом. Например, аргумент --sort принимает одно из следующих значений: none, size, time, version, extension или width:\n```{shell}\n$ ls --sort size\n\n01_12_morphology2.png              03_03_Russita-counterfeeding.png\n01_11_transducer_composition.png   01_08_multiple_words_optimized.png\n01_07_multiple_words.png           01_09_first_transducer.png\n01_05_elephant.png                 01_00_stemmers.png\n03_04_Russita-counterbleeding.png  01_01_light_switch.jpg\n01_04_turnstile_automaton.png      01_10_morphology.png\n03_01_Russita-feeding.png          01_06_elephant_short.png\n01_02_light_switch_automaton.png   01_03_turnstile.jpg\n03_02_Russita-bleeding.png            \n```\nНекоторые аргументы позволяют пользователям ввести что-то не из заранее заданного списка. Например, это важно при указании путей, имен файлов, интернет-ссылок и т. п.\nСледующая программа, с которой мы познакомимся — pwd (print working directory). Это программа позволяет увидеть текущую папку.\n```{shell}\n$ pwd\n\n/home/agricolamz/work/materials/2025_morphological_transducers/images\n```\nЗаодно мы увидели, что пути в юниксоподобных операционных системах пишут, используя слэш (“forward” slash).\nДля того чтобы перейти в какую-нибудь другую папку, используют команду cd (change directory).\n```{shell}\n$ pwd\n\n/home/agricolamz/work/materials/2025_morphological_transducers/images\n\n$ cd /home/agricolamz/Documents/CV\n$ pwd\n\n/home/agricolamz/Documents/CV\n```\nСтоит знать о возможности дать программе cd относительный путь:\n\n. — текущая папка;\n.. — родительская папка\nsome_name — имя папки в текущей папке\n- — предыдущая папка, в которой пользователь находился\n\n```{shell}\n$ pwd\n\n/home/agricolamz/Documents/CV\n\n$ cd -\n$ pwd\n\n/home/agricolamz/work/materials/2025_morphological_transducers/images\n\n$ cd ..\n$ pwd\n\n/home/agricolamz/work/materials/2025_morphological_transducers\n\n$ ls -d */\ndata/  docs/  images/  task_tests/\n\n$ cd data\n$ pwd\n\n/home/agricolamz/work/materials/2025_morphological_transducers/data\n```\nДля создания файла традиционно используют программу touch, хотя это, видимо, не ее прямое предназначение (см. детали в touch --help):\n```{shell}\n$ ls\n\n04_kabardian_verb.csv\n\n$ touch new_file.txt\n$ ls\n\n04_kabardian_verb.csv  new_file.txt\n```\nДля того чтобы удалить файл, используется программа rm:\n```{shell}\n$ ls\n\n04_kabardian_verb.csv  new_file.txt\n\n$ rm new_file.txt\n$ ls\n\n04_kabardian_verb.csv\n```\nДля того чтобы создать папку, используют программу mkdir. Для того чтобы удалить — программа rm с аргументом r (recursive).\n```{shell}\n$ ls\n\n04_kabardian_verb.csv\n\n$ mkdir some_stuff\n\n$ ls\n\n04_kabardian_verb.csv  some_stuff\n\n$ echo \"hi there\" &gt; some_stuff/new_file.txt\n\n$ ls some_stuff\n\nnew_file.txt\n\n$ rm -r some_stuff\n$ ls\n\n04_kabardian_verb.csv\n```\nРассмотрим теперь несколько команд для работы с текстами. Любой текстовый файл можно прочитать при помощи команды cat:\n```{shell}\n$ cat 04_kabardian_verb.csv\n\ntranslation,surface,abs,dir,io,ben/mal/com,dyn,root,pst/pot,dcl\nя иду,сокIуэ,сы,,,,о,кIуэ,,\nя шел,сыкIуащ,сы,,,,,кIуэ,а,щ\nя пойду,сыкIуэнщ,сы,,,,,кIуэ,н,щ\nя иду сюда,сыкъокIуэ,сы,къэ,,,о,кIуэ,,\nя шел сюда,сыкъэкIуащ,сы,къэ,,,,кIуэ,а,щ\nя пойду сюда,сыкъэкIуэнщ,сы,къэ,,,,кIуэ,н,щ\nты идешь,уокIуэ,у,,,,о,кIуэ,,\nты шел,укIуащ,у,,,,,кIуэ,а,щ\nты пойдешь,укIуэнщ,у,,,,,кIуэ,н,щ\nты идешь сюда,укъокIуэ,у,къэ,,,о,кIуэ,,\nты шел сюда,укъэкIуащ,у,къэ,,,,кIуэ,а,щ\nты пойдешь сюда,укъэкIуэнщ,у,къэ,,,,кIуэ,н,щ\nмы идем,докIуэ,ды,,,,о,кIуэ,,\nмы шли,дыкIуащ,ды,,,,,кIуэ,а,щ\nмы пойдем,дыкIуэнщ,ды,,,,,кIуэ,н,щ\nмы идем сюда,дыкъокIуэ,ды,къэ,,,о,кIуэ,,\nмы шли сюда,дыкъэкIуащ,ды,къэ,,,,кIуэ,а,щ\nмы пойдем сюда,дыкъэкIуэнщ,ды,къэ,,,,кIуэ,н,щ\nвы идете,фокIуэ,фы,,,,о,кIуэ,,\nвы шли,фыкIуащ,фы,,,,,кIуэ,а,щ\nвы пойдете,фыкIуэнщ,фы,,,,,кIуэ,н,щ\nвы идете сюда,фыкъокIуэ,фы,къэ,,,о,кIуэ,,\nвы шли сюда,фыкъэкIуащ,фы,къэ,,,,кIуэ,а,щ\nвы пойдете сюда,фыкъэкIуэнщ,фы,къэ,,,,кIуэ,н,щ\nя для тебя иду,сыпхуокIуэ,сы,,п,хуэ,о,кIуэ,,\nя для тебя ходил,сыпхуэкIуащ,сы,,п,хуэ,,кIуэ,а,щ\nя для тебя пойду,сыпхуэкIуэнщ,сы,,п,хуэ,,кIуэ,н,щ\nя против твоей воли иду,сыпфIокIуэ,сы,,п,фIэ,о,кIуэ,,\nя против твоей воли ходил,сыпфIыкIуащ,сы,,п,фIэ,,кIуэ,а,щ\nя против твоей воли пойду,сыпфIыкIуэнщ,сы,,п,фIэ,,кIуэ,н,щ\nя с тобой иду,сыбдокIуэ,сы,,п,дэ,о,кIуэ,,\nя с тобой ходил,сыбдэкIуащ,сы,,п,дэ,,кIуэ,а,щ\nя с тобой пойду,сыбдэкIуэнщ,сы,,п,дэ,,кIуэ,н,щ\nя с вами иду,сывдокIуэ,сы,,ф,дэ,о,кIуэ,,\nя с вами ходил,сывдэкIуащ,сы,,ф,дэ,,кIуэ,а,щ\nя с вами пойду,сывдэкIуэнщ,сы,,ф,дэ,,кIуэ,н,щ\nты со мной идешь,уздокIуэ,у,,с,дэ,о,кIуэ,,\nты со мной ходил,уздэкIуащ,у,,с,дэ,,кIуэ,а,щ\nты со мной пойдешь,уздэкIуэнщ,у,,с,дэ,,кIуэ,н,щ\nты с нами идешь,уддокIуэ,у,,т,дэ,о,кIуэ,,\nты с нами ходил,уддэкIуащ,у,,т,дэ,,кIуэ,а,щ\nты с нами пойдешь,уддэкIуэнщ,у,,т,дэ,,кIуэ,н,щ\nя для вас иду,сыфхуокIуэ,сы,,ф,хуэ,о,кIуэ,,\nя для вас ходил,сыфхуэкIуащ,сы,,ф,хуэ,,кIуэ,а,щ\nя для вас пойду,сыфхуэкIуэнщ,сы,,ф,хуэ,,кIуэ,н,щ\nя против вашей воли иду,сыффIокIуэ,сы,,ф,фIэ,о,кIуэ,,\nя против вашей воли ходил,сыффIэкIуащ,сы,,ф,фIэ,,кIуэ,а,щ\nя против вашей воли пойду,сыффIэкIуэнщ,сы,,ф,фIэ,,кIуэ,н,щ\nты для меня идешь,усхуокIуэ,у,,с,хуэ,о,кIуэ,,\nты против моей воли идешь,усфIокIуэ,у,,с,фIэ,о,кIуэ,,\nты для нас идешь,утхуокIуэ,у,,т,хуэ,о,кIуэ,,\nты против нашей воли идешь,утфIокIуэ,у,,т,фIэ,о,кIуэ,,\n```\nНе очень приятно выглядит… Но это связано с тем, что это .csv файл, для его чтения и отображения следует использовать другие инструменты, например, команду column:\n```{shell}\n$ column -s, -t 04_kabardian_verb.csv\n\ntranslation                 surface       abs  dir  io  ben/mal/com  dyn  root  pst/pot  dcl\nя иду                       сокIуэ        сы                         о    кIуэ           \nя шел                       сыкIуащ       сы                              кIуэ  а        щ\nя пойду                     сыкIуэнщ      сы                              кIуэ  н        щ\nя иду сюда                  сыкъокIуэ     сы   къэ                   о    кIуэ           \nя шел сюда                  сыкъэкIуащ    сы   къэ                        кIуэ  а        щ\nя пойду сюда                сыкъэкIуэнщ   сы   къэ                        кIуэ  н        щ\nты идешь                    уокIуэ        у                          о    кIуэ           \nты шел                      укIуащ        у                               кIуэ  а        щ\nты пойдешь                  укIуэнщ       у                               кIуэ  н        щ\nты идешь сюда               укъокIуэ      у    къэ                   о    кIуэ           \nты шел сюда                 укъэкIуащ     у    къэ                        кIуэ  а        щ\nты пойдешь сюда             укъэкIуэнщ    у    къэ                        кIуэ  н        щ\nмы идем                     докIуэ        ды                         о    кIуэ           \nмы шли                      дыкIуащ       ды                              кIуэ  а        щ\nмы пойдем                   дыкIуэнщ      ды                              кIуэ  н        щ\nмы идем сюда                дыкъокIуэ     ды   къэ                   о    кIуэ           \nмы шли сюда                 дыкъэкIуащ    ды   къэ                        кIуэ  а        щ\nмы пойдем сюда              дыкъэкIуэнщ   ды   къэ                        кIуэ  н        щ\nвы идете                    фокIуэ        фы                         о    кIуэ           \nвы шли                      фыкIуащ       фы                              кIуэ  а        щ\nвы пойдете                  фыкIуэнщ      фы                              кIуэ  н        щ\nвы идете сюда               фыкъокIуэ     фы   къэ                   о    кIуэ           \nвы шли сюда                 фыкъэкIуащ    фы   къэ                        кIуэ  а        щ\nвы пойдете сюда             фыкъэкIуэнщ   фы   къэ                        кIуэ  н        щ\nя для тебя иду              сыпхуокIуэ    сы        п   хуэ          о    кIуэ           \nя для тебя ходил            сыпхуэкIуащ   сы        п   хуэ               кIуэ  а        щ\nя для тебя пойду            сыпхуэкIуэнщ  сы        п   хуэ               кIуэ  н        щ\nя против твоей воли иду     сыпфIокIуэ    сы        п   фIэ          о    кIуэ           \nя против твоей воли ходил   сыпфIыкIуащ   сы        п   фIэ               кIуэ  а        щ\nя против твоей воли пойду   сыпфIыкIуэнщ  сы        п   фIэ               кIуэ  н        щ\nя с тобой иду               сыбдокIуэ     сы        п   дэ           о    кIуэ           \nя с тобой ходил             сыбдэкIуащ    сы        п   дэ                кIуэ  а        щ\nя с тобой пойду             сыбдэкIуэнщ   сы        п   дэ                кIуэ  н        щ\nя с вами иду                сывдокIуэ     сы        ф   дэ           о    кIуэ           \nя с вами ходил              сывдэкIуащ    сы        ф   дэ                кIуэ  а        щ\nя с вами пойду              сывдэкIуэнщ   сы        ф   дэ                кIуэ  н        щ\nты со мной идешь            уздокIуэ      у         с   дэ           о    кIуэ           \nты со мной ходил            уздэкIуащ     у         с   дэ                кIуэ  а        щ\nты со мной пойдешь          уздэкIуэнщ    у         с   дэ                кIуэ  н        щ\nты с нами идешь             уддокIуэ      у         т   дэ           о    кIуэ           \nты с нами ходил             уддэкIуащ     у         т   дэ                кIуэ  а        щ\nты с нами пойдешь           уддэкIуэнщ    у         т   дэ                кIуэ  н        щ\nя для вас иду               сыфхуокIуэ    сы        ф   хуэ          о    кIуэ           \nя для вас ходил             сыфхуэкIуащ   сы        ф   хуэ               кIуэ  а        щ\nя для вас пойду             сыфхуэкIуэнщ  сы        ф   хуэ               кIуэ  н        щ\nя против вашей воли иду     сыффIокIуэ    сы        ф   фIэ          о    кIуэ           \nя против вашей воли ходил   сыффIэкIуащ   сы        ф   фIэ               кIуэ  а        щ\nя против вашей воли пойду   сыффIэкIуэнщ  сы        ф   фIэ               кIуэ  н        щ\nты для меня идешь           усхуокIуэ     у         с   хуэ          о    кIуэ           \nты против моей воли идешь   усфIокIуэ     у         с   фIэ          о    кIуэ           \nты для нас идешь            утхуокIуэ     у         т   хуэ          о    кIуэ           \nты против нашей воли идешь  утфIокIуэ     у         т   фIэ          о    кIуэ                \n```\nДостаточно популярные базовые программы для работы с текстовыми данными это wc и grep. Программа wc выводит подсчет относительно файла: строчек, слов и байт.\n```{shell}\n$ wc 04_kabardian_verb.csv\n\n53  181 3796 04_kabardian_verb.csv\n```\nПри помощи аргументов можно выбрать, что выводить:\n```{shell}\n$ wc -l 04_kabardian_verb.csv\n\n53 04_kabardian_verb.csv\n\n$ wc -lw 04_kabardian_verb.csv\n\n53 181 04_kabardian_verb.csv\n\n$ wc -m 04_kabardian_verb.csv\n\n2314 04_kabardian_verb.csv\n```\nФайлов может быть несколько:\n```{shell}\n$ echo \"hi there\\n and good bye\" &gt; new_file.txt\n$ wc 04_kabardian_verb.csv new_file.txt\n\n  53  181 3796 04_kabardian_verb.csv\n   2    5   24 new_file.txt\n  55  186 3820 total\n\n$ rm new_file.txt\n```\nКоманда grep ищет некоторое выражение в текстовых файлах и является достаточно мощным инструментом.\n```{shell}\ngrep иду 04_kabardian_verb.csv\n\nя иду,сокIуэ,сы,,,,о,кIуэ,,\nя иду сюда,сыкъокIуэ,сы,къэ,,,о,кIуэ,,\nя для тебя иду,сыпхуокIуэ,сы,,п,хуэ,о,кIуэ,,\nя против твоей воли иду,сыпфIокIуэ,сы,,п,фIэ,о,кIуэ,,\nя с тобой иду,сыбдокIуэ,сы,,п,дэ,о,кIуэ,,\nя с вами иду,сывдокIуэ,сы,,ф,дэ,о,кIуэ,,\nя для вас иду,сыфхуокIуэ,сы,,ф,хуэ,о,кIуэ,,\nя против вашей воли иду,сыффIокIуэ,сы,,ф,фIэ,о,кIуэ,,\n\ngrep -E \"иду|идет\" 04_kabardian_verb.csv\n\nя иду,сокIуэ,сы,,,,о,кIуэ,,\nя иду сюда,сыкъокIуэ,сы,къэ,,,о,кIуэ,,\nвы идете,фокIуэ,фы,,,,о,кIуэ,,\nвы идете сюда,фыкъокIуэ,фы,къэ,,,о,кIуэ,,\nя для тебя иду,сыпхуокIуэ,сы,,п,хуэ,о,кIуэ,,\nя против твоей воли иду,сыпфIокIуэ,сы,,п,фIэ,о,кIуэ,,\nя с тобой иду,сыбдокIуэ,сы,,п,дэ,о,кIуэ,,\nя с вами иду,сывдокIуэ,сы,,ф,дэ,о,кIуэ,,\nя для вас иду,сыфхуокIуэ,сы,,ф,хуэ,о,кIуэ,,\nя против вашей воли иду,сыффIокIуэ,сы,,ф,фIэ,о,кIуэ,,\n```\nПоследнее, что следует обсудить в данном разделе — оператор |. Этот оператор позволяет отправить результат работы одной функции в другую, например:\n```{shell}\n$ cat 04_kabardian_verb.csv | wc -l\n\n53\n```\nВ данном случае программа wc приняла результат работы cat. Это полезный инструмент, если хочется запустить несколько программ, не делая промежуточных переменных. Вот еще один пример:\n```{shell}\n$ ls ../images | sort -r\n\n03_04_Russita-counterbleeding.png\n03_03_Russita-counterfeeding.png\n03_02_Russita-bleeding.png\n03_01_Russita-feeding.png\n01_12_morphology2.png\n01_11_transducer_composition.png\n01_10_morphology.png\n01_09_first_transducer.png\n01_08_multiple_words_optimized.png\n01_07_multiple_words.png\n01_06_elephant_short.png\n01_05_elephant.png\n01_04_turnstile_automaton.png\n01_03_turnstile.jpg\n01_02_light_switch_automaton.png\n01_01_light_switch.jpg\n01_00_stemmers.png\n```\nНе вдаваясь в подробности, стоит оговориться, что если мы хотим записать результат работы функции в файл, то для этого используется другой оператор: &gt;. Мы видели выше его использование:\n```{shell}\n$ echo \"hi there\" &gt; new_file.txt\n```",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Введение в командную строку</span>"
    ]
  },
  {
    "objectID": "05_technical_class.html#компиляция-трансдьюсеров",
    "href": "05_technical_class.html#компиляция-трансдьюсеров",
    "title": "5  Введение в командную строку",
    "section": "5.3 Компиляция трансдьюсеров",
    "text": "5.3 Компиляция трансдьюсеров\nТеперь попробуем шаг за шагом скомпилировать наш морфологический трансдьюсер, используя программы lexd и hfst. Первым делом нужно установить необходимые программы hfst, lexd:\n```{shell}\n$ curl -s https://apertium.projectjj.com/apt/install-nightly.sh | sudo bash\n$ sudo apt-get install hfst lexd\n```\nСоздадим файлы example.lexd и example.twol:\n```{lexd}\nPATTERNS\nNoun (Suffix[-adj] | (Suffix[adj] Inflection))?\n\nLEXICON Noun\nночь\nпечь\n\nLEXICON Suffix\n&lt;dim&gt;:ка\n&lt;adj&gt;:н[adj]\n\nLEXICON Inflection\n&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ой\n```\n```{twol}\nAlphabet\n  а е й к н о п ч ь ь:0;\n\nRules\n\n\"чк чн пишется без ь\"\n! например, ночьной -&gt; ночной или печька -&gt; печка\n\nь:0 &lt;=&gt; _ к;\n        _ н;\n```\nСкомпилируем .lexd файл:\n```{shell}\n$ lexd example.lexd\n\n0   1   н   н   0.000000    \n0   2   п   п   0.000000    \n1   3   о   о   0.000000    \n2   3   е   е   0.000000    \n3   4   ч   ч   0.000000    \n4   5   ь   ь   0.000000    \n5   6   &lt;dim&gt;   к   0.000000    \n5   7   &lt;adj&gt;   н   0.000000    \n6   8   @0@ а   0.000000    \n7   9   &lt;m&gt; о   0.000000    \n9   10  &lt;sg&gt;    й   0.000000    \n10  8   &lt;nom&gt;   @0@ 0.000000    \n5   0.000000\n8   0.000000 \n```\nМы видим, что программа lexd преобразовала наш файл example.lexd в трансдьюсер в ATT формате (см. Раздел 1.4.1). Так как основной формат все равно завязан на программу hfst, можно перенаправить ATT в команду hfst-txt2fst. Флаг -o отвечает за имя файла, куда записать результат, так что пользователь может задать любое свое имя, отличное от lexd.hfst.\n```{shell}\n$ lexd example.lexd | hfst-txt2fst -o lexd.hfst\n```\nВ результате был получен бинарный файл .hfst, который используется для дальнейшей работы. Например, можно использовать функцию hfst-lookup для того, чтобы обратиться к трансдьюсеру.\n```{shell}\n$ echo \"ночь\" | hfst-lookup lexd.hfst \n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; ночь  ночь    0,000000\n\n$ echo \"ночька\" | hfst-lookup lexd.hfst\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\nUsing HFST basic transducer format and performing slow lookups\n&gt; ночька    ночька+?    inf\n```\nМожно использовать программу hfst-fst2strings, чтобы посмотреть на все формы:\n```{shell}\n$ hfst-fst2strings lexd.hfst\n\nночь\nночь&lt;dim&gt;:ночька\nночь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночьной\nпечь\nпечь&lt;dim&gt;:печька\nпечь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печьной\n```\nТеперь можно скомпилировать .twol часть:\n```{shell}\n$ hfst-twolc example.twol -o twol.hfst\n\nReading input from task.twol.\nWriting output to twol.hfst.\nReading alphabet.\nReading rules and compiling their contexts and centers.\nCompiling rules.\nStoring rules.\n```\nВсе это достаточно скучные сообщения, поэтому можно использовать аргумент -q (quiet), чтобы их не было.\n```{shell}\n$ hfst-twolc -q example.twol -o twol.hfst\n```\nМожет быть вам захотелось бы посмотреть на twol.hfst, но …\n```{shell}\n$ hfst-fst2strings twol.hfst\n\nhfst-fst2strings: Error: Transducer is cyclic. Use one or more of these options: -n, -N, -r, -l, -L, -c\n```\nДавайте воспользуемся аргументом -n:\n```{shell}\n$ hfst-fst2strings -n 10 twol.hfst\n\nь\nььк:ьк\nьькь:ькь\nьькььк:ькьк\nьькьькь:ькькь\nьькьькььк:ькькьк\nьькьькьькь:ькькькь\nьькьькьькььк:ькькькьк\nьькьькьькьькь:ькькькькь\nьькьькьькьькььк:ькькькькьк\n\n$ hfst-fst2strings -r 10 twol.hfst\n\n@_IDENTITY_SYMBOL_@@#@@#@:@_IDENTITY_SYMBOL_@\nа\nе\nк\nн\nнк\nо\nоье\nп\nь@_IDENTITY_SYMBOL_@й \n```\nЯ считаю, что из преведенных фрагментов достаточно сложно что-то понять. Но попробуем пересечь наши трансдьюсеры:\n```{shell}\n$ hfst-compose-intersect lexd.hfst twol.hfst -o result.hfst\n```\nТеперь мы можем посмотреть, что же у нас получилось:\n```{shell}\n$ hfst-fst2strings result.hfst    \n\nночь&lt;dim&gt;:ночка\nночь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночной\nночь\nпечь&lt;dim&gt;:печка\nпечь&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печной\nпечь\n```\nВсе! Мы сумели скомпелировать трансдьюсер, не используя Makefile, которым мы пользовались все это время.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Введение в командную строку</span>"
    ]
  },
  {
    "objectID": "06_transducer_manipulation.html",
    "href": "06_transducer_manipulation.html",
    "title": "6  Операции с трансдьюсерами",
    "section": "",
    "text": "6.1 Некоторые команды hfst\nПосмотрим на следующий трансдьюсер.\nВ большинстве команд ниже работают флаги -i — входящий файл; -o — исходящий файл. Если -o отсутствует, то результат печатается в консоль.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Операции с трансдьюсерами</span>"
    ]
  },
  {
    "objectID": "06_transducer_manipulation.html#некоторые-команды-hfst",
    "href": "06_transducer_manipulation.html#некоторые-команды-hfst",
    "title": "6  Операции с трансдьюсерами",
    "section": "",
    "text": "6.1.1 hfst-fst2strings\nПечатает формы:\n```{shell}\n$ hfst-fst2strings generator.hfst\n```\n\n\nночь&lt;N&gt;&lt;dim&gt;:ночка\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночной\nночь&lt;N&gt;&lt;nom&gt;:ночь\nночь&lt;N&gt;&lt;acc&gt;:ночь\nпечь&lt;N&gt;&lt;dim&gt;:печка\nпечь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печной\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;acc&gt;:печь\n\n\nТакже существует программа hfst-expand, которая делает то же самое.\n\n\n6.1.2 hfst-fst2txt\nПечатает трансдьюсер в AT&T формате:\n```{shell}\n$ hfst-fst2txt generator.hfst\n```\n\n\n0   1   @0@ @0@ 0.000000\n1   2   н   н   0.000000\n1   3   п   п   0.000000\n2   4   о   о   0.000000\n3   4   е   е   0.000000\n4   5   ч   ч   0.000000\n5   6   ь   @0@ 0.000000\n5   7   ь   ь   0.000000\n6   8   &lt;N&gt; @0@ 0.000000\n7   9   &lt;N&gt; @0@ 0.000000\n8   10  &lt;nom&gt;   @0@ 0.000000\n8   10  &lt;acc&gt;   @0@ 0.000000\n8   11  &lt;dim&gt;   к   0.000000\n8   12  &lt;adj&gt;   н   0.000000\n9   13  &lt;nom&gt;   @0@ 0.000000\n9   13  &lt;acc&gt;   @0@ 0.000000\n11  14  @0@ а   0.000000\n12  15  &lt;m&gt; о   0.000000\n13  16  @0@ @0@ 0.000000\n14  16  @0@ @0@ 0.000000\n15  17  &lt;sg&gt;    й   0.000000\n16  0.000000\n17  14  &lt;nom&gt;   @0@ 0.000000\n\n\n\n\n6.1.3 hfst-summarise\nДанная программа печатает саммари для трансдьюсера.\n```{shell}\n$ hfst-summarise generator.hfst\n```\n\n\nname: \"compose(text(&lt;stdin&gt;), intersect(examples/twol.hfst))\"\nfst type: OpenFST\narc type: tropical\ninput symbol table: yes\noutput symbol table: yes\n# of states: 18\n# of arcs: 22\ninitial state: 0\n# of final states: 1\n# of input/output epsilons: 3\n# of input epsilons: 4\n# of output epsilons: 11\n# of ... accessible states: ???\n# of ... coaccessible states: ???\n# of ... connected states: ???\n# of ... strongly conn components: ???\nexpanded: ???\nmutable: yes\nacceptor: no\ninput deterministic: no\noutput deterministic: no\ninput label sorted: ???\noutput label sorted: ???\nweighted: yes\ncyclic: no\ncyclic at initial state: no\ntopologically sorted: ???\naccessible: ???\ncoaccessible: ???\nstring: ???\nminimised: ???\n\nRead 1 transducers in total.\n\n\nЕсли вам не хочется смотреть на выдачу целиком, можно совмещать с grep:\n```{shell}\n$ hfst-summarise generator.hfst | grep cyclic\n```\n\n\ncyclic: no\ncyclic at initial state: no\n\n\n\n\n6.1.4 hfst-name\nЕсли хочется, чтобы у трансдьюсера было осмысленное имя, то можно при создании дать ему имя при помощи функции hfst-name или hfst-edit-metadata. Посмотреть имя трансдьюсера можно при помощи аргумента -p (и некоторых других):\n```{shell}\n$ hfst-compose-intersect lexd.hfst twol.hfst | hfst-name -n 'the best fst ever' -o generator.hfst\n$ hfst-name -p generator.hfst\n```\n\n\n\"the best fst ever\"\n\n\nПрограмма hfst-edit-metadata позволяет настроить, например, следующие поля:\n\n--add=author=...\n--add=licence=...\n--add=name='...'\n\n\n\n6.1.5 hfst-dump-alphabets\nПечатает алфавит:\n```{shell}\n$ hfst-dump-alphabets generator.hfst\n```\n\n\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n@_EPSILON_SYMBOL_@\n@_IDENTITY_SYMBOL_@\n@_UNKNOWN_SYMBOL_@\nа\nе\nй\nк\nн\nо\nп\nч\nь\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n@_EPSILON_SYMBOL_@\nа\nе\nй\nк\nн\nо\nп\nч\nь\n\n\nМне не очень понятно, почему он печатает все два раза, однако этот фрагмент документации намекает мне, что это решаемая проблема:\nAlphabet dump options:\n  -1, --exclude-seen       Ignore alphabets seen in automaton\n  -2, --exclude-metadata   Ignore alphabets from headers\n```{shell}\n$ hfst-dump-alphabets -1 generator.hfst\n```\n\n\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n@_EPSILON_SYMBOL_@\n@_IDENTITY_SYMBOL_@\n@_UNKNOWN_SYMBOL_@\nа\nе\nй\nк\nн\nо\nп\nч\nь\n\n\nТакже при помощи программы grep полезно проверить какие-нибудь символы:\n```{shell}\n$ hfst-dump-alphabets -1 generator.hfst | grep ъ\n```\n```{shell}\n$ hfst-dump-alphabets -1 generator.hfst | grep к\n```\n\n\nк\n\n\n\n\n6.1.6 hfst-substitute\nЗаменяет единицы в трансдьюсере (-f from; -t to):\n```{shell}\n$ hfst-substitute -f \"ч\" -t \"Ч\" -i generator.hfst | hfst-fst2strings\n```\n\n\nноЧь&lt;N&gt;&lt;dim&gt;:ноЧка\nноЧь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ноЧной\nноЧь&lt;N&gt;&lt;nom&gt;:ноЧь\nноЧь&lt;N&gt;&lt;acc&gt;:ноЧь\nпеЧь&lt;N&gt;&lt;dim&gt;:пеЧка\nпеЧь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:пеЧной\nпеЧь&lt;N&gt;&lt;nom&gt;:пеЧь\nпеЧь&lt;N&gt;&lt;acc&gt;:пеЧь\n\n\nМожно заменять конкретные соответствия (строчки в AT&T записи).\n```{shell}\n$ hfst-substitute -f \"ь:@0@\" -t \"ъ:ъ\" -i generator.hfst | hfst-fst2strings\n```\n\n\nночъ&lt;N&gt;&lt;dim&gt;:ночъка\nночъ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночъной\nночь&lt;N&gt;&lt;nom&gt;:ночь\nночь&lt;N&gt;&lt;acc&gt;:ночь\nночъ&lt;N&gt;&lt;dim&gt;:ночъка\nночъ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночъной\nпечъ&lt;N&gt;&lt;dim&gt;:печъка\nпечъ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печъной\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;acc&gt;:печь\nпечъ&lt;N&gt;&lt;dim&gt;:печъка\nпечъ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печъной\n\n\nВажно отметить, что в таком случае не работает расписывание ъ -&gt; ъ:ъ.\nЕсли хочется чего-то удалить, то следует использовать запись @0@ или @_EPSILON_SYMBOL_@:\n```{shell}\n$ hfst-substitute -f \"ч\" -t \"@0@\" -i generator.hfst | hfst-fst2strings\n```\n\n\nноь&lt;N&gt;&lt;dim&gt;:нока\nноь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ноной\nноь&lt;N&gt;&lt;nom&gt;:ноь\nноь&lt;N&gt;&lt;acc&gt;:ноь\nпеь&lt;N&gt;&lt;dim&gt;:пека\nпеь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:пеной\nпеь&lt;N&gt;&lt;nom&gt;:пеь\nпеь&lt;N&gt;&lt;acc&gt;:пеь\n\n\nЗамены можно делать по списку, для этого нужно иметь .tsv файл. Ниже содержимое файла label_file.tsv:\n&lt;N&gt; @_EPSILON_SYMBOL_@\n&lt;acc&gt;   @_EPSILON_SYMBOL_@\n&lt;adj&gt;   @_EPSILON_SYMBOL_@\n&lt;dim&gt;   @_EPSILON_SYMBOL_@\n&lt;m&gt; @_EPSILON_SYMBOL_@\n&lt;nom&gt;   @_EPSILON_SYMBOL_@\n&lt;sg&gt;    @_EPSILON_SYMBOL_@\nМы можем удалить все теги и получить соответствие основы-формы:\n```{shell}\n$ hfst-substitute -F label_file.tsv -i generator.hfst | hfst-fst2strings\n```\n\n\nночь:ночка\nночь:ночной\nночь\nночь\nпечь:печка\nпечь:печной\nпечь\nпечь\n\n\nОпытным путем я выяснил, что это команда не будет работать с записью @0@.\n\n\n6.1.7 hfst-lookup\n```{shell}\n$ echo \"ночь&lt;N&gt;&lt;dim&gt;\" | hfst-lookup generator.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ночь&lt;N&gt;&lt;dim&gt;   ночка   0,000000\n\n\n\n&gt; \n\n\n\nКроме того, можно на вход подавать несколько форм, разделяя их переносом строки (\\n), или программой cat читать из файла:\n```{shell}\n$ echo \"ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\\nночь&lt;N&gt;&lt;dim&gt;\" | hfst-lookup generator.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt; ночной  0,000000\n\n\n\n&gt; ночь&lt;N&gt;&lt;dim&gt;   ночка   0,000000\n\n\n\n&gt; \n\n\n\nВажно также отметить, что при помощи флага -O можно задать тип, в котором нужно печатать результат:\n\nxerox\n\n```{shell}\n$ echo \"ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\\nночь&lt;N&gt;&lt;dim&gt;\" | hfst-lookup -O xerox generator.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt; ночной  0,000000\n\n\n\n&gt; ночь&lt;N&gt;&lt;dim&gt;   ночка   0,000000\n\n\n\n&gt; \n\n\n\n\ncg — Constraint Grammar\n\n```{shell}\n$ echo \"ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\\nночь&lt;N&gt;&lt;dim&gt;\" | hfst-lookup -O cg generator.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; \"&lt;ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;&gt;\"\n\n    \"ночной\"    0,000000\n\n\n\n&gt; \"&lt;ночь&lt;N&gt;&lt;dim&gt;&gt;\"\n\n    \"ночка\" 0,000000\n\n\n\n&gt; \n\n\n\n\napertium\n\n```{shell}\n$ echo \"ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\\nночь&lt;N&gt;&lt;dim&gt;\" | hfst-lookup -O apertium generator.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ^ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;/*ночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;$\n\n&gt; ^ночь&lt;N&gt;&lt;dim&gt;/*ночь&lt;N&gt;&lt;dim&gt;$\n\n&gt; \n\n\n\n\n\n6.1.8 hfst-invert\nЭто очень полезная функция, так как она позволяет переворачивать трансдьюсер. В прошлом разделе мы видели генератор, а что если мы хотим наоборот - не генератор, а анализатор?\n```{shell}\n$ hfst-invert generator.hfst -o analyzer.hfst\n$ echo \"ночка\" | hfst-lookup analyzer.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ночка  ночь&lt;N&gt;&lt;dim&gt;    0,000000\n\n\n\n&gt; \n\n\n\nЛюбой трансдьюсер обратим — и это очень классно: вы делаете один объект, а он и анализатор, и генератор.\n\n\n6.1.9 hfst-fst2fst\nВозможно, Вы заметили предупреждение, которое возвращает hfst-lookup. Оптимизированный формат можно сделать при помощи программы hfst-fst2fst, он полезен и для других программ hfst.\n```{shell}\n$ hfst-fst2fst -O analyzer.hfst -o analyzer.hfstol\n$ echo \"ночка\" | hfst-lookup analyzer.hfstol\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ночка  ночь&lt;N&gt;&lt;dim&gt;    0,000000\n\n\n\n&gt; \n\n\n\nПрограмма hfst-lookup больше не выдает предупреждения. Еще более ожидаемый вывод будет, если использовать программу hfst-optimized-lookup:\n```{shell}\n$ echo \"ночка\" | hfst-optimized-lookup analyzer.hfstol\n```\n\n\nночка   ночь&lt;N&gt;&lt;dim&gt;\n\n\n\n\n\n6.1.10 hfst-proc и hfst-proc2\nПрограмма hfst-proc парсит текст. Важно, чтобы трансдьюсер был в оптимизированной форме, т. е. .hfstol. Обратите внимание на то, как экранируется восклицательный знак.\n```{shell}\n$ echo \"какая-то Ночка и ПЕЧЬ\\!\" | hfst-proc analyzer.hfstol\n```\n\n\n^какая/*какая$-^то/*то$ ^Ночка/Ночь&lt;N&gt;&lt;dim&gt;$ ^и/*и$ ^ПЕЧЬ/ПЕЧЬ&lt;N&gt;&lt;acc&gt;/ПЕЧЬ&lt;N&gt;&lt;nom&gt;$!\n\n\nРазные аргументы форматируют по-разному выдачу:\n\n-C/--cg Constraint Grammar\n\n```{shell}\n$ echo \"какая-то Ночка и ПЕЧЬ\\!\" | hfst-proc -C analyzer.hfstol\n```\n\n\n\"&lt;какая&gt;\"\n    \"*какая\"\n\"&lt;то&gt;\"\n    \"*то\"\n\"&lt;Ночка&gt;\"\n    \"ночь\"  N dim\n\"&lt;и&gt;\"\n    \"*и\"\n\"&lt;ПЕЧЬ&gt;\"\n    \"печь\"  N acc\n    \"печь\"  N nom\n\n\n\n-x Xerox\n\n```{shell}\n$ echo \"какая-то Ночка и ПЕЧЬ\\!\" | hfst-proc -x analyzer.hfstol\n```\n\n\nкакая   +?\n\nто  +?\n\nНочка   Ночь&lt;N&gt;&lt;dim&gt;\n\nи   +?\n\nПЕЧЬ    ПЕЧЬ&lt;N&gt;&lt;acc&gt;\nПЕЧЬ    ПЕЧЬ&lt;N&gt;&lt;nom&gt;\n\n\n\n\n-w возвращать словарный вид слова, а не тот вид, в котором он встретился в тексте.\n\n```{shell}\n$ echo \"какая-то Ночка и ПЕЧЬ\\!\" | hfst-proc -xw analyzer.hfstol\n```\n\n\nкакая   +?\n\nто  +?\n\nНочка   ночь&lt;N&gt;&lt;dim&gt;\n\nи   +?\n\nПЕЧЬ    печь&lt;N&gt;&lt;acc&gt;\nПЕЧЬ    печь&lt;N&gt;&lt;nom&gt;\n\n\n\n\nфлаг -N позволяет настроить, сколько форм будет возвращаться при анализе, например ниже исчез разбор ПЕЧЬ&lt;N&gt;&lt;nom&gt;.\n\n```{shell}\n$ echo \"какая-то Ночка и ПЕЧЬ\\!\" | hfst-proc -xN 1 analyzer.hfstol\n```\n\n\nкакая   +?\n\nто  +?\n\nНочка   Ночь&lt;N&gt;&lt;dim&gt;\n\nи   +?\n\nПЕЧЬ    ПЕЧЬ&lt;N&gt;&lt;acc&gt;\n\n\n\nКроме того, существует программа hfst-proc2, которая, кажется, возвращает только то, что анализатор может проанализировать:\n```{shell}\n$ echo \"какая-то ночка и печь\\!\" | hfst-proc2 analyzer.hfstol\n```\n\n\nночка\nпечь\n\n\n\nФлаг -a позволяет напечатать все:\n```{shell}\n$ echo \"какая-то ночка и печь\\!\" | hfst-proc2 -a analyzer.hfstol\n```\n\n\nкакая-то \nночка\n и \nпечь\n\\!\n\n\n\nФлаг -C печатает в CoNLL-U:\n```{shell}\n$ echo \"какая-то ночка и печь\\!\" | hfst-proc2 -С analyzer.hfstol\n```\n\n\n1   ночка   _   _   _   _   _   _   _   ночь&lt;N&gt;&lt;dim&gt;\n2   печь    _   _   _   _   _   _   _   печь&lt;N&gt;&lt;nom&gt;\n\n\n\n\n6.1.11 hfst-pair-test\nЭта программа проверяет twol:\n```{shell}\n$ echo \"н о ч к а\" | hfst-pair-test twol.hfst\n```\n\n\nTest passed.\n\n\n```{shell}\n$ echo \"н о ч ь к а\" | hfst-pair-test twol.hfst\n```\n\n\nRule \"чк чн пишется без ь\" fails:\n#:0 н о ч HERE ---&gt; ь к а #:0 \n\nFAIL: н о ч ь к а REJECTED\n\nTest failed.\n\n\n```{shell}\n$ echo \"н о ч ь:0 к а\" | hfst-pair-test twol.hfst\n```\n\n\nTest passed.\n\n\nТесты могут быть отрицательными:\n```{shell}\n$ echo \"н о ч ь к а\" | hfst-pair-test -N twol.hfst\n```\n\n\nTest passed.\n\n\n```{shell}\n$ echo \"н о ч к а\" | hfst-pair-test -N twol.hfst\n```\n\n\nFAIL: н о ч к а PASSED\n\nTest failed.\n\n\nВообще стоит обратить внимание на мануал этой программы: она подробнее многих других программ hfst.\n\n\n6.1.12 hfst-kill-paths\nУбирает все пути, содержащие некоторый символ:\n```{shell}\n$ hfst-kill-paths -S о generator.hfst | hfst-fst2strings\n```\n\n\nпечь&lt;N&gt;&lt;acc&gt;:печь\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;dim&gt;:печка",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Операции с трансдьюсерами</span>"
    ]
  },
  {
    "objectID": "06_transducer_manipulation.html#некоторые-операции-с-двумя-трансдьюсерами",
    "href": "06_transducer_manipulation.html#некоторые-операции-с-двумя-трансдьюсерами",
    "title": "6  Операции с трансдьюсерами",
    "section": "6.2 Некоторые операции с двумя трансдьюсерами",
    "text": "6.2 Некоторые операции с двумя трансдьюсерами\nСоздадим себе второй очень похожий трансдьюсер:\n```{shell}\n$ hfst-substitute -f \"о\" -t \"О\" -i generator.hfst | hfst-name -n 'transducer 2' -o generator2.hfst\n```\n\n6.2.1 hfst-compare\nСравнивает трансдьюсеры\n```{shell}\n$ hfst-compare generator.hfst generator.hfst\n```\n\n\nthe best fst ever == the best fst ever\n\n\n```{shell}\n$ hfst-compare generator.hfst generator2.hfst\n```\n\n\nthe best fst ever != transducer 2\n\n\n```{shell}\n$ hfst-compare generator.hfst analyzer.hfst\n```\n\n\nthe best fst ever != invert(the best fst ever)\n\n\n\n\n6.2.2 hfst-concatenate\nКонкатенация трансдьюсеров - последнее состояние одного трансдьюсера становится начальным состоянием второго.\n```{shell}\nhfst-concatenate generator.hfst generator2.hfst | hfst-fst2strings | head \n```\n\n\nночь&lt;N&gt;&lt;dim&gt;нОчь&lt;N&gt;&lt;dim&gt;:ночканОчка\nночь&lt;N&gt;&lt;dim&gt;нОчь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночканОчнОй\nночь&lt;N&gt;&lt;dim&gt;нОчь&lt;N&gt;&lt;nom&gt;:ночканОчь\nночь&lt;N&gt;&lt;dim&gt;нОчь&lt;N&gt;&lt;acc&gt;:ночканОчь\nночь&lt;N&gt;&lt;dim&gt;печь&lt;N&gt;&lt;dim&gt;:ночкапечка\nночь&lt;N&gt;&lt;dim&gt;печь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночкапечнОй\nночь&lt;N&gt;&lt;dim&gt;печь&lt;N&gt;&lt;nom&gt;:ночкапечь\nночь&lt;N&gt;&lt;dim&gt;печь&lt;N&gt;&lt;acc&gt;:ночкапечь\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;нОчь&lt;N&gt;&lt;dim&gt;:ночнойнОчка\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;нОчь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночнойнОчнОй\n\n\n\n\n6.2.3 hfst-conjunct или hfst-intersect\nПересечение трансдьюсеров: выделяет общее для двух трансдьюсеров.\n```{shell}\nhfst-conjunct generator.hfst generator2.hfst | hfst-fst2strings\n```\n\n\nпечь&lt;N&gt;&lt;dim&gt;:печка\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;acc&gt;:печь\n\n\n\n\n6.2.4 hfst-disjunct или hfst-union\nОбъединение трансдьюсеров.\n```{shell}\nhfst-disjunct generator.hfst generator2.hfst | hfst-fst2strings\n```\n\n\nночь&lt;N&gt;&lt;dim&gt;:ночка\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночной\nночь&lt;N&gt;&lt;nom&gt;:ночь\nночь&lt;N&gt;&lt;acc&gt;:ночь\nпечь&lt;N&gt;&lt;dim&gt;:печка\nпечь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печной\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;acc&gt;:печь\nнОчь&lt;N&gt;&lt;dim&gt;:нОчка\nнОчь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:нОчнОй\nнОчь&lt;N&gt;&lt;nom&gt;:нОчь\nнОчь&lt;N&gt;&lt;acc&gt;:нОчь\nпечь&lt;N&gt;&lt;dim&gt;:печка\nпечь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печнОй\nпечь&lt;N&gt;&lt;nom&gt;:печь\nпечь&lt;N&gt;&lt;acc&gt;:печь\n\n\n\n\n6.2.5 hfst-subtract или hfst-minus\nВычитание трансдьюсеров.\n```{shell}\nhfst-subtract generator.hfst generator2.hfst | hfst-fst2strings\n```\n\n\nночь&lt;N&gt;&lt;dim&gt;:ночка\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ночной\nночь&lt;N&gt;&lt;nom&gt;:ночь\nночь&lt;N&gt;&lt;acc&gt;:ночь\nпечь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:печной",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Операции с трансдьюсерами</span>"
    ]
  },
  {
    "objectID": "06_transducer_manipulation.html#создание-транслитераторов",
    "href": "06_transducer_manipulation.html#создание-транслитераторов",
    "title": "6  Операции с трансдьюсерами",
    "section": "6.3 Создание транслитераторов",
    "text": "6.3 Создание транслитераторов\nДля того, чтобы создать транслитерацию, давайте запишем наши соответствия в такой файл:\n```{lexd}\nPATTERNS\nsegments\n\nLEXICON segments\nа:a\nе:e\nй:j\nк:k\nн:n\nо:o\nп:p\nч:tʃ\nь:\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n```\n```{shell}\n$ lexd correspondences.lexd | hfst-txt2fst -o correspondences.hfst\n$ hfst-fst2strings correspondences.hfst\n```\n\n\nа:a\nе:e\nй:j\nк:k\nн:n\nо:o\nп:p\nч:tɕ\nь:\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n\n\nПолучившийся трансдьюсер делает достаточно скучную вещь: если ему дать один символ он его переведет, а вот большее количество — нет:\n```{shell}\necho \"ч\" | hfst-lookup correspondences.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ч  tɕ  0,000000\n\n\n\n&gt; \n\n\n\n```{shell}\necho \"че\" | hfst-lookup correspondences.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; че че+?    inf\n\n\n\n&gt; \n\n\n\nЧтобы наш трансдьюсер делал преобразования во всей строке, нужно его зациклить при помощи программы hfst-repeat:\n```{shell}\n$ lexd correspondences.lexd | hfst-txt2fst | hfst-repeat -f 1 -o correspondences.hfst\n```\n```{shell}\necho \"ч\" | hfst-lookup correspondences.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ч  tɕ  0,000000\n\n\n\n&gt; \n\n\n\n```{shell}\necho \"че\" | hfst-lookup correspondences.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; че tɕe 0,000000\n\n\n\n&gt; \n\n\n\nЧтобы соединить транслитератор с уже готовыми трансдьюсерами, достаточно сделать композицию:\n```{shell}\nhfst-compose analyzer.hfst correspondences.hfst | hfst-fst2strings\n```\n\n\nночка:notɕ&lt;N&gt;&lt;dim&gt;\nночной:notɕ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\nночь:notɕ&lt;N&gt;&lt;nom&gt;\nночь:notɕ&lt;N&gt;&lt;acc&gt;\nпечка:petɕ&lt;N&gt;&lt;dim&gt;\nпечной:petɕ&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\nпечь:petɕ&lt;N&gt;&lt;nom&gt;\nпечь:petɕ&lt;N&gt;&lt;acc&gt;\n\n\n```{shell}\nhfst-compose generator.hfst correspondences.hfst | hfst-fst2strings\n```\n\n\nночь&lt;N&gt;&lt;dim&gt;:notɕka\nночь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:notɕnoj\nночь&lt;N&gt;&lt;nom&gt;:notɕ\nночь&lt;N&gt;&lt;acc&gt;:notɕ\nпечь&lt;N&gt;&lt;dim&gt;:petɕka\nпечь&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:petɕnoj\nпечь&lt;N&gt;&lt;nom&gt;:petɕ\nпечь&lt;N&gt;&lt;acc&gt;:petɕ\n\n\nА что если в ходе транслитерация графема чувствительна к контексту? Например, русское &lt;е&gt; может транскрибироваться как [e] (мел) и как [je] (еда). Я полагаю, у решения этой проблемы есть две стратегии:\n\nдобавлять все соответствия в файл соответствий, например,\n\n...\nче:tɕe\nпе:pʲe\nе:je\n...\n\nдобавлять еще один twol трансдьюсер, который исправляет какие-то вещи\n\n```{lexd}\nPATTERNS\nsegments\n\nLEXICON segments\nа:a\nе:{E}\nй:j\nк:k\nн:n\nо:o\nп:p\nч:tɕ\nь:\n```\n```{twol}\nAlphabet\n  %{E%}:je\n  %{E%}:e\n  %{E%}:ʲe\n;\n\nRules\n\"E в начале слова\"\n%{E%}:je &lt;=&gt; .#. _ ;\n\n\"E после мягких\"\n%{E%}:e &lt;=&gt; ɕ _ ;\n```\n```{shell}\n$ hfst-twolc -q correspondences_twol.twol -o correspondences_twol.hfst\n$ lexd correspondences2.lexd | hfst-txt2fst | hfst-repeat -f 1 | hfst-compose-intersect correspondences_twol.hfst -o correspondences.hfst\n$ echo \"ечене\" | hfst-lookup correspondences.hfst\n```\n\n\n\nhfst-lookup: Warning: It is not possible to perform fast lookups with OpenFST, std arc, tropical semiring format automata.\n\nUsing HFST basic transducer format and performing slow lookups\n\n&gt; ечене  jetɕenʲe    0,000000\n\n\n\n&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Операции с трансдьюсерами</span>"
    ]
  },
  {
    "objectID": "06_transducer_manipulation.html#перевод-основы",
    "href": "06_transducer_manipulation.html#перевод-основы",
    "title": "6  Операции с трансдьюсерами",
    "section": "6.4 Перевод основы",
    "text": "6.4 Перевод основы\nЛингвисты в отличие от NLP-специалистов и компьютерных лингвистов предпочитают, чтобы основы в примерах, которые они приводят, были переведены. Этого можно достичь точно таким же набором инструментов, какой мы использовали при создании транслитератора. Создадим файл с соответствиями:\n```{lexd}\nPATTERNS\ntranslations\n\nLEXICON translations\nпечь:stove\nночь:night\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n```\n```{shell}\n$ lexd ru_en.lexd | hfst-txt2fst | hfst-repeat -f 1 -o ru_en.hfst\n$ hfst-compose analyzer.hfst ru_en.hfst | hfst-fst2strings\n```\n\n\nночка:night&lt;N&gt;&lt;dim&gt;\nночной:night&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\nночь:night&lt;N&gt;&lt;nom&gt;\nночь:night&lt;N&gt;&lt;acc&gt;\nпечка:stove&lt;N&gt;&lt;dim&gt;\nпечной:stove&lt;N&gt;&lt;adj&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;\nпечь:stove&lt;N&gt;&lt;nom&gt;\nпечь:stove&lt;N&gt;&lt;acc&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Операции с трансдьюсерами</span>"
    ]
  },
  {
    "objectID": "07_disambiguation.html",
    "href": "07_disambiguation.html",
    "title": "7  Введение в cg3: разрешение морфологической неоднозначности",
    "section": "",
    "text": "7.1 Constraint grammar\nПарадигма Constraint grammar (CG) (Karlsson и др. 1995; Bick и Didriksen 2015) — это правиловая процедурная система обработки текста, позволяющая решать достаточно большой набор разнообразных задач, такие как\nCG разрабатывалась начиная с 1980-ых Фредом Кралссоном и другими в университете Хельсинки. С самого начала эта парадигма задумывалась как модуль, который работает с любым текстом на любом языке, так что каждой входной единице сопоставляется некоторая выходная единица (включая пунктуацию и другие топографические особенности). Слово constraint в описании подчеркивает фильтрующую функцию, которую носит CG модуль: на вход мы подаем некоторый текст и морфологические разборы, а CG модуль удаляет, изменяет или модифицирует морфологические разборы.\nВажно отметить, что CG правила легко соединяются вместе с морфологическими трансдьюсерами, но они так же работают и в других фреймворках: пока я готовился к этой лекции, я нашел .cg3 файл в репозитории, в котором использовался uniparser Тимофея Архангельского.\nОтмечу неожиданное преимущество правиловых фреймворков: их достаточно легко превращать в текст, объясняющий процедуру. Попытку сделать такое для правил CG можно посмотреть в следующей работе (Swanson 2025).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Введение в `cg3`: разрешение морфологической неоднозначности</span>"
    ]
  },
  {
    "objectID": "07_disambiguation.html#constraint-grammar",
    "href": "07_disambiguation.html#constraint-grammar",
    "title": "7  Введение в cg3: разрешение морфологической неоднозначности",
    "section": "",
    "text": "разрешение неоднозначности;\nприписывание тэгов, например, синтаксических ролей;\nразрешение анафоры;\nпостроение деревьев зависимостей;\nchunking — выделение границ синтаксических единиц (без внутренней структуры, отношений вершины-зависимое и т. п.) (Bick 2013);\nи многие другие.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Введение в `cg3`: разрешение морфологической неоднозначности</span>"
    ]
  },
  {
    "objectID": "07_disambiguation.html#cg3",
    "href": "07_disambiguation.html#cg3",
    "title": "7  Введение в cg3: разрешение морфологической неоднозначности",
    "section": "7.2 cg3",
    "text": "7.2 cg3\nВ наших лекциях для разрешения морфологической неоднозначности мы будем использовать cg3 (полное название vislcg3). VISL (Visual Interactive Syntax Learning) — подпроект датской компании GrammarSoft ApS, а CG3 обозначает новую, третью, версию реализации парадигмы. Чтобы установить достаточно следующих команд.\n```{shell}\n$ curl -s https://apertium.projectjj.com/apt/install-nightly.sh | sudo bash\n$ sudo apt-get install cg3\n```\nПервая команда добавляет в операционную систему новый источник программ, он такой же как и для команд lexd и twol. Вторая команда — стандартный способ установить программу. У программы есть большая документация, но начинается она со следующего предостережения:\n\nThis manual should be regarded as a guideline. Some of the features or functionality described is either not implemented yet, or does not work exactly as advertised. A good place to see what is and is not implemented is to run the regression test suite as the test names are indicative of features. The individual tests are also good starting points to find examples on how a specific feature works.\n\ncg3 по умолчанию угадывает формат, в котором ей поступают данные на вход и отдает в таком же формате. Однако есть и программы более узкого назначения:\n\ncg-conv — программа для преобразования входных/выходных типов данных (например, между апертиумовским и CG) ;\ncg-comp компилирует файл CG правил в бинарный файл ;\ncg-proc обрабатывает входные данные при помощи файла CG правил или его бинарного варианта ;\nи некоторые другие.\n\nВ программе cg3 достаточно много аргументов (см. cg3 -h), нам важно три:\n\n-g — указывает файл с CG правилами ;\n-I — файл для чтения вместо стандартного stdin ;\n-O — файл для записи результата работы вместо стандартного stdout.\n\n\n7.2.1 Первый пример\nРассмотрим такой трансдьюсер, записанный в example_1.lexd:\n\n\nPATTERNS\nPreposition\nNoun\nVerb\n\nLEXICON Preposition\nв&lt;pr&gt;:в\n\nLEXICON Noun\nгород&lt;n&gt;&lt;nom&gt;&lt;sg&gt;:город\nгород&lt;n&gt;&lt;acc&gt;&lt;sg&gt;:город\nпоезд&lt;n&gt;&lt;nom&gt;&lt;sg&gt;:поезд\n\nLEXICON Verb\nехать&lt;v&gt;&lt;npst&gt;&lt;p3&gt;&lt;sg&gt;:едет\n\n\nМы его можем скомпилировать в оптимизированный:\n```{shell}\n$ lexd example_1.lexd | hfst-txt2fst | hfst-invert | hfst-fst2fst -O -o analyzer.hfstol\n```\nПодадим предложение на вход процессору:\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc analyzer.hfstol\n```\n\n\n^Поезд/Поезд&lt;n&gt;&lt;nom&gt;&lt;sg&gt;$ ^едет/ехать&lt;v&gt;&lt;npst&gt;&lt;p3&gt;&lt;sg&gt;$ ^в/в&lt;pr&gt;$ ^город/город&lt;n&gt;&lt;acc&gt;&lt;sg&gt;/город&lt;n&gt;&lt;nom&gt;&lt;sg&gt;$.\n\n\nАпертиумовский формат неудобно читать, так что добавим флаг -C, чтобы hfst-proc преобразовал получившееся в CG формат:\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol\n```\n\n\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\nМы видим, что форма город имеет два возможных разбора. Кроме того мы видим, что один из разборов точно неправильный. Для того чтобы убрать ненужный разбор, создадим следующий файл example_1.cg3:\n\n\nREMOVE (nom) IF (-1C (pr)) ;\n\n\nПрименим написанное правило к нашему трансдьюсеру:\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_1.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n\n\nТаким образом мы избавились от ненужного разбора. В следующих разделах мы углубимся в возможности cg3.\n\n\n7.2.2 Структура .cg3 файла\nФайлы с CG правилами принято записывать в файлы с расширениями .cg3 или .rlx. В принципе никакой необходимой структуры такие файлы не предполагают, однако для читаемости имеет смысл делить логические фрагменты на разделы SECTION. Такие разделы могут иметь названия. Кроме того можно вводить специальные разделы BEFORE-SECTIONS и AFTER-SECTIONS, которые будут запускаться до и перед разделами SECTION. Также для отлаживания кода полезно знать команду END, после которой правила из файла не читаются. Содержание файла обычно заключается в наборе команд для декларации переменных при помощи команд LIST и SET и манипуляции\n\nс тегами;\nчтениями;\nзависимостями;\nотношениями;\nединицами анализа (в формализме CG их называют когортами, cohort);\nи др.\n\nКомментарии отделяются при помощи знака #.\nКроме того, есть некоторый набор настроек для чтения грамматики, который добавляется при помощи команды OPTIONS.\nВот фрагменты искусственного .сg3 файла, который приводиться здесь для иллюстрации:\n```{cg3}\nDELIMITERS = \"&lt;.&gt;\" \"&lt;!&gt;\" \"&lt;?&gt;\" \"&lt;...&gt;\";\nOPTIONS += addcohort-attach ;\n\n&lt;...&gt;\n\n# важный комментарий\n\nLIST Det = art ;\nLIST PRON = prde prps prn ;\nSET Sem/Time = Month | Months | Year | Century | Season | Seasons | TimeOfDay ; \n\n&lt;...&gt;\n\n1SECTION\n\nREMOVE Dat IF (NOT 0 Anim OR Cog OR Ant) (NOT 0 Pron) ;\nSELECT Gen IF (0C N) (-1C A OR Det) ;\n\n&lt;...&gt;\n\n2SECTION some-other-forms ;\n\nREMOVE Imper IF (1C Fin) ;\n\n&lt;...&gt;\n```\n\n1\n\nЕсли раздел SECTION не поименована, то можно не ставить точки с запятой.\n\n2\n\nЕсли раздел SECTION поименован, то а) в названии не должно быть пробелов б) строка должна заканчиваться точкой с запятой.\n\n\n\n\n7.2.3 Списки и операции над ними\nСписки это поименованные переменные, содержащие некоторые объекты. Из примера выше понятно, что объекты в списке разделяются пробелом, а конец списка обозначается точкой с запятой.\nОдин из первых списков, который даже имеет отдельное название — раздел DELIMITERS. В данном разделе перечисляются единицы, которые останавливают парсинг. Списки DELIMITERS в файлах .cg3 на GitHub не отличаются креативностью и обычно содержат вполне ожидаемые\n```{cg3}\nDELIMITERS = \"&lt;.&gt;\" \"&lt;!&gt;\" \"&lt;?&gt;\" \"&lt;...&gt;\" ;\n```\nОднако, может быть для ваших задач будет иметь смысл добавить какой-то знак переноса строки, на случай, если будет отсутвовать пунктуация в конце абзаца. Важно отметить, что при отсутствии раздела DELIMITERS в файле с правилами программа cg3 выдает предупреждение.\nКроме того, существует раздел SOFT-DELIMITERS, который я видел заполненный только запятой \"&lt;,&gt;\".\nОстальные списки задаются при помощи команды LIST, после которой следует дать переменной некоторое имя, например,\n```{cg3}\nLIST number = sg pl ;\n1LIST number += du ;\n```\n\n1\n\nОперация += добавляет тег или несколько тегов к уже созданному списку, хотя и не понятно, что мешало сразу все задать.\n\n\nФункция SET также позволяет задать списки, однако делает это на основании одной из следующих операций или их сочетаний\n\nобъединение OR и |;\n\n```{cg3}\nLIST A = a b c d ;\nLIST B = c d e f ;\nSET new_set = a OR b ;\n```\na b c d e f\n\nпрямое произведение +;\n\n```{cg3}\nLIST A = N ;\nLIST B = sg du pl ;\nSET new_set = a + b ;\n```\n(N sg) (N du) (N pl)\n\nисключение -;\n\n```{cg3}\nLIST A = a b c d ;\nLIST B = c d e f ;\nSET new_set = a - b ;\n```\na b\n\nразность \\;\n\n```{cg3}\nLIST A = a b c d ;\nLIST B = c d e f ;\nSET new_set = a \\ b ;\n```\na b\n\nсиммитричная разность ∆ (U+2206);\n\n```{cg3}\nLIST A = a b c d ;\nLIST B = c d e f ;\nSET new_set = a ∆ b ;\n```\na b e f\n\nпересечение ∩;\n\n```{cg3}\nLIST A = a b c d ;\nLIST B = c d e f ;\nSET new_set = a ∩ b ;\n```\nc d\n\n\n7.2.4 Операции с чтениями\n\nSELECT удаляет все чтения из когорты, кроме тех, что подходят к контексту.\n\nSELECT &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_SELECT.cg3\n```\n\n\nSELECT (\"город\" n acc sg) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_SELECT.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n\n\n\nREMOVE удаляет чтение из когорты. Может удалить даже последнее чтение.\n\nREMOVE &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_REMOVE.cg3\n```\n\n\nREMOVE (\"город\" n nom sg) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_REMOVE.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n\n\n\nRESTORE восстанавливает чтение удаленное при помощи SELECT или REMOVE. Не работает, если все чтения были удалены.\n\nRESTORE &lt;restore_target&gt; &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_RESTORE.cg3\n```\n\n\nSELECT (\"город\" n acc sg) ;\nRESTORE (\"город\" n nom sg) (\"город\" n);\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_RESTORE.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\n\n7.2.5 Операции с тегами\nПервая группа команд, которую мы рассмотрим, измененяет теги.\n\nADD — добавляет тэг к набору тегов, разрешая в дальнейшем одной из операций MAP, ADD или REPLACE работать с анализируемым чтением ;\n\nADD &lt;tags&gt; [BEFORE|AFTER &lt;tags&gt;] &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_ADD.cg3\n```\n\n\nADD (NEW_TAG) (v) ;\nADD (ANOTHER_TAG) AFTER (npst) (v) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_ADD.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst ANOTHER_TAG p3 sg NEW_TAG\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\nMAP — добавляет тэг к набору тегов, запрещая в дальнейшем одной из операций MAP, ADD или REPLACE работать с анализируемым чтением ;\n\nMAP &lt;tags&gt; [BEFORE|AFTER &lt;tags&gt;] &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_MAP.cg3\n```\n\n\nMAP (NEW_TAG) (v) ;\nADD (ANOTHER_TAG) AFTER (npst) (v) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_MAP.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg NEW_TAG\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\nSUBSTITUTE — заменяет один набор тэгов на другой набор тэгов. Может быть сделана нулевая замена, для этого следует использовать знак *.\n\nSUBSTITUTE &lt;locate tags&gt; &lt;replacement tags&gt; &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_SUBSTITUTE.cg3\n```\n\n\nSUBSTITUTE (sg) (SG) (v) ;\nSUBSTITUTE (nom) (*) (\"поезд\") ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_SUBSTITUTE.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 SG\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\nREPLACE — оставляет только выделенный тэг, а остальные теги убирает, запрещая в дальнейшем одной из операций MAP, ADD или REPLACE работать с анализируемым чтением ;\n\n```{shell}\n$ cat example_REPLACE.cg3\n```\n\n\nREPLACE (NEW_TAG) (v) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_REPLACE.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" NEW_TAG\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\nUNMAP снимает блокирующее поведение команд MAP и REPLACE. По-умолчанию UNMAP работает только с единственными чтениями, но если хочется чтобы блокировка была снята с нескольких чтений одной формы, стоит добавить UNSAFE.\n\nUNMAP &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_UNMAP.cg3\n```\n\n\nMAP (NEW_TAG) (v) ;\nUNMAP (NEW_TAG) ;\nADD (ANOTHER_TAG) AFTER (npst) (v) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_UNMAP.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst ANOTHER_TAG p3 sg NEW_TAG\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n```{shell}\n$ cat example_UNSAFE.cg3\n```\n\n\nMAP (NEW_TAG) (\"город\" n) ;\nUNMAP UNSAFE (NEW_TAG) ;\nADD (ANOTHER_TAG) (\"город\" n) ;\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_UNSAFE.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg NEW_TAG ANOTHER_TAG\n    \"город\" n nom sg NEW_TAG ANOTHER_TAG\n\n\n\nAPPEND добавляет новое прочтение к некоторой единице. Следите, чтобы среди добавляемого была начальная форма.\n\nAPPEND &lt;tags&gt; &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_APPEND.cg3\n```\n\n\nAPPEND (\"поезд\" n nom sg NEW_READING) (\"поезд\" n) ;\n\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_APPEND.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n    \"поезд\" n nom sg NEW_READING\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\nCOPY копирует прочтение и добавляет к нему тег или набор тегов.\n\nCOPY &lt;extra tags&gt; [EXCEPT &lt;except tags&gt;] [BEFORE|AFTER &lt;tags&gt;] &lt;target&gt; [contextual_tests] ;\n```{shell}\n$ cat example_COPY.cg3\n```\n\n\nCOPY (NEW) AFTER (n) (\"поезд\" n nom sg);\n\n\n```{shell}\n$ echo \"Поезд едет в город.\" | hfst-proc -C analyzer.hfstol | cg3 -g example_COPY.cg3\n```\n\n\nWarning: No soft or hard delimiters defined in grammar. Hard limit of 500 cohorts may break windows in unintended places.\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg\n    \"поезд\" n NEW nom sg\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;город&gt;\"\n    \"город\" n acc sg\n    \"город\" n nom sg\n\n\n\n\n7.2.6 Контекстные тесты\nБольшинство примеров операций выше имели в своих шаблонах раздел [contextual_tests], который делает инструменты значительно функциональнее. Во-первых, можно задать контекст справа или слева:\n```{cg3}\n1... (1 N)\n2... (-2* V)\n```\n\n1\n\nЕсть ли хотя бы одно чтение с тегом N в следующей когорте?\n\n2\n\nЕсть ли хотя бы одно чтение с тегом V две или более когорты назад?\n\n\nПосле номера можно добавить C, тогда тест будет исполнятся только в случае если все чтения в когорте соответствуют условию.\n```{cg3}\n1... (1С N)\n2... (-2С* V)\n```\n\n1\n\nВсе ли чтения имеют тег N в следующей когорте?\n\n2\n\nВсе ли чтения имеют тег V две или более когорты назад?\n\n\nРезультат теста можно перевернуть, добавив NOT:\n```{cg3}\n1... (NOT 1 N)\n2... (NOT -2* V)\n```\n\n1\n\nВ следующей когорте не должно быть чтения с тегом N.\n\n2\n\nДве и более когорты назад не должно быть чтения с тегом V.\n\n\n\n\n\n\n\n\nЗадание 07_01\n\n\n\nНапишите cg3 правила, которые в нашем примере удаляют лишнее чтение и разметят синтаксические роли S, O и V. Избавьтесь также от предупреждения про разделители.\n\n\n\"&lt;Поезд&gt;\"\n    \"поезд\" n nom sg S\n\"&lt;едет&gt;\"\n    \"ехать\" v npst p3 sg V\n\"&lt;в&gt;\"\n    \"в\" pr\n\"&lt;большой&gt;\"\n    \"*большой\"\n\"&lt;город&gt;\"\n    \"город\" n acc sg O\n\n\n\n\nМы посмотрели далеко не все возможности cg3: в документации можно найти инструкции по созданию новых когорт, добавлению зависимостей и отношений — очень полезное для создания синтаксических корпусов, трибанков.\n\n\n\n\nBick, Eckhard. 2013. «Using Constraint Grammar for Chunking». В Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013), 13–26.\n\n\nBick, Eckhard, и Tino Didriksen. 2015. «CG-3 — beyond classical constraint grammar». В Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015), 31–39.\n\n\nKarlsson, Fred, Atro Voutilainen, Juha Heikkilae, и Arto Anttila. 1995. Constraint Grammar: a language-independent system for parsing unrestricted text. Т. 4. Walter de Gruyter.\n\n\nSwanson, Daniel Glen. 2025. «Towards Natural Language Explanations of Constraint Grammar Rules». В Proceedings of the 9th Workshop on Constraint Grammar and Finite State NLP, 28–31.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Введение в `cg3`: разрешение морфологической неоднозначности</span>"
    ]
  },
  {
    "objectID": "08_coverage.html",
    "href": "08_coverage.html",
    "title": "8  Метрики качества",
    "section": "",
    "text": "8.1 Покрытие\nКак и в остальных инструментах машинного обучения, при работе над морфологическими анализаторами нам нужна некоторая мера, которая покажет качество получившегося трансдьюсера. Морфологические анализаторы могут делать это на материале некоторого корпуса. В качестве игрушечного примера мы рассмотрим следующий корпус, состоящий из порошка пользователя с ником Кисычев:\nДля целей данного занятия мы рассмотрим следующий игрушечный трансдьюсер:\nСкомпилируем наш трансдьюсер:\nСразу отметим недостатки данного трансдьюсера:\nПокрытие (coverage, naïve coverage) — это простейший способ оценить качество трансдьюсера. Его высчитывают как долю форм, которая разбирается трансдьюсером. Посчитаем сколько токенов всего в корпусе:\n17\nПосчитаем, сколько токенов не разбирает трансдьюсер:\n6\nТаким образом, покрытие нашего трансдьюсера приблизительно соответствует \\(\\frac{17-6}{17} \\approx 0.65\\). Не стоит сильно доверять этой мере, так как она совершенно не учитывает качество разбора, таким образом завышая качество.\nИногда вместо описанного покрытия считают аналогичную меру, удаляя повторяющиеся токены и разборы, чтобы не получалось слишком большое значение из-за того, что трансдьюсер разбирает самые частотные слова. В таком случае, описанную выше меру называют coverage1, а с удалением эффекта частотности — coverage2.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Метрики качества</span>"
    ]
  },
  {
    "objectID": "08_coverage.html#покрытие",
    "href": "08_coverage.html#покрытие",
    "title": "8  Метрики качества",
    "section": "",
    "text": "```{shell}\n$ cat corpus.txt | wc -w\n```\n\n\n```{shell}\n$ cat corpus.txt | hfst-proc -C analyzer.hfstol | grep -c \"*\"\n```",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Метрики качества</span>"
    ]
  },
  {
    "objectID": "08_coverage.html#точность-и-полнота",
    "href": "08_coverage.html#точность-и-полнота",
    "title": "8  Метрики качества",
    "section": "8.2 Точность и полнота",
    "text": "8.2 Точность и полнота\nТочность (precision) и полнота (recall) — метрики, используемые при оценке большей части алгоритмов классификации в машинном обучении. Иногда их используют сами по себе, а иногда в виде производных метрик, например F1-меры. Напомним, как выглядит матрица ошибок на основе которой считаются меры:\n\n\n\n\n\n\n\n\n\n🐕\n🐈\n\n\n\n\n🤖💭🐕\nистинно-положительные (TP)\nложно-положительные (FP)\n\n\n🤖💭🐈\nложно-отрицательные объекты (FN)\nистинно-отрицательные (TN)\n\n\n\n\n\n\\[точность = \\frac{TP}{TP+FP}\\]\n\n\\[полнота = \\frac{TP}{TP+FN}\\]\n\n\n\\[F_1 = 2\\times\\frac{точность \\times полнота}{точность + полнота} = \\frac{2\\times TP}{2\\times TP+FP+FN}\\]\nВ применении к морфологическому анализу, данные метрики должны учитывать качество разбора, а это значит, что нам нужно завести золотой стандарт, с которым мы будем сравнивать. Золотой стандарт не обязан иметь один единственный разбор для каждой формы, как лингвисты ожидают от оглоссированного текста. Поэтому на каждый токен в тексте мы получаем список разборов из золотого стандарта и из нашего анализатора.\n\n\n\n\n\n\n\n\n\nесть в золотом стандарте\nнет в золотом стандарте\n\n\n\n\nесть в анализаторе\nсовпадения (TP)\n(FP)\n\n\nнет в анализаторе\n(FN)\n(TN)1\n\n\n\n\nТочность (precision) — количество разборов анализатора, совпадающих с разбором золотого стандарта (TP), деленное на количество всех разборов анализатора (TP + FP).\nПолнота (recall) — количество разборов анализатора, совпадающих с разбором золотого стандарта (TP), деленное на колиество разборов в золотом стандарте (TP + FN).\n\nКак разбор анализатора может совпадать с разбором золотого стандарта? Можно считать только случаи полного совпадения. Однако разумным кажется смотреть и на другие совпадения:\n\nсовпадение основы\nсовпадение частеречного тега\nсовпадение набора не частеречных тегов\nсовпадение основы и частеречного тега\nсовпадение частеречного тега и набора не частеречных тегов\n\nКроме того, можно еще проверять работу морфологического сегментатора (руками &lt;-&gt; рук-ами).\nВот как может выглядеть таблица с золотым стандартом. Я намеренно ввел несостыковки:\n\nтег PRON вместо PR;\nформа глагол имеет лишь один тег &lt;aсс&gt;, а тег &lt;sg&gt; пропущен;\nв форме рассказ перепутан порядок тегов (sg, nom вместо nom, sg).\n\n\n\n\n  \n\n\n\nК сожалению, нам неизвестно какого-то единого инструмента, который бы подсчитывал необходимые метрики, поэтому вот некоторый код, который делает это для\n\nоснов;\nчастеречных тегов;\nне частеречных тегов;\nвсего вместе.\n\nЕсли будете делать сами, не забудьте несколько вещей. Во-первых, имеет смысл выкинуть дубликаты и отсортировать теги. Во-вторых, следует помнить, что отсутствие нечастеречных тегов может быть верным, а не ошибкой, например, в случае предлогов. В таком случае не стоит вводить штраф за отсутствие тегов.\nКод на R и на Python используют немного разную логику.\n\nRPython\n\n\n\nlibrary(tidyverse)\n\nread_csv(\"examples/08_gold_standard.csv\") |&gt; \n  mutate(tags = map_chr(tags, function(x) str_split(x, \", \") |&gt; unlist() |&gt; unique() |&gt; sort() |&gt; str_c(collapse = \", \")),\n         gold_standard = \"gold_standard\") -&gt;\n  gold_standard\n\nsystem(\"cat examples/08_corpus.txt | hfst-proc examples/analyzer.hfstol\", intern = TRUE) |&gt; \n  str_split(\" \") |&gt; \n  unlist() |&gt; \n  enframe() |&gt;\n  rename(token_id = name) |&gt; \n  mutate(token = str_extract(value, \"(?&lt;=\\\\^).*?(?=/)\"),\n         analysis = str_remove(value, \"^.*?/\"),\n         analysis = str_remove(analysis, \"\\\\$\"),\n         analysis = str_split(analysis, \"/\")) |&gt; \n  unnest_longer(analysis) |&gt; \n  mutate(stem = str_remove_all(analysis, \"&lt;.*?&gt;\"),\n         stem = if_else(str_detect(stem, \"\\\\*\"), \"\", stem),\n         stem = if_else(is.na(stem), \"\", stem),\n         pos = str_extract(analysis, \"(&lt;N&gt;)|(&lt;V&gt;)|(&lt;PRON&gt;)|(&lt;ADV&gt;)|(&lt;CONJ&gt;)|(&lt;PREP&gt;)\"),\n         pos = if_else(is.na(pos), \"\", pos),\n         tags = str_remove(analysis, pos),\n         tags = str_remove(tags, stem),\n         tags = if_else(is.na(tags), \"\", tags),\n         tags = str_remove_all(tags, \"(^&lt;)|(&gt;$)\"),\n         pos = str_remove_all(pos, \"[&lt;&gt;]\"),\n         tags = map_chr(tags, function(x) str_split(x, \"&gt;&lt;\") |&gt; unlist() |&gt; unique() |&gt; sort() |&gt; str_c(collapse = \", \")),\n         analyzer = \"analyzer\") |&gt; \n  select(token_id, token, stem, pos, tags, analyzer) -&gt;\n  analysis\n  \nanalysis |&gt; \n  full_join(gold_standard) |&gt; \n  distinct() |&gt; \n  arrange(token_id) |&gt; \n  count(analyzer, gold_standard) |&gt; \n  mutate(measure = case_when(!is.na(analyzer) & !is.na(gold_standard) ~ \"TP\",\n                             is.na(analyzer) & !is.na(gold_standard) ~ \"FN\",\n                             !is.na(analyzer) & is.na(gold_standard) ~ \"FP\",\n                             is.na(analyzer) & is.na(gold_standard) ~ \"TN\")) |&gt; \n  select(measure, n) |&gt; \n  pivot_wider(names_from = measure, values_from = n) |&gt; \n  summarise(overall_precision = TP/(TP+FP),\n            overall_recall = TP/(TP+FN),\n            overall_F_1 = 2*(overall_precision*overall_recall)/(overall_precision+overall_recall))\n\n\n  \n\n\nanalysis |&gt; \n  select(token_id, token, stem, analyzer) |&gt; \n  full_join(gold_standard |&gt; select(token_id, token, stem, gold_standard)) |&gt; \n  distinct() |&gt; \n  arrange(token_id) |&gt; \n  count(analyzer, gold_standard) |&gt; \n  mutate(measure = case_when(!is.na(analyzer) & !is.na(gold_standard) ~ \"TP\",\n                             is.na(analyzer) & !is.na(gold_standard) ~ \"FN\",\n                             !is.na(analyzer) & is.na(gold_standard) ~ \"FP\",\n                             is.na(analyzer) & is.na(gold_standard) ~ \"TN\")) |&gt; \n  select(measure, n) |&gt; \n  pivot_wider(names_from = measure, values_from = n) |&gt; \n  summarise(stem_precision = TP/(TP+FP),\n            stem_recall = TP/(TP+FN),\n            stem_F_1 = 2*(stem_precision*stem_recall)/(stem_precision+stem_recall))\n\n\n  \n\n\nanalysis |&gt; \n  select(token_id, token, stem, pos, analyzer) |&gt; \n  full_join(gold_standard |&gt; select(token_id, token, stem, pos, gold_standard)) |&gt; \n  distinct() |&gt; \n  arrange(token_id) |&gt; \n  count(analyzer, gold_standard) |&gt; \n  mutate(measure = case_when(!is.na(analyzer) & !is.na(gold_standard) ~ \"TP\",\n                             is.na(analyzer) & !is.na(gold_standard) ~ \"FN\",\n                             !is.na(analyzer) & is.na(gold_standard) ~ \"FP\",\n                             is.na(analyzer) & is.na(gold_standard) ~ \"TN\")) |&gt; \n  select(measure, n) |&gt; \n  pivot_wider(names_from = measure, values_from = n) |&gt; \n  summarise(stem_pos_precision = TP/(TP+FP),\n            stem_pos_recall = TP/(TP+FN),\n            stem_pos_F_1 = 2*(stem_pos_precision*stem_pos_recall)/(stem_pos_precision+stem_pos_recall))\n\n\n  \n\n\nanalysis |&gt; \n  select(token_id, tags, analyzer) |&gt; \n  full_join(gold_standard |&gt; select(token_id, tags, gold_standard)) |&gt; \n  distinct() |&gt; \n  arrange(token_id) |&gt; \n  count(analyzer, gold_standard) |&gt; \n  mutate(measure = case_when(!is.na(analyzer) & !is.na(gold_standard) ~ \"TP\",\n                             is.na(analyzer) & !is.na(gold_standard) ~ \"FN\",\n                             !is.na(analyzer) & is.na(gold_standard) ~ \"FP\",\n                             is.na(analyzer) & is.na(gold_standard) ~ \"TN\")) |&gt; \n  select(measure, n) |&gt; \n  pivot_wider(names_from = measure, values_from = n) |&gt; \n  summarise(tags_precision = TP/(TP+FP),\n            tags_recall = TP/(TP+FN),\n            tags_F_1 = 2*(tags_precision*tags_recall)/(tags_precision+tags_recall))\n\n\n  \n\n\n\n\n\n```{python}\n# Возможно, удобнее посмотреть тетрадку: https://github.com/agricolamz/2025_morphological_transducers/blob/main/examples/08_quality_metrics.ipynb\nimport pandas as pd\nimport re\n\ndf_gold = pd.read_csv('08_gold_standard.csv')\ndf_gold['tags'] = df_gold['tags'].apply(lambda x: set(x.replace(' ','').split(',')) if str(x) !='nan' else set())\ndf_gold['tags'] = df_gold['tags'].apply(lambda x: '_'.join(sorted(x)))\ndf_gold['full'] = df_gold['stem'] + '-' + df_gold['pos'] + '-' + df_gold['tags']\ndf_gold\n\nwith open('analysis.txt') as f:  # (! cat 08_corpus.txt | hfst-proc -C analyzer.hfstol &gt; analysis.txt)\n    text = f.read()\n    text += '\\n'\n\nanalysis = {}\nwords = re.findall('''\"&lt;(.*?)&gt;\"\\n((\\t.*?\\n)+)''', text)\nfor id, word in enumerate(words):\n    stem_s = set()\n    pos_s = set()\n    tags_s = set()\n    full_s = set()\n    if not word[1].startswith('\\t\"*'):\n        for razbor in word[1].strip('\\n').split('\\n'):\n            stem, pos_tags = razbor.replace('\"', '').strip('\\t').split('\\t')\n            pos_tags_split = pos_tags.split(' ')\n            pos = pos_tags_split[0]\n            if len(pos_tags_split) &gt; 1:\n                tags = '_'.join(sorted(set(pos_tags_split[1:])))\n            else:\n                tags = ''\n            full = stem + '-' + pos  + '-' + tags\n            stem_s.add(stem)\n            pos_s.add(pos)\n            tags_s.add(tags)\n            full_s.add(full)\n\n    analysis[id] = {'token': word[0],\n                    'stem': stem_s,\n                    'pos': pos_s,\n                    'tags': tags_s,\n                    'full': full_s}\n\ndef get_metrics(an_part):  # an_part - то, для чего мы считаем метрики: 'stem' / 'pos' / 'tags' / 'full'\n    # можно получить метрики для каждого слова, а потом посчитать среднее\n    precision_s = []  # для подсчёта средних метрик\n    recall_s = []\n    f1_s = []\n    # а можно считать метрики в конце, используя суммарные tp (good_s), fp (bad_s) и fn (not_found_s)\n    good_s = 0\n    bad_s = 0\n    not_found_s = 0\n\n    for id, row in df_gold.iterrows():\n        gold = row[an_part]\n        pred = analysis[id][an_part]\n\n        good = 1 if gold in pred else 0  # наличие правильного предсказания\n        bad = len(pred - {gold})  # кол-во неправильных предсказаний\n        not_found = len({gold}) - good  # кол-во того, что не предсказали (хотя должны были)\n        good_s += good\n        bad_s += bad\n        not_found_s += not_found\n\n        if len(pred) != 0:\n            precision = good / len(pred)  # good/(good+bad)\n        else:\n            precision = 1\n\n        if len({gold}) != 0:\n            recall = good / len({gold})  # good/(good+not_found)\n        else:\n            recall = 1\n        if precision+recall != 0:\n            f1 = 2*(precision*recall)/(precision+recall)\n        else:\n            f1 = 0\n\n        precision_s.append(precision)\n        recall_s.append(recall)\n        f1_s.append(f1)\n\n    mean_precision = sum(precision_s)/len(precision_s)\n    mean_recall = sum(recall_s)/len(recall_s)\n    mean_f1 = sum(f1_s)/len(f1_s)\n    fin_precision = good_s/(good_s+bad_s)\n    fin_recall = good_s/(good_s+not_found_s)\n    fin_f1 = 2*(fin_precision*fin_recall)/(fin_precision+fin_recall)\n\n    return {'mean_precision': mean_precision,\n            'mean_recall': mean_recall,\n            'mean_f1': mean_f1,\n            'fin_precision': fin_precision,\n            'fin_recall': fin_recall,\n            'fin_f1': fin_f1}\n\nan_part = 'full'\nget_metrics(an_part)\n```\n\n\n\nКак и в случае, описанном выше, можено различать precision1, recall1 и F1-меру1 и их аналоги с суфиксом 2, если удалять эффект частотности.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Метрики качества</span>"
    ]
  },
  {
    "objectID": "08_coverage.html#sec-tests",
    "href": "08_coverage.html#sec-tests",
    "title": "8  Метрики качества",
    "section": "8.3 Развитие анализатора",
    "text": "8.3 Развитие анализатора\nСоздание морфологического парсера, согласно моим ожиданиям, это многоступенчатая процедура, которая включает в себя работу с грамматиками, словарем и проверкой некоторых мер качества на корпусе. Разработка без того или иного ресурса возможна, но значительно усложняет работу. Этот факт немного подрывает наш тезис о том, что трансдьюсеры хороши в тех случаях, когда недостаточно данных для обучения нейросетей. В любом случае, получившийся результат по духу будет значительно ближе к лингвистическому описанию, поэтому мы называем его иногда машиночитаемым лингвистическим описанием. Как и в других видах разработки, имеет смысл покрывать некоторые фрагменты кода тестами. Для этого даже придумали свою меру — code coverage, которая показывает долю кода, покрытого тестами. Конечно, такие инструменты не будут работать с теми программами, которые мы обсуждали, однако все равно, имеет смысл создавать таблицу с формами и ожидаемыми разборами, чтобы можно было проверить, например, не поломали ли мы что-то, изменяя twol правила. Такого рода проверку легко организовать в виде файла со списком форм в формате, который выдает программа hfst-fst2strings, и дальше при помощи grep смотреть, есть ли записанные разборы в выдаче hfst-fst2strings.\n```{shell}\ngrep -xvf file_with_tests.txt generated_forms.txt\n```\nВ какой-то момент может наступить ситуация, когда основные части речи сделаны и дополнены данными словарей. В таком случае остается только нисходяще отсортировать нераспознанные формы, чтобы работать с единицами, которые, согласно распределению Ципфа, сильнее всего увеличивают метрики.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Метрики качества</span>"
    ]
  },
  {
    "objectID": "08_coverage.html#footnotes",
    "href": "08_coverage.html#footnotes",
    "title": "8  Метрики качества",
    "section": "",
    "text": "Таинственные случаи, которые целенаправлено оставлены неразобранными в золотом стандарте и не были разобраны анализатором.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Метрики качества</span>"
    ]
  },
  {
    "objectID": "09_hygiene.html",
    "href": "09_hygiene.html",
    "title": "9  Как распространять работу?",
    "section": "",
    "text": "9.1 Cимволы подстановки (wildcards)\nСуществует достаточно много причин, по которой необходимо уметь распространять свою работу:\nКак и во многих языках программирования командная строка поддерживает символы подстановки. Хоть я приведу все примеры с программой ls, легко представить себе и более осмысленные задачи. Для примера, представим, что у нас есть папка с большим количеством файлов:\na_1.csv\na_1.txt\nb_2_2.csv\nc_1.txt\nd_2.txt\nee_1.txt\nf_2.txt\ng_1.txt\nhh_2.txt\ni_1.txt\nj_2.txt\nk_1.txt\nll_2.txt\nm_1.txt\nn_2.csv\noo_1.csv\npp_2.csv\nq_1.txt\nr_2.txt\ns_1.txt\nt_2.txt\nu_1.txt\nv_2.txt\nw_1.txt\nx_2.csv\ny_1.txt\nz_2.txt\nМы можем вывести все файлы содержащие двойку, добавив * – любое количество символов:\nexamples/09_multiple_files/b_2_2.csv\nexamples/09_multiple_files/d_2.txt\nexamples/09_multiple_files/f_2.txt\nexamples/09_multiple_files/hh_2.txt\nexamples/09_multiple_files/j_2.txt\nexamples/09_multiple_files/ll_2.txt\nexamples/09_multiple_files/n_2.csv\nexamples/09_multiple_files/pp_2.csv\nexamples/09_multiple_files/r_2.txt\nexamples/09_multiple_files/t_2.txt\nexamples/09_multiple_files/v_2.txt\nexamples/09_multiple_files/x_2.csv\nexamples/09_multiple_files/z_2.txt\nЭто же используют для поиска файлов с каким-либо определенным расширением:\nexamples/09_multiple_files/a_1.csv\nexamples/09_multiple_files/b_2_2.csv\nexamples/09_multiple_files/n_2.csv\nexamples/09_multiple_files/oo_1.csv\nexamples/09_multiple_files/pp_2.csv\nexamples/09_multiple_files/x_2.csv\nКроме того звездочку можно использовать и без каких либо окружающих символов, тогда будут перечислены все файлы. Например, следующий команда удаляет все файлы в папке multiple_files.\nЕсли известно точное количество символов, то можно использовать символ ?, который обозначает один символ. Выведем только файлы с четырьмя символами до расширения:\nexamples/09_multiple_files/ee_1.txt\nexamples/09_multiple_files/hh_2.txt\nexamples/09_multiple_files/ll_2.txt\nexamples/09_multiple_files/oo_1.csv\nexamples/09_multiple_files/pp_2.csv\nИскать можно и по расширению:\nexamples/09_multiple_files/a_1.csv\nexamples/09_multiple_files/a_1.txt\nКроме того можно задавать группы при помощи квадратных скобок:\nexamples/09_multiple_files/a_1.csv\nexamples/09_multiple_files/a_1.txt\nexamples/09_multiple_files/ee_1.txt\nexamples/09_multiple_files/i_1.txt\nexamples/09_multiple_files/oo_1.csv\nexamples/09_multiple_files/u_1.txt",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Как распространять работу?</span>"
    ]
  },
  {
    "objectID": "09_hygiene.html#cимволы-подстановки-wildcards",
    "href": "09_hygiene.html#cимволы-подстановки-wildcards",
    "title": "9  Как распространять работу?",
    "section": "",
    "text": "```{shell}\n$ ls multiple_files\n```\n\n\n```{shell}\n$ ls multiple_files/*2*\n```\n\n\n```{shell}\n$ ls multiple_files/*.csv\n```\n\n\n```{shell}\n$ rm multiple_files/*\n```\n\n```{shell}\n$ ls multiple_files/????.*\n```\n\n\n```{shell}\n$ ls multiple_files/a_1.???\n```\n\n\n```{shell}\n$ ls multiple_files/[aouie]*\n```",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Как распространять работу?</span>"
    ]
  },
  {
    "objectID": "09_hygiene.html#введение-в-git",
    "href": "09_hygiene.html#введение-в-git",
    "title": "9  Как распространять работу?",
    "section": "9.2 Введение в git",
    "text": "9.2 Введение в git\n\n\n\n\n\nисточник: https://xkcd.com/1597/\n\n\n\n\n\n9.2.1 .gitignore\nСимволы подстановки очень полезны при создании файла .gitignore. .gitignore — это специальный скрытый файл1, в котором вы можете перечислить файлы, которые программа git должна игнорировать (см. документацию).\nПри создании морфологического анализатора достаточно обмениваться файлами .lexd, .twol, .cg3 и инструкциями по компиляции анализатора, которые обычно записывают в Makefile (см. раздел Раздел 9.3). Получающиеся файлы вроде .hfst или .hfstol хотелось бы не закоммитить случайно созданона GitHub. В связи с этим в вашем репозитории имеет смысл перечислить файлы, которые следует игнорировать версии контроля. Вот как будет выглядеть такой файл .gitignore:\n*.hfst\n*.hfstol\nПозже может так случится, что вы будете хранить в той же папке литературу и/или данные, которые нельзя выкладывать, их тоже можно будет добавить в файл .gitignore. Кроме того у GitHub есть ограничения на размер файла (вроде 100мб?..), если он увидит большие файлы, то он не даст их добавить, так что если они есть в вашем проекте, их тоже следует добавить в .gitignore.\n\n\n9.2.2 git hooks\nКажется удобным, чтобы проверки, которые мы обсудили в Раздел 8.3, можно было проводить каждый раз перед тем как делать коммит. Для этого были придуманы git hooks (подробнее можно почитать здесь). Если Вы написали скрипт для проверки, то его можно записать в файл .git/hooks/pre-commit, тогда каждый раз он будет запускаться. Можно даже сделать так, чтобы коммит не отправлялся, если тест не проходит.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Как распространять работу?</span>"
    ]
  },
  {
    "objectID": "09_hygiene.html#sec-makefile",
    "href": "09_hygiene.html#sec-makefile",
    "title": "9  Как распространять работу?",
    "section": "9.3 Программа make",
    "text": "9.3 Программа make\nВ ходе курса мы видели, что для того, чтобы скомпилировать морфологический анализатор и генератор достаточно достаточно маленьких текстовых файлов и целая вереница команд в командной строке. Понятно, что достаточно бессмысленно пробовать держать в памяти нужный порядок команд. Для этого придумали (в 1976 году) программу, которая называется make, которая последовательно запускает череду инструкций, которые записаны в виде терминальных команд в файле Makefile (см. огромную документацию). В результате, в идеальном сценарии, пользователь скачивает папку, набирает в терминале make и получает рабочий анализатор.\nMakefile представляет собой собрание рецептов следующей структуры:\n```{Makefile}\ntarget: dependency1 dependency2 ...\n[Tab ↹]&lt;script&gt;\n```\nОбратите внимание, что перед скриптом стоит один таб (ваш текстовый редактор может заменить его на пробелы).\nРассмотрим следующий игрушечный трансдьюсер:\n```{shell}\n$ cat example.lexd\n```\n\n\nPATTERNS\nNoun [&lt;N&gt;:] Suffix[-adj] | (Suffix[adj] AdjInflection)\nNoun [&lt;N&gt;:] NounInflection\n\nLEXICON Noun\nночь\nпечь\n\nLEXICON Suffix\n&lt;dim&gt;:ка\n&lt;adj&gt;:н[adj]\n\nLEXICON AdjInflection\n&lt;m&gt;&lt;sg&gt;&lt;nom&gt;:ой\n\nLEXICON NounInflection\n&lt;nom&gt;:\n&lt;acc&gt;:\n\n\n```{shell}\n$ cat example.twol\n```\n\n\nAlphabet\n  а е й к н о п ч ь ь:0;\n\nRules\n\n\"чк чн пишется без ь\"\n! например, ночьной -&gt; ночной или печька -&gt; печка\n\nь:0 &lt;=&gt; _ к;\n        _ н;\n\n\n```{shell}\n$ cat ru_en.lexd\n```\n\n\nPATTERNS\ntranslations\n\nLEXICON translations\nпечь:stove\nночь:night\n&lt;N&gt;\n&lt;acc&gt;\n&lt;adj&gt;\n&lt;dim&gt;\n&lt;m&gt;\n&lt;nom&gt;\n&lt;sg&gt;\n\n\nСоздадим следующий файл Makefile:\n```{shell}\n$ cat Makefile\n```\n\n\nanalyzer_english_stem.hfst: analyzer.hfst ru_en.lexd\n    lexd ru_en.lexd | hfst-txt2fst | hfst-repeat -f 1 -o ru_en.hfst\n    hfst-compose analyzer.hfst ru_en.hfst -o analyzer_english_stem.hfst\n    rm ru_en.hfst\n\nanalyzer.hfst: generator.hfst\n    hfst-invert generator.hfst -o analyzer.hfst\n\ngenerator.hfst: example.lexd twol.hfst\n    hfst-twolc -q example.twol -o twol.hfst\n    lexd example.lexd | hfst-txt2fst | hfst-compose-intersect twol.hfst -o generator.hfst\n    rm twol.hfst\n\nclean: \n    rm -f *.hfst\n\n\nЗапустим наш Makefile:\n```{shell}\n$ make\n```\n\n\nhfst-twolc -q example.twol -o twol.hfst\nlexd example.lexd | hfst-txt2fst | hfst-compose-intersect twol.hfst -o generator.hfst\nrm twol.hfst\nhfst-invert generator.hfst -o analyzer.hfst\nlexd ru_en.lexd | hfst-txt2fst | hfst-repeat -f 1 -o ru_en.hfst\nhfst-compose analyzer.hfst ru_en.hfst -o analyzer_english_stem.hfst\nrm ru_en.hfst\n\n\nПрименим получившийся трансдьюсер:\n```{shell}\n$ echo \"ночка\" | hfst-lookup analyzer_english_stem.hfst\n```\n\n\nночка   night&lt;N&gt;&lt;dim&gt;   0,000000\n\n\nТеперь мы научились хранить инструкции по компиляции в Makefile.\nВ рецептах можно использовать переменные:\n\n$@ (целевой объект),\n$^ (список зависимостей),\n$&lt; (первая из зависимостей).\n\nТогда наш Makefile можно переписать следующим образом:\n```{shell}\n$ cat Makefile\n```\n\n\nanalyzer_english_stem.hfst: analyzer.hfst ru_en.lexd\n    lexd ru_en.lexd | hfst-txt2fst | hfst-repeat -f 1 -o ru_en.hfst\n    hfst-compose $&lt; ru_en.hfst -o $@\n    rm ru_en.hfst\n\nanalyzer.hfst: generator.hfst\n    hfst-invert $&lt; -o $@\n\ngenerator.hfst: example.lexd example.twol\n    hfst-twolc -q example.twol -o twol.hfst\n    lexd $&lt; | hfst-txt2fst | hfst-compose-intersect twol.hfst -o $@\n    rm twol.hfst\n\nclean: \n    rm -f *.hfst\n\n\nРади интереса можно посмотреть на Makefile, который мы использовали в начале занятий:\n\n\n.DEFAULT_GOAL: requirements\n\n.PHONY: requirements forms clean\n\nrequirements:\n    @curl -s https://apertium.projectjj.com/apt/install-nightly.sh | sudo bash\n    sudo apt-get install hfst lexd\n\ntask.generator.hfst: task.lexd\n    @lexd task.lexd | hfst-txt2fst -o lexd.hfst\n    @if [ ! -f task.twol ]; then\\\n        mv lexd.hfst $@;\\\n    else\\\n        hfst-twolc -q task.twol -o twol.hfst;\\\n        hfst-compose-intersect lexd.hfst twol.hfst -o $@;\\\n    fi\n\ntask.analyzer.hfst: task.generator.hfst\n    @hfst-invert $^ -o $@\n\nanalysis: task.analyzer.hfst\n    @echo \"$(FORM)\" | hfst-lookup $^\n    @rm -f *.hfst\n\ngeneration: task.generator.hfst\n    @echo \"$(FORM)\" | hfst-lookup $^\n    @rm -f *.hfst\n\nfor_test_%.txt:\n    @curl -s https://raw.githubusercontent.com/agricolamz/2025_morphological_transducers/refs/heads/main/task_tests/$@ -o $@\n\nforms: task.generator.hfst\n    @hfst-fst2strings $^\n    @rm -f *.hfst\n\nclean: \n    @rm -f *.hfst *.txt\n\ntest_%: for_test_%.txt task.generator.hfst\n    @hfst-fst2strings task.generator.hfst &gt; generated_forms.txt\n    @$(eval N_over_wrong=`grep -xvf $&lt; generated_forms.txt | wc -l`)\n    @if [ \"$(N_over_wrong)\" != \"0\" ]; then\\\n        echo \"Overgeneration or wrong generation:\\n\" &gt; test_results.txt;\\\n        grep -xvf $&lt; generated_forms.txt &gt;&gt; test_results.txt;\\\n    fi\n    @$(eval N_not_generated=`grep -xvf generated_forms.txt $&lt;  | wc -l`)\n    @if [ \"$(N_not_generated)\" != \"0\" ]; then\\\n        echo \"\\nNot generated:\\n\" &gt;&gt; test_results.txt;\\\n        grep -xvf generated_forms.txt $&lt; &gt;&gt; test_results.txt;\\\n    fi\n    @if [ ! -f test_results.txt ]; then\\\n        echo \"\\e[32mAll tests for the task have passed.\\e[0m\";\\\n    else\\\n        echo \"\\e[31mIt looks like your lexd/twol files failed one of the tests.\\n\\e[0m\";\\\n        cat test_results.txt;\\\n    fi\n    @rm -f *.hfst *.txt",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Как распространять работу?</span>"
    ]
  },
  {
    "objectID": "09_hygiene.html#footnotes",
    "href": "09_hygiene.html#footnotes",
    "title": "9  Как распространять работу?",
    "section": "",
    "text": "Скрытые файлы или папки начинаются с точки и не высвечиваются по команде ls (но высвечиваются по команде ls -a). Скрытые файлы часто хранят конфигурационные файлы или какие-то данные, которые используют программы. Эта категория файлов создана, чтобы пользователь случайно не переписал или удалил какой-то из файлов.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Как распространять работу?</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Список литературы",
    "section": "",
    "text": "Arefyev, N. V., T. Y. Gratsianova, and K. P. Popov. 2018.\n“Morphological Segmentation with Sequence to Sequence Neural\nNetwork.” In KКомпьютерная Лингвистика и Интеллектуальные\nТехнологии, 85–95.\n\n\nBeesley, K. R., and L. Karttunen. 2003a. Finite State Morphology:\nXerox Tools and Techniques. Stanford: Center for Study of Language\nInformation.\n\n\n———. 2003b. “Two-Level Rule Compiler.”\n\n\nBick, Eckhard. 2013. “Using Constraint Grammar for\nChunking.” In Proceedings of the 19th Nordic Conference of\nComputational Linguistics (NODALIDA 2013), 13–26.\n\n\nBick, Eckhard, and Tino Didriksen. 2015. “CG-3 — Beyond Classical\nConstraint Grammar.” In Proceedings of the 20th Nordic\nConference of Computational Linguistics (NODALIDA 2015), 31–39.\n\n\nBolshakova, E. I., and A. S. Sapin. 2019a. “Bi-LSTM Model for\nMorpheme Segmentation of Russian Words.” In Artificial\nIntelligence and Natural Language: 8th Conference, AINL 2019, Tartu,\nEstonia, November 20–22, 2019, Proceedings 8, 151–60. Springer.\n\n\n———. 2019b. “Comparing Models of Morpheme Analysis for Russian\nWords Based on Machine Learning.” In Компьютерная Лингвистика\nи Интеллектуальные Технологии, 104–13.\n\n\n———. 2020. “An Experimental Study of Neural Morpheme Segmentation\nModels for Russian Word Forms.” In CMCL, 79–89.\n\n\nChomsky, N., and M. Halle. 1968. The Sound Pattern of\nEnglish. New York, Evanstone, London: Haper & Row.\n\n\nGaripov, T., D. Morozov, and A. Glazkova. 2023. “Generalization\nAbility of CNN-Based Morpheme Segmentation.” In 2023\nIvannikov Ispras Open Conference (ISPRAS), 58–62. IEEE.\n\n\nGrönroos, S.-A., S. Virpioja, P. Smit, and M. Kurimo. 2014.\n“Morfessor FlatCat: An\nHMM-Based Method for Unsupervised and Semi-Supervised\nLearning of Morphology.” In Proceedings of COLING 2014, the\n25th International Conference on Computational Linguistics: Technical\nPapers, 1177–85.\n\n\nJohnson, C. D. 1972. Formal Aspects of Phonological\nDescription. The Hague, Paris: Mouton.\n\n\nKarlsson, F. 2013. Finnish: An Essential Grammar. Routledge.\n\n\nKarlsson, Fred, Atro Voutilainen, Juha Heikkilae, and Arto Anttila.\n1995. Constraint Grammar: A Language-Independent System for Parsing\nUnrestricted Text. Vol. 4. Walter de Gruyter.\n\n\nKenesei, István, Robert M Vago, and Anna Fenyvesi. 1998. Hungarian\n(Descriptive Grammars). London; New York: Routledge.\n\n\nKiparsky, P. 1982 (1968). “Linguistic Universals and Linguistic\nChange.” In Explanation in Phonology, edited by P.\nKiparsky. Dordrecht, Cinnaminson: Foris Publications.\n\n\nKoskenniemi, K. 1983. “Two-Level Morphology: A General\nComputational Model for Word-Form Recognition and Production.”\nPhD thesis, University of Helsenki, Department of General Linguistics.\n\n\nLindén, K., E. Axelson, S. Hardwick, T. A. Pirinen, and M. Silfverberg.\n2011. “Hfst—Framework for Compiling and Applying\nMorphologies.” In Systems and Frameworks for Computational\nMorphology: Second International Workshop, SFCM 2011, Zurich,\nSwitzerland, August 26, 2011. Proceedings 2, 67–85. Springer.\n\n\nMealy, G. H. 1955. “A Method for Synthesizing Sequential\nCircuits.” The Bell System Technical Journal 34 (5):\n1045–79.\n\n\nMoore, E. F. 1956. “Gedanken-Experiments on Sequential\nMachines.” Automata Studies 34: 129–53.\n\n\nOrtiz Rojas, S., M. L. Forcada, and G. Ramı́rez Sánchez. 2005.\n“Construcción y Minimización Eficiente\nde Transductores de Letras a Partir de Diccionarios Con\nParadigmas.” Procesamiento Del Lenguaje Natural 35:\n51–57.\n\n\nPrince, A., and P. Smolensky. 1994. “Optimality Theory: Constraint\nInteraction in Generative Grammar.” Rutgers University,\nPiscataway, NJ., Rutgers Center for Cognitive Science.\n\n\nSingh, J., and V. Gupta. 2017. “A Systematic Review of Text\nStemming Techniques.” Artificial Intelligence Review 48:\n157–217.\n\n\nSorokin, Alexey, and Anastasia Kravtsova. 2018. “Deep\nConvolutional Networks for Supervised Morpheme Segmentation of Russian\nLanguage.” In Artificial Intelligence and Natural Language:\n7th International Conference, AINL 2018, St. Petersburg, Russia, October\n17–19, 2018, Proceedings 7, 3–10. Springer.\n\n\nStraka, Milan. 2018. “UDPipe 2.0 Prototype at CoNLL\n2018 UD Shared Task.” In Proceedings of the\nCoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal\nDependencies, 197–207.\n\n\nSumbatova, N. R., and R. O. Mutalov. 2003. A Grammar of\nItsari Dargwa. Muenchen: Lincom Europa.\n\n\nSwanson, Daniel Glen. 2025. “Towards Natural Language Explanations\nof Constraint Grammar Rules.” In Proceedings of the 9th\nWorkshop on Constraint Grammar and Finite State NLP, 28–31.\n\n\nSwanson, D., and N. Howell. 2021. “Lexd: A Finitestate Lexicon\nCompiler for Non-Suffixational Morphologies.” In Multilingual\nFacilitation, 133–46.\n\n\nWintner, S. 2008. “Strengths and Weaknesses of Finite-State\nTechnology: A Case Study in Morphological Grammar Development.”\nNatural Language Engineering 14 (4): 457–69.\n\n\nАркадьев, П. М., Ю. А. Ландер, А. Б. Летучий, Н. Р. Сумбатова, and Я. Г.\nТестелец. 2009. “Введение. Основные Сведения Об Адыгейском\nЯзыке.” In Аспекты Полисинтетизма: Очерки По Грамматике\nАдыгейского Языка, 17–120.\n\n\nАрхангельский, Т. А. 2012. “Принципы Построения Морфологического\nПарсера Для Разноструктурных Языков.” PhD thesis, Московский\nгосударственный университет им. М. В. Ломоносова.\n\n\nИткин, И. Б. 2007. Русская Морфонология. Москва: Гнозис.\n\n\nКумахов, М. А., М. Л. Апажев, З. Х. Бижева, Б. Ч. Бижоев, Дж. Н. Коков,\nХ. Т. Таов, and Р. Х. Темирова. 2006. “Кабардино-Черкесский Язык в\nДвух Томах.”\n\n\nРогава, Г. В., and З. И. Керашева. 1966. Грамматика Адыгейского\nЯзыка. Майкоп, Краснодар: Краснодарское книж. издательство.",
    "crumbs": [
      "Список литературы"
    ]
  }
]